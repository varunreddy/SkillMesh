{
  "registry_name": "skillmesh-tool-catalog",
  "version": "0.4.0",
  "updated_at": "2026-02-28",
  "tools": [
    {
      "id": "viz.matplotlib-seaborn",
      "title": "Visualization with Matplotlib and Seaborn",
      "domain": "visualization",
      "instruction_file": "instructions/visualization-matplotlib-seaborn.md",
      "description": "Publication-grade charting and statistical visualization workflows.",
      "tags": [
        "matplotlib",
        "seaborn",
        "chart",
        "heatmap",
        "dashboard"
      ],
      "tool_hints": [
        "matplotlib.pyplot",
        "seaborn",
        "pandas"
      ],
      "examples": [
        "Build trend/distribution dashboard",
        "Export report-ready charts"
      ],
      "aliases": [
        "charting",
        "data-viz"
      ],
      "dependencies": [
        "matplotlib",
        "seaborn",
        "pandas"
      ],
      "input_contract": {
        "required": "tabular dataframe or tidy data",
        "optional": "style palette, chart type hints"
      },
      "output_artifacts": [
        "png_figures",
        "svg_figures",
        "figure_manifest"
      ],
      "quality_checks": [
        "axis_labels_present",
        "units_present",
        "legend_consistency"
      ],
      "constraints": [
        "avoid_axis_misleading_scales",
        "save_dpi_at_least_180"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "viz",
        "owner": "community"
      },
      "instruction_text": "# Visualization Expert (Matplotlib + Seaborn)\n\nUse this expert when the task needs clear, publication-quality figures that communicate data insights accurately and accessibly.\n\n## When to use this expert\n- The user needs charts, plots, or visual summaries to accompany analysis results.\n- A report or presentation requires embedded figures with consistent styling.\n- Exploratory data analysis needs visual diagnostics (distributions, correlations, trends).\n- The task explicitly mentions chart type selection, styling, or figure export.\n\n## Execution behavior\n\n1. Clarify the analytical goal before choosing a chart type: comparison (bar, dot), distribution (histogram, KDE, box), relationship (scatter, heatmap), composition (stacked bar, pie only if few categories), or trend (line, area).\n2. Use seaborn as the primary API for statistical plots (`sns.histplot`, `sns.boxplot`, `sns.scatterplot`) and fall back to matplotlib for fine-grained layout control (`fig, axes = plt.subplots(...)`).\n3. Apply a consistent theme early: call `sns.set_theme(style=\"whitegrid\")` or equivalent, and set a colorblind-safe palette (`sns.color_palette(\"colorblind\")` or cubehelix).\n4. Label every axis with a human-readable name and units (e.g., \"Revenue (USD, thousands)\"). Add a descriptive title that states the insight, not just the variable name.\n5. Add legends only when needed (multiple series). Position legends outside the plot area when they overlap data.\n6. For multi-panel figures, use `plt.subplots(nrows, ncols, figsize=(...))` with shared axes where scales are comparable. Use `fig.suptitle()` for an overarching title.\n7. Export with `fig.savefig(path, dpi=200, bbox_inches=\"tight\")` for raster formats and `format=\"svg\"` for vector output. Always call `plt.close(fig)` after saving to free memory.\n8. Register each figure as an artifact with a caption explaining what the reader should take away from the chart.\n\n## Decision tree\n- If comparing groups -> use bar chart (vertical for few groups, horizontal for long labels) or dot plot; avoid pie charts unless categories sum to a meaningful whole and there are fewer than 6 slices.\n- If showing distribution -> prefer `histplot` with KDE overlay for continuous data; use `countplot` for categorical. Use violin or box plots when comparing distributions across groups.\n- If showing correlation -> use a heatmap with `annot=True` and diverging colormap centered at zero; mask the upper triangle for symmetric matrices.\n- If the y-axis does not start at zero -> add a visible axis break annotation or explicitly state the truncation in the caption. Do not silently truncate.\n- If the figure has more than 8 colors -> switch to a sequential or grouped color strategy; too many distinct hues become indistinguishable.\n- If the audience is non-technical -> simplify: remove grid lines, reduce tick density, and add direct annotations on key data points.\n\n## Anti-patterns\n- NEVER use rainbow/jet colormaps. They are perceptually non-uniform and misleading for continuous data.\n- NEVER produce a chart without axis labels or a title. Unlabeled charts are uninterpretable outside the notebook.\n- NEVER use 3D plots for data that lives in 2D. 3D bar charts and 3D scatter plots distort perception and add no information.\n- NEVER rely on color alone to encode meaning. Use markers, line styles, or hatching as redundant channels for accessibility.\n- NEVER embed raw matplotlib figures in reports without calling `bbox_inches=\"tight\"`, which clips whitespace and prevents label cutoff.\n\n## Common mistakes\n- Using `plt.show()` inside a script that also calls `savefig`, causing a blank saved image because `show()` clears the figure.\n- Setting `figsize` too small for the number of subplots, resulting in overlapping tick labels.\n- Forgetting to convert datetime columns before plotting, which produces unreadable x-axis labels.\n- Using seaborn's default `hue` without controlling the legend order, leading to inconsistent color-to-category mapping across figures.\n- Saving figures at 72 DPI (screen default) for print, where 200+ DPI is needed.\n- Plotting raw counts on a bar chart when the groups have vastly different sizes, making comparison misleading. Use rates or normalized values.\n\n## Output contract\n- Include at least one explanatory caption per figure stating the insight, not just the chart type.\n- Export at `dpi >= 200` for report integration; use PNG for documents and SVG for web where supported.\n- Keep figure code reproducible from raw DataFrame inputs. No manual pixel editing.\n- Use colorblind-safe palettes by default. Document palette choice in code comments.\n- When multiple figures are produced, maintain consistent axis scales and color mappings across related charts.\n- Close all figure handles after saving to avoid memory leaks in long-running sessions.\n- Report the file path and format of each saved figure as an artifact.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to ensure tidy data suitable for plotting (long format for seaborn).\n- Before this expert -> use the **Statistics Expert** to compute summary statistics, confidence intervals, or test results to annotate on charts.\n- After this expert -> use the **PDF Creation Expert** or **Slide Creation Expert** to embed figures in deliverables.\n- Related -> the **Geospatial Expert** for map-based visualizations that require projected coordinates."
    },
    {
      "id": "ml.model-export",
      "title": "Machine Learning Model Export and Packaging",
      "domain": "machine_learning",
      "instruction_file": "instructions/machine-learning-export.md",
      "description": "Retrieve this expert for saving trained models with reproducible metadata and deployment-safe export formats.",
      "tags": [
        "machine-learning",
        "model-export",
        "pickle",
        "joblib",
        "onnx",
        "sklearn",
        "xgboost"
      ],
      "tool_hints": [
        "joblib",
        "pickle",
        "skl2onnx",
        "onnxruntime"
      ],
      "examples": [
        "Export an sklearn pipeline with version metadata",
        "Convert gradient-boosted model to ONNX for serving"
      ],
      "aliases": [
        "ml-model-export",
        "machine-learning-model-export-and-packaging"
      ],
      "dependencies": [
        "joblib",
        "onnxruntime",
        "skl2onnx",
        "xgboost",
        "lightgbm"
      ],
      "input_contract": {
        "required": "trained model/pipeline object and target serving environment",
        "optional": "latency constraints, interoperability target (ONNX/joblib), and versioning policy"
      },
      "output_artifacts": [
        "serialized_model_artifacts",
        "inference_compatibility_report",
        "model_metadata_manifest"
      ],
      "quality_checks": [
        "model_reload_test_passes",
        "inference_parity_validated",
        "version_and_dependency_metadata_recorded"
      ],
      "constraints": [
        "never_export_unvalidated_model",
        "avoid_format_without_runtime_compatibility_check"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "ml",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Machine Learning Export Expert\n\nUse this expert when trained models must be packaged for reproducible reuse or serving.\n\n## Execution behavior\n\n1. Capture training metadata (library versions, feature schema, preprocessing steps).\n2. Export baseline artifact with `joblib` or `pickle` for Python-native reuse.\n3. Export interoperable artifact (ONNX) when cross-runtime serving is needed.\n4. Validate exports by running a smoke prediction test after save/load.\n5. Version model files and keep checksum metadata for integrity checks.\n\n## Output contract\n\n- Persist model card metadata next to artifact files.\n- Record expected input columns and dtypes.\n- Include fallback behavior when ONNX conversion is unsupported.\n- Never export without a post-load inference check."
    },
    {
      "id": "ml.sklearn-modeling",
      "title": "Scikit-learn Modeling and Evaluation",
      "domain": "machine_learning",
      "instruction_file": "instructions/sklearn-modeling.md",
      "description": "Leakage-safe sklearn pipelines with CV, tuning, and diagnostics.",
      "tags": [
        "sklearn",
        "pipeline",
        "cross-validation",
        "metrics",
        "classification",
        "regression"
      ],
      "tool_hints": [
        "sklearn.pipeline",
        "sklearn.model_selection",
        "sklearn.metrics"
      ],
      "examples": [
        "Train leakage-safe classifier",
        "Tune model with stratified CV"
      ],
      "aliases": [
        "classical-ml",
        "tabular-ml"
      ],
      "dependencies": [
        "scikit-learn",
        "pandas",
        "numpy"
      ],
      "input_contract": {
        "required": "feature matrix X and target y",
        "optional": "sample weights, groups for CV"
      },
      "output_artifacts": [
        "trained_pipeline",
        "metrics_report",
        "cv_fold_scores"
      ],
      "quality_checks": [
        "train_validation_split",
        "no_data_leakage",
        "metric_variance_reported"
      ],
      "constraints": [
        "never_report_train_only_metric"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "ml"
      },
      "instruction_text": "# Scikit-learn Modeling Expert\n\nUse this expert when tasks require robust model training and evaluation in scikit-learn, including classification, regression, clustering, and dimensionality reduction on tabular data.\n\n## When to use this expert\n- The task involves training or comparing classical ML models on structured/tabular data.\n- Feature engineering, preprocessing pipelines, or model selection is explicitly needed.\n- Cross-validated performance estimates with proper leakage controls are required.\n- The user needs reproducible sklearn Pipeline artifacts for downstream serving or reporting.\n\n## Execution behavior\n\n1. Inspect the target variable distribution and decide split strategy: use `StratifiedKFold` for classification with class imbalance, `TimeSeriesSplit` for temporal data, or `train_test_split` with a fixed `random_state` for simple cases.\n2. Build a single `Pipeline` (or `ColumnTransformer` + `Pipeline`) that chains every preprocessing step (imputation, scaling, encoding, feature selection) with the estimator. Never fit transformers outside the pipeline.\n3. Perform model comparison using `cross_val_score` or `cross_validate` with at least 5 folds before committing to a single algorithm.\n4. For hyperparameter search, prefer `RandomizedSearchCV` for wide exploration and `GridSearchCV` only when the search space is already narrow. Always pass `scoring` explicitly.\n5. If nested cross-validation is needed (e.g., research papers, unbiased estimates), wrap the search inside an outer `cross_val_score` loop.\n6. After selecting the best model, refit on the full training set and evaluate once on the held-out test set.\n7. Track metrics aligned to the objective: F1-macro or ROC-AUC for imbalanced classification, MAE/RMSE for regression, silhouette score for clustering.\n8. Persist the trained pipeline with `joblib.dump` alongside a metadata dict recording feature names, target name, sklearn version, and metric scores.\n\n## Decision tree\n- If classes are imbalanced (minority < 15%) -> use `class_weight='balanced'` or SMOTE inside the pipeline, and report precision-recall AUC instead of accuracy.\n- If features mix numeric and categorical -> use `ColumnTransformer` with separate sub-pipelines, not ad-hoc encoding before the pipeline.\n- If dataset has < 1000 rows -> prefer simpler models (LogisticRegression, Ridge) and use repeated stratified k-fold to reduce variance in estimates.\n- If the user asks for \"feature importance\" -> use `permutation_importance` on the test set, not `.feature_importances_` from tree models alone, to avoid bias toward high-cardinality features.\n- If temporal ordering matters -> never shuffle; use `TimeSeriesSplit` and make this explicit in the output.\n\n## Anti-patterns\n- NEVER fit a scaler or encoder on the full dataset before splitting. This leaks information from test into train and inflates metrics.\n- NEVER report accuracy alone on imbalanced datasets. It hides poor minority-class performance.\n- NEVER use `cross_val_predict` to report performance metrics directly; it is designed for generating out-of-fold predictions, not for computing scores.\n- NEVER hard-code column indices in a ColumnTransformer. Use column name lists so the pipeline survives column reordering.\n- NEVER call `.predict()` on training data and present it as model performance.\n\n## Common mistakes\n- Forgetting to set `random_state` in both the splitter and the estimator, making results non-reproducible across runs.\n- Using `LabelEncoder` on input features instead of `OrdinalEncoder` or `OneHotEncoder`. `LabelEncoder` is designed for the target variable only.\n- Applying SMOTE or oversampling before the train/test split, which leaks synthetic samples into the test set.\n- Selecting features based on correlation with the target computed on the full dataset, then evaluating on a \"held-out\" set that influenced feature selection.\n- Using `r2_score` on very small test sets where a single outlier can flip R-squared negative, without noting the instability.\n- Ignoring convergence warnings from `LogisticRegression` or `SVM`. Increase `max_iter` or scale features.\n\n## Output contract\n- Include selected features, preprocessing steps, and estimator hyperparameters in artifact metadata.\n- Report confidence intervals or fold-wise variance for key metrics (e.g., \"F1-macro: 0.82 +/- 0.03\").\n- Provide a confusion matrix for classification or residual diagnostics for regression.\n- Never claim performance from train-only evaluation.\n- Save the fitted pipeline as a single serialized file that can be loaded for inference without re-fitting.\n- Record the sklearn version used (`sklearn.__version__`) in the metadata for reproducibility.\n- If feature importance is computed, include the method used and any caveats.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to handle nulls, type coercion, and deduplication.\n- After this expert -> use the **Machine Learning Export Expert** to package the pipeline for serving or ONNX conversion.\n- After this expert -> use the **Visualization Expert** to plot confusion matrices, ROC curves, or residual distributions.\n- Related -> the **Gradient Boosting Expert** for tree-based alternatives that often outperform sklearn defaults on tabular data.\n- Related -> the **Statistics Expert** for hypothesis testing on model coefficients or residual diagnostics."
    },
    {
      "id": "ml.gradient-boosting",
      "title": "Gradient Boosting with XGBoost and LightGBM",
      "domain": "machine_learning",
      "instruction_file": "instructions/gradient-boosting-xgb-lgbm.md",
      "description": "High-performance boosting, tuning, and SHAP-based attribution.",
      "tags": [
        "xgboost",
        "lightgbm",
        "catboost",
        "shap",
        "boosting"
      ],
      "tool_hints": [
        "xgboost",
        "lightgbm",
        "catboost",
        "shap"
      ],
      "examples": [
        "Tune XGBoost with early stopping",
        "Compare LightGBM vs CatBoost"
      ],
      "aliases": [
        "gbm",
        "tree-boosting"
      ],
      "dependencies": [
        "xgboost",
        "lightgbm",
        "catboost",
        "shap",
        "scikit-learn"
      ],
      "input_contract": {
        "required": "tabular train/validation datasets",
        "optional": "class weights and monotonic constraints"
      },
      "output_artifacts": [
        "booster_model",
        "feature_importance",
        "shap_summary"
      ],
      "quality_checks": [
        "early_stopping_enabled",
        "class_imbalance_strategy",
        "feature_leakage_review"
      ],
      "constraints": [
        "consistent_validation_metric_across_models"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "boosting"
      },
      "instruction_text": "# Gradient Boosting Expert (XGBoost / LightGBM / CatBoost)\n\nUse this expert for high-performance tabular modeling with gradient boosting frameworks, including classification, regression, and ranking tasks.\n\n## When to use this expert\n- The task involves structured/tabular data where tree-based models are expected to excel.\n- The user needs state-of-the-art predictive performance with feature attribution.\n- Early stopping, hyperparameter tuning, or framework comparison is required.\n- SHAP-based model interpretation or class imbalance handling is requested.\n\n## Execution behavior\n\n1. Build a clean train/validation/test split with leakage controls. For temporal data, split by time. For classification, use stratified splits to preserve class ratios.\n2. Configure early stopping with a patience of 20-50 rounds on the validation set. Pass `eval_set` (XGBoost/LightGBM) or `eval_set` (CatBoost) explicitly.\n3. Start with sensible defaults: `learning_rate=0.05`, `max_depth=6`, `n_estimators=2000` (relying on early stopping to find the right count). Tune in this order: (a) `n_estimators` via early stopping, (b) `max_depth` and `min_child_weight`, (c) `subsample` and `colsample_bytree`, (d) regularization (`reg_alpha`, `reg_lambda`), (e) `learning_rate` reduction with proportional `n_estimators` increase.\n4. For multi-framework comparison, hold folds constant (pass the same `cv` splitter) and use identical metric definitions. Report results in a comparison table.\n5. Compute SHAP values using `shap.TreeExplainer` for global and local feature attribution. Generate summary plots, dependence plots for top features, and force plots for individual predictions when interpretability is requested.\n6. Handle class imbalance with `scale_pos_weight` (XGBoost), `is_unbalance` (LightGBM), or `auto_class_weights` (CatBoost). Compare against SMOTE-in-pipeline only if simple weighting underperforms.\n7. For categorical features, prefer LightGBM or CatBoost native categorical handling over one-hot encoding when cardinality > 10.\n8. Save the final model with native `.save_model()` format and record hyperparameters, best iteration, and validation metric in metadata.\n\n## Decision tree\n- If dataset has > 100k rows and many categorical features -> prefer LightGBM for speed; use CatBoost if categoricals have high cardinality and natural ordering is absent.\n- If dataset is small (< 5k rows) -> reduce `max_depth` to 3-4 and increase regularization to prevent overfitting; consider whether a simpler sklearn model might suffice.\n- If the task is ranking -> use `XGBRanker` or `LGBMRanker` with `lambdarank` objective.\n- If feature interactions matter for explanation -> use SHAP interaction values, not just main-effect importance.\n- If prediction latency is critical -> export to ONNX or use LightGBM's `predict_disable_shape_check` for faster inference.\n- If reproducibility is mandatory -> pin `random_state` in the booster AND the data split, and record library version.\n\n## Anti-patterns\n- NEVER set `n_estimators` to a fixed value without early stopping. This either underfits or overfits by construction.\n- NEVER tune hyperparameters on the test set. Use a validation set or inner cross-validation; the test set is touched exactly once.\n- NEVER compare frameworks with different preprocessing (e.g., one-hot for XGBoost but native categoricals for CatBoost) and call it a fair comparison.\n- NEVER ignore the `best_iteration` attribute after early stopping. Predictions must use `best_iteration` to avoid including over-trained trees.\n- NEVER rely solely on `feature_importances_` (gain-based) for feature selection. Gain importance is biased toward high-cardinality and correlated features.\n\n## Common mistakes\n- Using `eval_metric` that does not match the business objective (e.g., `logloss` for early stopping but reporting `F1`).\n- Forgetting to pass `categorical_feature` to LightGBM, causing it to treat integer-encoded categoricals as continuous.\n- Setting `scale_pos_weight` AND applying SMOTE simultaneously, which double-corrects for imbalance.\n- Running SHAP on the training set instead of the validation/test set, which inflates apparent feature relevance.\n- Not setting `verbosity=0` or `verbose=-1` during hyperparameter search, flooding logs with thousands of training lines.\n- Using `pickle` instead of the framework's native `.save_model()`, which breaks across library version upgrades.\n\n## Output contract\n- Report best hyperparameters, best iteration number, and validation metric trajectory (or at minimum start/best/final values).\n- Include the class-imbalance strategy used and its rationale.\n- Provide SHAP summary plots or feature importance rankings with the method explicitly named.\n- Never report train-only metrics as final performance. Always include validation or test metrics.\n- Record the framework name and version (e.g., `xgboost==2.0.3`) in artifact metadata.\n- If multiple frameworks were compared, include a side-by-side metric table with identical folds.\n- Save the model in native format alongside a JSON metadata sidecar.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** for null handling and type coercion. Gradient boosters handle NaNs natively (XGBoost, LightGBM) but benefit from clean categoricals.\n- Before this expert -> use the **Scikit-learn Modeling Expert** if a quick linear baseline is needed for comparison.\n- After this expert -> use the **Visualization Expert** to plot SHAP summaries, learning curves, or metric comparisons.\n- After this expert -> use the **Machine Learning Export Expert** to convert the model to ONNX or package it for serving.\n- Related -> the **Statistics Expert** for post-hoc significance tests when comparing model performance across folds."
    },
    {
      "id": "dl.pytorch-training",
      "title": "Deep Learning with PyTorch",
      "domain": "deep_learning",
      "instruction_file": "instructions/pytorch-training.md",
      "description": "PyTorch training loops with mixed precision, GPU, and checkpoints.",
      "tags": [
        "pytorch",
        "torch",
        "cuda",
        "amp",
        "checkpoint",
        "dataloader"
      ],
      "tool_hints": [
        "torch",
        "torch.nn",
        "torch.utils.data",
        "torch.cuda.amp"
      ],
      "examples": [
        "Train CNN with AMP",
        "Checkpoint best model with patience"
      ],
      "aliases": [
        "deep-learning",
        "torch-training"
      ],
      "dependencies": [
        "torch",
        "torchvision",
        "numpy"
      ],
      "input_contract": {
        "required": "tensor-ready dataset and model definition",
        "optional": "scheduler and mixed precision flag"
      },
      "output_artifacts": [
        "model_checkpoint",
        "training_history",
        "validation_report"
      ],
      "quality_checks": [
        "seed_set",
        "checkpoint_saved",
        "validation_evaluated"
      ],
      "constraints": [
        "do_not_skip_validation",
        "log_epoch_metrics"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "deep-learning"
      },
      "instruction_text": "# PyTorch Training Expert\n\nUse this expert for deep learning pipelines with reproducible training and evaluation, covering CNNs, RNNs, transformers, and custom architectures.\n\n## When to use this expert\n- The task requires training or fine-tuning a neural network on image, text, sequence, or tabular data.\n- Mixed precision training, learning rate scheduling, or gradient clipping is needed.\n- The user needs reproducible training with deterministic seeds and checkpoint recovery.\n- Custom loss functions, architectures, or training loops are involved.\n\n## Execution behavior\n\n1. Set deterministic seeds early: `torch.manual_seed(seed)`, `torch.cuda.manual_seed_all(seed)`, `np.random.seed(seed)`, `random.seed(seed)`. Set `torch.backends.cudnn.deterministic = True` and `torch.backends.cudnn.benchmark = False` for full reproducibility, noting the performance cost.\n2. Define the `Dataset` and `DataLoader` with explicit `num_workers`, `pin_memory=True` (for GPU), and a seeded `generator` for shuffling. Use `persistent_workers=True` when `num_workers > 0` to avoid repeated worker startup.\n3. Build the model, loss function, and optimizer as separate, explicit blocks. Use `model.to(device)` before constructing the optimizer so parameters are on the correct device.\n4. Configure a learning rate scheduler (e.g., `CosineAnnealingLR`, `OneCycleLR`, `ReduceLROnPlateau`). For `ReduceLROnPlateau`, step with the validation metric; for others, step per epoch or per batch as documented.\n5. Enable mixed precision with `torch.amp.GradScaler` and `torch.amp.autocast(device_type=\"cuda\")`. Wrap only the forward pass and loss computation in autocast; keep the backward pass and optimizer step outside.\n6. Implement gradient clipping with `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)` after `loss.backward()` and before `optimizer.step()` to stabilize training.\n7. Track train and validation loss per epoch. Implement early stopping with a patience counter on the validation metric. Save the best model state with `torch.save({\"epoch\": e, \"model_state_dict\": ..., \"optimizer_state_dict\": ..., \"best_val_loss\": ...}, path)`.\n8. After training, load the best checkpoint, run a final evaluation on the held-out test set, and report metrics.\n\n## Decision tree\n- If dataset fits in memory -> preload in `__init__` and transform in `__getitem__`. If too large -> use lazy loading with caching or memory-mapped files.\n- If training is unstable (loss spikes) -> reduce learning rate, add gradient clipping, or check for NaN in inputs. Enable anomaly detection with `torch.autograd.set_detect_anomaly(True)` temporarily.\n- If GPU memory is insufficient -> reduce batch size first, then try gradient accumulation (`accumulation_steps`), then try mixed precision, and only then consider model parallelism.\n- If fine-tuning a pretrained model -> freeze early layers with `param.requires_grad = False`, use a lower learning rate for pretrained layers (differential LR), and a higher rate for the new head.\n- If validation loss diverges from train loss early -> the model is overfitting. Add dropout, weight decay, data augmentation, or reduce model capacity.\n- If using multi-GPU -> prefer `DistributedDataParallel` over `DataParallel` for better scaling. Initialize the process group before model wrapping.\n\n## Anti-patterns\n- NEVER call `optimizer.step()` before `loss.backward()`. The gradient buffers will be stale or zero.\n- NEVER forget `optimizer.zero_grad()` (or `model.zero_grad()`) at the start of each training step. Accumulated gradients from prior batches corrupt updates.\n- NEVER evaluate with the model in training mode. Always call `model.eval()` and wrap inference in `with torch.no_grad():` to disable dropout/batchnorm updates and save memory.\n- NEVER move data to GPU inside the model's `forward()` method. Move inputs to device in the training loop before passing to the model.\n- NEVER use `torch.save(model, path)` for production checkpoints. Save `model.state_dict()` instead, which is architecture-independent and more portable.\n\n## Common mistakes\n- Constructing the optimizer before calling `model.to(device)`, causing parameters to remain on CPU while the model is on GPU.\n- Forgetting to call `scaler.update()` after `scaler.step(optimizer)` when using mixed precision, which silently disables gradient scaling.\n- Using `ReduceLROnPlateau` but stepping it with the training loss instead of the validation metric, defeating its purpose.\n- Setting `num_workers` too high, causing excessive memory usage or deadlocks on systems with limited shared memory.\n- Not handling the `DataLoader` worker random seed, leading to identical augmentations across workers. Pass a `worker_init_fn` that seeds each worker differently.\n- Loading a checkpoint without calling `model.eval()` or without restoring the optimizer state, which breaks training resumption.\n\n## Output contract\n- Record the device choice (CPU / GPU model) and precision mode (FP32 / AMP).\n- Include epoch-wise metric history (train loss, val loss, learning rate) as a logged table or saved CSV.\n- Save at least one recoverable checkpoint containing model state, optimizer state, epoch number, and best validation metric.\n- Report final model performance on held-out validation or test data, not on training data.\n- Record the PyTorch version and CUDA version used.\n- If mixed precision was used, note any operations that required FP32 fallback.\n- Include total training time and throughput (samples/second) when relevant.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to prepare tabular inputs or the **OpenCV Expert** for image preprocessing pipelines.\n- After this expert -> use the **Machine Learning Export Expert** to convert the trained model to ONNX or TorchScript for serving.\n- After this expert -> use the **Visualization Expert** to plot training curves, confusion matrices, or attention maps.\n- Related -> the **Gradient Boosting Expert** for tabular tasks where tree models may outperform neural networks with less tuning.\n- Related -> the **NLP Expert** for transformer fine-tuning workflows using Hugging Face integration."
    },
    {
      "id": "stats.scipy-statsmodels",
      "title": "Statistical Inference with SciPy and Statsmodels",
      "domain": "statistics",
      "instruction_file": "instructions/statistics-scipy-statsmodels.md",
      "description": "Hypothesis testing, ANOVA, and regression diagnostics.",
      "tags": [
        "scipy",
        "statsmodels",
        "anova",
        "ols",
        "p-value",
        "diagnostics"
      ],
      "tool_hints": [
        "scipy.stats",
        "statsmodels.api",
        "statsmodels.formula.api"
      ],
      "examples": [
        "Run Welch t-test with effect size",
        "Fit OLS with robust errors"
      ],
      "aliases": [
        "inference",
        "classical-stats"
      ],
      "dependencies": [
        "scipy",
        "statsmodels",
        "pandas",
        "numpy"
      ],
      "input_contract": {
        "required": "well-defined hypotheses and structured sample groups",
        "optional": "multiple-testing correction strategy"
      },
      "output_artifacts": [
        "test_results",
        "model_summary",
        "diagnostic_plots"
      ],
      "quality_checks": [
        "assumptions_checked",
        "effect_size_reported",
        "pvalue_reported"
      ],
      "constraints": [
        "avoid_causal_claim_for_observational_data"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "stats"
      },
      "instruction_text": "# Statistics Expert (SciPy + Statsmodels)\n\nUse this expert for inferential statistics, classical modeling, hypothesis testing, and diagnostic checks on observational or experimental data.\n\n## When to use this expert\n- The task requires hypothesis testing (t-test, chi-square, ANOVA, non-parametric tests).\n- Regression modeling with coefficient interpretation, diagnostics, or robust standard errors is needed.\n- Effect size estimation, power analysis, or multiple comparison corrections are requested.\n- The user needs to validate statistical assumptions before drawing conclusions.\n\n## Execution behavior\n\n1. Profile the data for distributional properties: check normality (Shapiro-Wilk for n < 5000, D'Agostino-Pearson or Q-Q plot for larger samples), variance homogeneity (Levene's test), and independence structure.\n2. Select the test family with explicit rationale: parametric tests when assumptions hold, non-parametric alternatives (Mann-Whitney U, Kruskal-Wallis, Wilcoxon) when they do not. Document why the chosen test is appropriate.\n3. State the null and alternative hypotheses in plain language before running the test. Define the significance threshold (alpha) upfront, defaulting to 0.05 unless the domain requires stricter control.\n4. Compute the test statistic, p-value, AND an appropriate effect size (Cohen's d, eta-squared, Cramer's V, r, or odds ratio). A p-value without effect size is incomplete.\n5. For regression models, use `statsmodels.formula.api` (OLS, GLM, MixedLM) to get summary tables with coefficients, standard errors, confidence intervals, and R-squared. Check residuals for normality, heteroscedasticity (Breusch-Pagan), and influential points (Cook's distance).\n6. When multiple hypotheses are tested, apply correction: Bonferroni for strict family-wise error control, Benjamini-Hochberg FDR for discovery-oriented analyses. Report both raw and adjusted p-values.\n7. For ANOVA-style comparisons, follow up with post-hoc pairwise tests (Tukey HSD, Dunn's test) only when the omnibus test is significant.\n8. Register statistical artifacts with test name, assumptions checked, results, and a one-sentence interpretation accessible to non-statisticians.\n\n## Decision tree\n- If comparing two group means with normal data -> independent-samples t-test (or Welch's t-test if variances differ). If non-normal -> Mann-Whitney U.\n- If comparing more than two groups -> one-way ANOVA (parametric) or Kruskal-Wallis (non-parametric), followed by post-hoc tests.\n- If examining association between two categorical variables -> chi-square test of independence (expected cell counts >= 5) or Fisher's exact test for small samples.\n- If the data has repeated measures or paired observations -> use paired t-test, Wilcoxon signed-rank, or repeated-measures ANOVA, not independent-samples tests.\n- If the sample size is very large (n > 10,000) -> p-values will be tiny even for trivial effects. Emphasize effect sizes and confidence intervals over significance.\n- If the user says \"correlation\" -> compute Pearson r for linear relationships, Spearman rho for monotonic, and always accompany with a scatter plot.\n\n## Anti-patterns\n- NEVER report a p-value without stating the test used, the sample sizes, and the effect size. Isolated p-values are uninterpretable.\n- NEVER use causal language (\"X causes Y\") for observational data or simple regression. Use \"is associated with\" or \"predicts\" instead.\n- NEVER run a parametric test on heavily skewed data without acknowledging the violation or switching to a non-parametric alternative.\n- NEVER perform stepwise variable selection (forward/backward) and present the final model as if it were pre-specified. This inflates false-positive rates.\n- NEVER interpret a non-significant result as \"no effect.\" It means the data are insufficient to reject the null at the chosen alpha.\n\n## Common mistakes\n- Using a one-sample t-test when a two-sample test is needed, or vice versa, because the hypothesis was not clearly stated.\n- Running Pearson correlation on data with obvious outliers without checking or using robust alternatives (Spearman).\n- Applying Bonferroni correction when tests are not independent, making it overly conservative. Use Holm-Bonferroni or FDR instead.\n- Reporting R-squared from OLS without checking residual plots. A high R-squared with patterned residuals indicates model misspecification.\n- Ignoring multicollinearity in multiple regression. Check VIF (variance inflation factor) and address correlated predictors.\n- Confusing statistical significance with practical significance, especially in large-sample studies where even negligible effects reach p < 0.05.\n\n## Output contract\n- Include null and alternative hypotheses, significance threshold, and sample sizes.\n- Report test statistic, p-value, effect size, and confidence interval together in a structured summary.\n- Flag multiple-comparison risk when many hypotheses are tested, and state the correction method used.\n- Avoid causal language for purely observational models.\n- Include assumption checks performed and their outcomes (passed/failed/borderline).\n- When regression is used, include diagnostic plots or at minimum a textual summary of residual checks.\n- Provide a plain-language interpretation suitable for non-statistical stakeholders.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to handle missing values and outliers that would violate test assumptions.\n- After this expert -> use the **Visualization Expert** to create diagnostic plots (Q-Q, residuals vs fitted, box plots by group).\n- After this expert -> use the **PDF Creation Expert** or **Slide Creation Expert** to embed statistical summaries in reports.\n- Related -> the **Scikit-learn Modeling Expert** for predictive modeling where coefficient interpretation is secondary to prediction accuracy.\n- Related -> the **Gradient Boosting Expert** when non-linear relationships make linear regression insufficient."
    },
    {
      "id": "scipy.optimization-signal",
      "title": "Scientific Computing with SciPy",
      "domain": "scientific_computing",
      "instruction_file": "instructions/scipy-optimization-signal.md",
      "description": "Retrieve this expert for optimization, curve fitting, integration, ODE solving, and signal processing tasks.",
      "tags": [
        "scipy",
        "optimize",
        "curve-fit",
        "ode",
        "integration",
        "interpolation",
        "signal-processing",
        "fft"
      ],
      "tool_hints": [
        "scipy.optimize",
        "scipy.integrate",
        "scipy.interpolate",
        "scipy.signal"
      ],
      "examples": [
        "Calibrate nonlinear parameters with bounds and confidence estimates",
        "Detect peaks and smooth noisy sensor time-series data"
      ],
      "aliases": [
        "scipy-optimization-signal",
        "scientific-computing-with-scipy"
      ],
      "dependencies": [
        "scipy",
        "numpy",
        "matplotlib"
      ],
      "input_contract": {
        "required": "mathematical objective, signal/data characteristics, and optimization constraints",
        "optional": "initial parameter guesses, bounds, and noise assumptions"
      },
      "output_artifacts": [
        "method_selection_rationale",
        "optimized_parameters_or_signal_features",
        "numerical_stability_notes"
      ],
      "quality_checks": [
        "convergence_or_stopping_criteria_checked",
        "residual_or_signal_quality_assessed",
        "sensitivity_to_initial_conditions_reviewed"
      ],
      "constraints": [
        "avoid_overinterpreting_unstable_numerical_results",
        "document_parameter_sensitivity"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "scipy",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Scientific Computing Expert (SciPy Optimization + Signal Processing)\n\nUse this expert for numerical optimization, curve fitting, system simulation, signal analytics, and spectral analysis using SciPy's optimize and signal modules.\n\n## When to use this expert\n- The task requires minimizing or fitting a mathematical objective function with constraints or bounds.\n- Signal processing is needed: filtering, denoising, peak detection, or spectral analysis.\n- The user needs to solve systems of equations, integrate ODEs, or run numerical simulations.\n- Convergence diagnostics or sensitivity analysis on optimization results is requested.\n\n## Execution behavior\n\n1. Define the objective function, gradient (if available), and constraints explicitly as Python callables. Document the mathematical formulation in a docstring or comment.\n2. Choose the solver based on problem class: `minimize` with `method='L-BFGS-B'` for smooth bounded problems, `method='SLSQP'` for constrained problems, `least_squares` for nonlinear curve fitting, `linprog` or `milp` for linear programs, `differential_evolution` for global non-convex search.\n3. Set bounds using `scipy.optimize.Bounds` and constraints using `LinearConstraint` or `NonlinearConstraint` objects (modern API). Avoid the legacy dict-based constraint format.\n4. Run the solver, then check `result.success` and `result.message`. If the solver did not converge, try: (a) different initial guesses, (b) tighter or looser tolerances, (c) an alternative method, (d) rescaling the variables.\n5. For signal processing, apply a window function (Hann, Hamming) before FFT to reduce spectral leakage. Use `scipy.signal.welch` for power spectral density estimation on noisy data.\n6. For filtering, design the filter with `scipy.signal.butter` or `scipy.signal.firwin`, then apply with `filtfilt` (zero-phase) for offline analysis or `lfilter` for causal/real-time scenarios.\n7. Validate fitted solutions with residual analysis, holdout data, or bootstrap confidence intervals. For signal results, compare raw and filtered outputs visually.\n8. Register outputs with parameter values, convergence status, solver name, tolerance, number of iterations, and physical units.\n\n## Decision tree\n- If the problem is convex and smooth -> use `L-BFGS-B` (bounded) or `trust-constr` (constrained). These are fast and reliable.\n- If the problem has multiple local minima -> use `differential_evolution`, `dual_annealing`, or `basinhopping` for global search, then refine with a local solver.\n- If fitting a parametric model to data -> use `curve_fit` for simple cases or `least_squares` with bounds for robust fitting. Prefer `loss='soft_l1'` for data with outliers.\n- If the signal is periodic -> use FFT (`scipy.fft.rfft`) with appropriate zero-padding and windowing. Use `scipy.signal.find_peaks` on the magnitude spectrum for dominant frequencies.\n- If the signal has broadband noise -> apply a Butterworth low-pass or band-pass filter. Choose filter order by inspecting the frequency response (`freqz`).\n- If solving an ODE system -> use `solve_ivp` with `method='RK45'` (default) or `method='BDF'` for stiff systems. Always pass `t_eval` for consistent output spacing.\n\n## Anti-patterns\n- NEVER ignore convergence warnings or `result.success == False`. A non-converged solution is not a solution.\n- NEVER use a single initial guess for non-convex problems and assume the result is the global minimum. Run multiple restarts or use a global method.\n- NEVER apply FFT to a signal without windowing. The implicit rectangular window causes severe spectral leakage at frequency boundaries.\n- NEVER design a filter without checking its frequency response (`freqz`). A poorly designed filter can distort the signal or introduce ringing artifacts.\n- NEVER hard-code the sampling rate. Extract it from the data or metadata and propagate it through all frequency-domain calculations.\n\n## Common mistakes\n- Using `minimize` with `method='Nelder-Mead'` on high-dimensional problems (>10 variables), where it is extremely slow and unreliable.\n- Forgetting to normalize or scale variables before optimization, causing the solver to struggle with ill-conditioned Hessians.\n- Applying `lfilter` instead of `filtfilt` for offline analysis, introducing unwanted phase distortion.\n- Setting FFT length equal to signal length without zero-padding, resulting in poor frequency resolution for short signals.\n- Using `curve_fit` without providing initial parameter guesses (`p0`), relying on the default all-ones starting point that may be far from the solution.\n- Ignoring the Nyquist limit when interpreting FFT results or designing filters. The maximum meaningful frequency is `fs / 2`.\n\n## Output contract\n- Include solver choice, method, stopping criteria (tolerance), and success flag for every optimization run.\n- Report sensitivity to initialization when the problem is non-convex: show results from at least 3 different starting points.\n- Preserve raw vs filtered signals in separate arrays for auditability and comparison.\n- Do not hide convergence warnings; surface them prominently in the output.\n- Include residual norms or goodness-of-fit metrics (R-squared, RMSE) for curve fitting results.\n- Report physical units for all numerical results (Hz for frequencies, seconds for time, etc.).\n- For filter design, include the filter order, cutoff frequencies, and a frequency response plot or description.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to handle missing timestamps, irregular sampling, or noisy sensor data.\n- After this expert -> use the **Visualization Expert** to plot optimization landscapes, convergence histories, or signal spectra.\n- After this expert -> use the **Statistics Expert** for confidence intervals on fitted parameters or hypothesis tests on signal features.\n- Related -> the **PyTorch Training Expert** when the optimization involves neural network-based surrogate models.\n- Related -> the **Chemistry Expert** for optimization of molecular properties or docking scores."
    },
    {
      "id": "chemistry.rdkit-cheminformatics",
      "title": "Chemistry and Cheminformatics (RDKit)",
      "domain": "chemistry",
      "instruction_file": "instructions/chemistry-rdkit.md",
      "description": "SMILES parsing, descriptors, fingerprints, and similarity workflows.",
      "tags": [
        "chemistry",
        "rdkit",
        "smiles",
        "fingerprints",
        "substructure"
      ],
      "tool_hints": [
        "rdkit.Chem",
        "rdkit.Chem.AllChem",
        "rdkit.DataStructs"
      ],
      "examples": [
        "Compute Morgan fingerprints",
        "Filter by Lipinski rules"
      ],
      "aliases": [
        "cheminformatics",
        "molecule-analytics"
      ],
      "dependencies": [
        "rdkit",
        "pandas",
        "numpy"
      ],
      "input_contract": {
        "required": "canonical SMILES list",
        "optional": "substructure SMARTS query"
      },
      "output_artifacts": [
        "descriptor_table",
        "fingerprint_matrix",
        "similarity_rankings"
      ],
      "quality_checks": [
        "invalid_smiles_reported",
        "sanitization_status_logged"
      ],
      "constraints": [
        "do_not_claim_biological_efficacy_without_experimental_data"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "chemistry"
      },
      "instruction_text": "# Chemistry Expert (RDKit)\n\nUse this expert for molecule-level feature extraction, cheminformatics workflows, virtual screening, and structure-activity analysis using RDKit.\n\n## When to use this expert\n- The task involves parsing, validating, or manipulating molecular structures (SMILES, SDF, MOL).\n- Molecular descriptors, fingerprints, or property calculations are needed for modeling or filtering.\n- Substructure searching, similarity screening, or scaffold analysis is requested.\n- The user needs to apply drug-likeness filters (Lipinski, Veber, PAINS) or assess ADMET properties.\n\n## Execution behavior\n\n1. Parse and sanitize all input molecules: use `Chem.MolFromSmiles(smi)` and check for `None` returns. Call `Chem.SanitizeMol(mol)` explicitly when reading from SDF or other formats. Log all unparseable inputs with their original identifiers.\n2. Canonicalize SMILES with `Chem.MolToSmiles(mol, canonical=True)` to ensure consistent representation. Use canonical SMILES as the primary molecular identifier in all outputs.\n3. Select fingerprint type based on the task: Morgan (ECFP-like, `radius=2, nBits=2048`) for general similarity and QSAR, MACCS keys (166 bits) for substructure-based screening, RDKit topological fingerprints for path-based similarity. Document the choice and parameters.\n4. Compute molecular descriptors using `Descriptors.MolWt`, `Descriptors.MolLogP`, `Descriptors.NumHDonors`, `Descriptors.NumHAcceptors`, `Descriptors.TPSA`, and others as needed. Use `Descriptors.CalcMolDescriptors(mol)` for batch descriptor computation.\n5. Apply drug-likeness filters: Lipinski Rule of Five (MW <= 500, LogP <= 5, HBD <= 5, HBA <= 10), Veber rules (TPSA <= 140, RotBonds <= 10). Flag violations rather than silently discarding molecules.\n6. For similarity search, compute Tanimoto similarity on fingerprints using `DataStructs.TanimotoSimilarity`. Use bulk screening with `DataStructs.BulkTanimotoSimilarity` for large libraries.\n7. For batch processing of large molecule sets (> 10k), use `Chem.ForwardSDMolSupplier` for streaming SDF reads, and process in chunks to control memory usage.\n8. Register molecule artifacts with canonical SMILES, computed properties, filter pass/fail flags, and fingerprint parameters in metadata.\n\n## Decision tree\n- If the task is QSAR modeling -> use Morgan fingerprints (radius=2, 2048 bits) as features. Consider adding physicochemical descriptors (MW, LogP, TPSA) as supplementary features.\n- If the task is substructure screening -> use `mol.HasSubstructMatch(pattern)` with SMARTS patterns. Pre-validate SMARTS with `Chem.MolFromSmarts`.\n- If the task is diversity selection -> compute pairwise Tanimoto distances and use MaxMin or sphere-exclusion picking to select a diverse subset.\n- If the task involves 3D conformers -> generate with `AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())` and optimize with `AllChem.MMFFOptimizeMolecule`. Always add hydrogens first with `Chem.AddHs(mol)`.\n- If comparing molecular similarity -> Tanimoto on Morgan FP is the standard baseline; use Dice coefficient when the focus is on shared features rather than overall similarity.\n- If molecules contain salts or mixtures -> use `rdMolStandardize.LargestFragmentChooser` to isolate the parent compound before computing properties.\n\n## Anti-patterns\n- NEVER skip SMILES validation. `Chem.MolFromSmiles` returns `None` for invalid input; processing `None` as a molecule will cause silent errors or crashes downstream.\n- NEVER compare fingerprints generated with different parameters (e.g., radius=2 vs radius=3, or different bit lengths). Similarity scores are meaningless across different fingerprint spaces.\n- NEVER use Euclidean distance on binary fingerprints. Use Tanimoto, Dice, or other set-based metrics appropriate for sparse binary vectors.\n- NEVER assume all SMILES in a dataset are valid or canonical. Always validate and re-canonicalize on ingestion.\n- NEVER report computed LogP or TPSA as experimental values. These are estimates from topological models and should be labeled as \"predicted\" or \"calculated.\"\n\n## Common mistakes\n- Forgetting to call `Chem.AddHs(mol)` before 3D embedding, which produces incorrect geometries for molecules where hydrogen placement matters.\n- Using Morgan fingerprint `radius=2` (ECFP4 equivalent) but calling it \"ECFP2.\" The ECFP diameter is 2x the radius, so radius=2 corresponds to ECFP4.\n- Computing descriptors on molecules containing explicit salts or counterions, skewing property values (e.g., inflated molecular weight).\n- Using `GetMorganFingerprintAsBitVect` with too few bits (e.g., 256), causing excessive bit collisions that reduce fingerprint quality.\n- Silently dropping invalid molecules from a dataset without reporting the count, making it unclear what fraction of the input was processable.\n- Not handling stereochemistry: ignoring `@` and `@@` in SMILES leads to treating enantiomers as identical, which matters for bioactivity modeling.\n\n## Output contract\n- Report invalid or unsanitizable molecules with their original identifiers instead of silently dropping them. Include the count and fraction of failures.\n- Include descriptor units and definitions where applicable (e.g., \"TPSA: topological polar surface area in Angstrom squared\").\n- Keep fingerprint settings (type, radius, nBits) in artifact metadata for reproducibility.\n- Distinguish computational screening heuristics (Lipinski flags, predicted LogP) from experimentally validated conclusions.\n- Provide canonical SMILES as the molecular identifier in all output tables.\n- When similarity searches are performed, report the threshold used and the number of hits.\n- Include RDKit version in metadata, as descriptor calculations can vary between releases.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to standardize molecule identifiers and handle missing SMILES in tabular datasets.\n- After this expert -> use the **Scikit-learn Modeling Expert** or **Gradient Boosting Expert** to build QSAR models using computed fingerprints and descriptors.\n- After this expert -> use the **Visualization Expert** to plot chemical space maps (PCA/UMAP on fingerprints), property distributions, or activity cliffs.\n- Related -> the **SciPy Optimization Expert** for molecular property optimization or docking score minimization.\n- Related -> the **Statistics Expert** for analyzing structure-activity relationships or testing descriptor significance."
    },
    {
      "id": "graph.networkx-analytics",
      "title": "Graph Analytics with NetworkX",
      "domain": "graph_analytics",
      "instruction_file": "instructions/graph-networkx.md",
      "description": "Retrieve this expert for graph construction, centrality analysis, community detection, and path-based reasoning tasks.",
      "tags": [
        "networkx",
        "graph",
        "centrality",
        "community-detection",
        "shortest-path",
        "pagerank",
        "node-link",
        "knowledge-graph"
      ],
      "tool_hints": [
        "networkx",
        "scipy.sparse",
        "pandas",
        "matplotlib"
      ],
      "examples": [
        "Compute centrality and detect communities in an interaction network",
        "Analyze shortest paths and bottleneck nodes in a supply graph"
      ],
      "aliases": [
        "graph-networkx-analytics",
        "graph-analytics-with-networkx"
      ],
      "dependencies": [
        "networkx",
        "pandas",
        "numpy",
        "scipy",
        "matplotlib"
      ],
      "input_contract": {
        "required": "graph definition, node-edge semantics, and analysis objective",
        "optional": "graph size characteristics, weighting rules, and output visualization needs"
      },
      "output_artifacts": [
        "graph_construction_pipeline",
        "network_metrics_and_paths",
        "interpretability_summary"
      ],
      "quality_checks": [
        "node_edge_integrity_validated",
        "metric_choice_matches_business_question",
        "large_graph_complexity_limits_noted"
      ],
      "constraints": [
        "avoid_confusing_directed_vs_undirected_semantics",
        "preserve_node_and_edge_identifier_integrity"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "graph",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Graph Analytics Expert (NetworkX)\n\nUse this expert for graph-centric analysis, topology-aware insights, and network science workflows on relational data.\n\n## When to use this expert\n- The task involves relational data that is best modeled as nodes and edges (social networks, citation graphs, dependency trees, knowledge graphs).\n- Centrality, community detection, shortest paths, or connectivity analysis is requested.\n- The user needs to construct a graph from edge lists, adjacency matrices, or hierarchical data.\n- Network visualization or topology-based feature extraction is required.\n\n## Execution behavior\n\n1. Build the graph from the data source with an explicit directed/undirected choice. Use `nx.from_pandas_edgelist(df, source, target, edge_attr)` for DataFrames or `G.add_edges_from(edge_list)` for raw lists. Validate that the expected node and edge counts match the input.\n2. Inspect basic topology: `G.number_of_nodes()`, `G.number_of_edges()`, `nx.is_connected(G)` (undirected) or `nx.is_weakly_connected(G)` (directed), and `nx.density(G)`. Report disconnected components if present.\n3. Compute centrality metrics aligned to the business question: degree centrality for popularity, betweenness for brokerage/bottlenecks, closeness for reachability, eigenvector or PageRank for influence propagation. Use `nx.pagerank(G)` for directed graphs with weighted edges.\n4. Run community detection using `nx.community.louvain_communities(G)` or Girvan-Newman for small graphs. Compare partition quality using modularity (`nx.community.modularity`). Test stability by running with multiple random seeds.\n5. Analyze shortest paths and connectivity: `nx.shortest_path_length(G, source, target)` for pairwise distances, `nx.average_shortest_path_length(G)` for global reachability (only on connected graphs), `nx.minimum_edge_cut` for vulnerability analysis.\n6. For large graphs (> 50k nodes), prefer approximate algorithms or sampling. Use `nx.approximate_current_flow_betweenness_centrality` or compute metrics on the largest connected component only.\n7. Generate graph visualizations using `nx.draw` with a layout algorithm suited to the graph structure: `spring_layout` for general graphs, `kamada_kawai_layout` for smaller graphs with meaningful distances, `circular_layout` for regular structures, `shell_layout` for hierarchical data.\n8. Register graph-derived metrics, community assignments, and visual summaries as artifacts with metadata describing the graph properties and algorithms used.\n\n## Decision tree\n- If the graph is directed and represents flow (traffic, supply chain) -> use in-degree/out-degree centrality and `nx.maximum_flow` for capacity analysis.\n- If the graph is social/collaboration -> use betweenness centrality for brokers and Louvain for community structure. Consider edge weights as interaction frequency.\n- If the graph is a dependency tree (package deps, task DAGs) -> use topological sort (`nx.topological_sort`), check for cycles (`nx.find_cycle`), and compute longest paths for critical-path analysis.\n- If nodes have attributes (features) -> combine graph metrics with node features for downstream ML. Export as a feature matrix with centrality columns appended.\n- If the graph has > 100k edges -> avoid `O(n^3)` algorithms (all-pairs shortest path, exact betweenness on full graph). Use sampling or approximate methods.\n- If comparing two graphs -> compute graph-level statistics (diameter, density, degree distribution) side by side rather than attempting graph isomorphism.\n\n## Anti-patterns\n- NEVER compute `average_shortest_path_length` on a disconnected graph. It will raise an error. Compute per-component or use the largest connected component.\n- NEVER use `spring_layout` for graphs with > 5000 nodes without increasing `iterations` and `k` parameters. The default produces unreadable hairballs.\n- NEVER interpret raw centrality scores as absolute rankings across different graphs. Centrality values are graph-size-dependent; normalize or compare within the same graph.\n- NEVER assume an undirected graph when the data has inherent directionality (e.g., citations, follower relationships). Using the wrong graph type invalidates centrality and path results.\n- NEVER run community detection without reporting the modularity score. Partitions with low modularity (< 0.3) may not represent meaningful community structure.\n\n## Common mistakes\n- Building a `Graph` (undirected) when the data represents directed relationships, causing edge loss when duplicate source-target pairs exist in both directions.\n- Forgetting to handle self-loops, which inflate degree centrality and can cause issues in community detection algorithms.\n- Using `nx.draw` with the default layout on a large graph, producing an uninformative visualization. Filter to a subgraph or use a specialized layout.\n- Computing betweenness centrality on the full graph when only a subset of nodes is of interest. Use `nx.betweenness_centrality_subset` for targeted analysis.\n- Not removing isolated nodes before computing metrics like average path length or clustering coefficient, which can skew results.\n- Interpreting high degree centrality as \"importance\" without considering the domain. In some networks, hubs are noise (e.g., shared utility nodes).\n\n## Output contract\n- Specify graph assumptions: weighted or unweighted, directed or undirected, number of nodes and edges, number of connected components.\n- Include top-k influential nodes with metric values and the centrality method used.\n- Report disconnected components, isolated nodes, and reachability caveats.\n- Keep node and edge provenance for reproducibility (original IDs, source data reference).\n- When community detection is performed, report the number of communities, modularity score, and partition method.\n- If graph visualization is produced, state the layout algorithm and any filtering applied.\n- For path analysis, report the source, target, path length, and whether the graph is weighted.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to validate edge lists, resolve duplicate edges, and handle missing node attributes.\n- After this expert -> use the **Visualization Expert** for publication-quality network plots with custom node coloring by community or centrality.\n- After this expert -> use the **Scikit-learn Modeling Expert** to build classifiers using graph-derived features (centrality, clustering coefficient) as input columns.\n- Related -> the **Statistics Expert** for testing whether observed network properties (e.g., degree distribution) differ from random graph baselines.\n- Related -> the **NLP Expert** for constructing knowledge graphs from text extraction outputs."
    },
    {
      "id": "cv.opencv-image-processing",
      "title": "Computer Vision with OpenCV",
      "domain": "computer_vision",
      "instruction_file": "instructions/opencv-image-processing.md",
      "description": "Image preprocessing, contour extraction, and edge/threshold pipelines.",
      "tags": [
        "opencv",
        "cv2",
        "image-processing",
        "contour",
        "thresholding"
      ],
      "tool_hints": [
        "cv2",
        "numpy",
        "skimage",
        "pillow"
      ],
      "examples": [
        "Object counting via contours",
        "Document cleanup pipeline"
      ],
      "aliases": [
        "image-cv",
        "vision-preprocessing"
      ],
      "dependencies": [
        "opencv-python",
        "numpy",
        "scikit-image",
        "pillow"
      ],
      "input_contract": {
        "required": "image path(s) or ndarray",
        "optional": "ROI masks and confidence thresholds"
      },
      "output_artifacts": [
        "processed_images",
        "contour_stats",
        "detection_overlay"
      ],
      "quality_checks": [
        "image_dimensions_valid",
        "contrast_assessed",
        "transform_params_logged"
      ],
      "constraints": [
        "preserve_original_images",
        "log_non_detected_cases"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "cv"
      },
      "instruction_text": "# OpenCV Image Processing Expert\n\nUse this expert for computer vision preprocessing, feature extraction, and measurement workflows using OpenCV (cv2).\n\n## When to use this expert\n- The task involves loading, transforming, or analyzing images (resize, crop, filter, threshold, segment).\n- Color space conversion, morphological operations, or contour extraction is needed.\n- The user needs to detect edges, keypoints, or objects in images.\n- Camera calibration, image stitching, or geometric transformations are requested.\n\n## Execution behavior\n\n1. Load and validate the image: use `cv2.imread(path, flags)` and immediately check that the result is not `None`. Log the shape `(height, width, channels)` and dtype. Be aware that OpenCV loads images in BGR order, not RGB.\n2. Convert color spaces in the correct order: BGR to RGB for display/matplotlib, BGR to GRAY for processing, BGR to HSV for color-based segmentation. Always use `cv2.cvtColor` with the correct conversion code (e.g., `cv2.COLOR_BGR2RGB`).\n3. Apply preprocessing steps in a logical sequence: resize (if needed for consistency), denoise (`cv2.GaussianBlur`, `cv2.fastNlMeansDenoising`), contrast enhancement (`cv2.equalizeHist` or CLAHE), then task-specific operations.\n4. For segmentation, choose the appropriate method: simple thresholding (`cv2.threshold`) for bimodal histograms, adaptive thresholding for uneven lighting, Otsu's method when the threshold should be auto-selected, or color-range masking in HSV space.\n5. Apply morphological operations to clean binary masks: `cv2.morphologyEx` with `MORPH_OPEN` to remove small noise, `MORPH_CLOSE` to fill small holes, `MORPH_DILATE`/`MORPH_ERODE` for boundary adjustment. Select kernel size based on feature scale.\n6. Extract contours with `cv2.findContours` using the appropriate retrieval mode: `RETR_EXTERNAL` for outermost contours only, `RETR_TREE` for full hierarchy. Filter contours by area, aspect ratio, or solidity to remove spurious detections.\n7. Compute measurements: `cv2.contourArea`, `cv2.arcLength`, `cv2.boundingRect`, `cv2.minEnclosingCircle`, `cv2.moments` for centroid calculation. Convert pixel measurements to physical units using a known scale factor when available.\n8. Save processed outputs with `cv2.imwrite` and register them as artifacts. Preserve originals and processed versions separately with descriptive filenames.\n\n## Decision tree\n- If detecting edges -> use Canny edge detection. Apply Gaussian blur first to reduce noise edges. Tune the low and high thresholds (typical ratio 1:2 or 1:3).\n- If segmenting by color -> convert to HSV, define hue/saturation/value ranges with `cv2.inRange`, and apply morphological cleanup. HSV is more robust to lighting variation than RGB.\n- If the image has uneven illumination -> use adaptive thresholding (`cv2.adaptiveThreshold`) or CLAHE (`cv2.createCLAHE`) instead of global operations.\n- If aligning or registering images -> use feature matching (ORB, SIFT) with `cv2.findHomography` and `cv2.warpPerspective`. Use RANSAC for robust estimation.\n- If measuring objects -> calibrate first (or use a known reference object in the scene), then convert pixel distances to physical units.\n- If processing a batch of images -> verify that all images have the same dimensions and channels, or resize to a common size before batch operations.\n\n## Anti-patterns\n- NEVER display images loaded by OpenCV directly in matplotlib without converting BGR to RGB. The colors will be swapped (blue faces, orange skies).\n- NEVER apply morphological operations with a kernel size of 1x1. It is a no-op that wastes computation.\n- NEVER use `cv2.resize` with interpolation `INTER_NEAREST` for downscaling photographic images. Use `INTER_AREA` for shrinking and `INTER_LINEAR` or `INTER_CUBIC` for enlarging.\n- NEVER assume `cv2.imread` succeeded without checking for `None`. Missing or corrupt files return `None` silently.\n- NEVER apply edge detection or contour finding on a color image directly. Convert to grayscale first.\n- NEVER hard-code threshold values without inspecting the histogram. Thresholds that work on one image may fail on another with different lighting.\n\n## Common mistakes\n- Confusing `(height, width)` with `(width, height)`. OpenCV uses `(rows, cols)` which is `(height, width)`, but functions like `cv2.resize` expect `(width, height)` as the `dsize` argument.\n- Forgetting that `cv2.findContours` modifies the input image in older OpenCV versions. Always pass a copy of the binary mask.\n- Using `cv2.GaussianBlur` with an even kernel size, which raises an error. Kernel sizes must be odd (3, 5, 7, ...).\n- Applying CLAHE to a color image channel-by-channel in RGB, which distorts colors. Convert to LAB, apply CLAHE to the L channel only, then convert back.\n- Not accounting for JPEG compression artifacts when doing fine-grained analysis. Use PNG or TIFF for lossless intermediate storage.\n- Setting contour area filters too aggressively, silently dropping valid small objects. Always visualize filtered contours to verify.\n\n## Output contract\n- Preserve original and processed images as separate files. Never overwrite the input.\n- Record parameter values for each transform step (kernel sizes, thresholds, interpolation methods) in metadata.\n- Include quality checks: flag low-contrast inputs, saturated regions, or images with unexpected dimensions.\n- Report detection results (contour count, areas, centroids) in a structured table or dict.\n- When detection confidence is weak (few contours, low contrast), report the limitation explicitly.\n- Save intermediate pipeline stages (grayscale, thresholded, morphed) when debugging or when the user requests pipeline transparency.\n- State the OpenCV version and whether the processing was done in 8-bit or floating-point precision.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** if image metadata (filenames, labels) needs normalization before batch processing.\n- After this expert -> use the **PyTorch Training Expert** to feed preprocessed images into a neural network for classification or detection.\n- After this expert -> use the **Visualization Expert** to create annotated figure panels comparing original and processed images.\n- After this expert -> use the **PDF Creation Expert** to embed image analysis results in a report.\n- Related -> the **SciPy Signal Expert** for frequency-domain image analysis or advanced filtering operations."
    },
    {
      "id": "geo.geopandas-spatial",
      "title": "Geospatial Analysis with GeoPandas",
      "domain": "geospatial",
      "instruction_file": "instructions/geospatial-geopandas.md",
      "description": "Spatial joins, CRS transforms, and map-ready geodata prep.",
      "tags": [
        "geopandas",
        "shapely",
        "spatial-join",
        "crs",
        "choropleth"
      ],
      "tool_hints": [
        "geopandas",
        "shapely",
        "pyproj",
        "folium"
      ],
      "examples": [
        "Point-in-polygon joins",
        "Reproject and aggregate by region"
      ],
      "aliases": [
        "gis",
        "spatial-analytics"
      ],
      "dependencies": [
        "geopandas",
        "shapely",
        "pyproj",
        "fiona"
      ],
      "input_contract": {
        "required": "valid geometry columns with declared CRS",
        "optional": "boundary datasets and map styling"
      },
      "output_artifacts": [
        "joined_geodataframe",
        "spatial_aggregate_table",
        "map_ready_layers"
      ],
      "quality_checks": [
        "crs_validated",
        "invalid_geometries_reported",
        "join_predicate_logged"
      ],
      "constraints": [
        "reproject_before_metric_distance_ops"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "geo"
      },
      "instruction_text": "# Geospatial Analysis Expert (GeoPandas)\n\nUse this expert for location-aware data processing, spatial joins, geometry operations, and map preparation using GeoPandas and related libraries.\n\n## When to use this expert\n- The task involves geographic data with coordinates, polygons, or spatial relationships.\n- Spatial joins, overlays, buffering, or distance calculations are needed.\n- The user needs to create choropleths, point maps, or other geospatial visualizations.\n- Coordinate reference system (CRS) management or projection selection is required.\n\n## Execution behavior\n\n1. Load spatial data with `gpd.read_file()` for shapefiles, GeoJSON, or GeoPackage. For CSV with lat/lon columns, create geometry with `gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\")`. Always set the CRS at load time.\n2. Validate geometry integrity immediately: check for null geometries (`gdf[gdf.geometry.is_empty | gdf.geometry.isna()]`), invalid geometries (`gdf[~gdf.is_valid]`), and fix with `gdf.geometry = gdf.geometry.buffer(0)` or `gdf.geometry = gdf.geometry.make_valid()`.\n3. Reproject datasets to a compatible CRS before any spatial operation that involves distance, area, or overlay. Use `gdf.to_crs(epsg=...)`. Choose a projected CRS appropriate to the region (e.g., UTM zone for local analysis, Albers Equal Area for continental).\n4. Use spatial joins with explicit predicates: `gpd.sjoin(left, right, predicate=\"intersects\")` for overlap, `predicate=\"within\"` for containment, `predicate=\"nearest\"` for proximity. Understand that `sjoin` returns one row per match, which can duplicate left-side rows.\n5. Build spatial indexes for large datasets by calling `gdf.sindex` before joins or queries. GeoPandas uses rtree or pygeos indexing automatically, but verifying the index exists before repeated operations avoids performance surprises.\n6. Aggregate spatial features to target boundaries (e.g., points to census tracts): use `sjoin` followed by `groupby` and `agg`, or `overlay` for polygon-to-polygon operations.\n7. For choropleth maps, normalize values (per-capita, per-area, percentiles) to avoid misleading visual comparisons between regions of different sizes. Use `gdf.plot(column=\"metric\", legend=True, scheme=\"quantiles\")` with a classification scheme.\n8. Register map-ready GeoDataFrames and summary outputs as artifacts, including CRS information, row counts, and any geometries that were dropped or repaired.\n\n## Decision tree\n- If measuring distances or areas -> reproject to an equal-area or equidistant projected CRS first. NEVER compute distances in EPSG:4326 (geographic degrees); the results are meaningless as metric values.\n- If joining points to polygons -> use `sjoin` with `predicate=\"within\"`. If points fall on polygon boundaries, consider buffering slightly or using `predicate=\"intersects\"`.\n- If the dataset has global coverage -> use an equal-area projection like Mollweide (ESRI:54009) for area calculations. For local analysis, use the appropriate UTM zone.\n- If creating a choropleth of counts -> normalize by area or population before mapping. Raw counts mislead because larger regions accumulate more events by chance.\n- If geometries are invalid (self-intersecting polygons) -> apply `buffer(0)` or `make_valid()` before any overlay operation. Invalid geometries cause silent failures in spatial predicates.\n- If performance is poor on large datasets -> ensure spatial indexing is active, simplify geometries with `gdf.geometry.simplify(tolerance)` for visualization, and consider GeoParquet format for faster I/O.\n\n## Anti-patterns\n- NEVER compute distances, areas, or buffers in a geographic CRS (EPSG:4326). Degree-based calculations produce incorrect metric results that vary by latitude.\n- NEVER join two GeoDataFrames with different CRS without reprojecting first. The spatial predicates will produce nonsensical matches.\n- NEVER plot raw counts on a choropleth without normalization. This produces a \"population density map\" regardless of the variable being shown.\n- NEVER drop null or invalid geometries silently. Report the count and identifiers of dropped features so the user can investigate.\n- NEVER use `gdf.plot()` for production maps without adding a basemap, scale bar, or north arrow context. Raw polygon plots lack geographic reference.\n\n## Common mistakes\n- Forgetting to set CRS when creating a GeoDataFrame from a CSV, resulting in a CRS-less dataset that silently fails spatial joins.\n- Confusing `overlay` (polygon-polygon set operations like intersection, union, difference) with `sjoin` (attribute join based on spatial predicates). Using the wrong one produces incorrect results.\n- Using `gdf.geometry.area` in EPSG:4326 and getting values in \"square degrees\" that have no physical meaning.\n- Applying `simplify()` with too large a tolerance, causing polygons to collapse or lose important boundary detail.\n- Not handling the duplicate rows created by `sjoin` when multiple right features match a single left feature. Always check for and handle duplicates after spatial joins.\n- Plotting large polygon datasets without simplification, causing slow rendering and unresponsive notebooks.\n\n## Output contract\n- State the CRS (EPSG code and name) for every major geospatial artifact produced.\n- Include the spatial join predicate used (`within`, `intersects`, `nearest`) and any buffer distances applied.\n- Report dropped or invalid geometries explicitly with counts and identifiers.\n- Preserve geospatial precision appropriate for the use case (do not over-simplify for analytical outputs).\n- For choropleth outputs, state the normalization method and classification scheme used.\n- Record the geometry types present (Point, Polygon, MultiPolygon) and any mixed-type issues encountered.\n- Include data source attribution and vintage (year) for administrative boundaries.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to standardize location identifiers (FIPS codes, ISO codes) and handle missing coordinates.\n- After this expert -> use the **Visualization Expert** for publication-quality static maps with annotations, scale bars, and insets.\n- After this expert -> use the **PDF Creation Expert** or **Slide Creation Expert** to embed maps in deliverables.\n- Related -> the **Statistics Expert** for spatial autocorrelation tests (Moran's I) or geographic regression models.\n- Related -> the **Scikit-learn Expert** for spatial feature engineering (distance to nearest X, count within radius) fed into predictive models."
    },
    {
      "id": "nlp.spacy-transformers",
      "title": "NLP Pipelines with spaCy and Transformers",
      "domain": "nlp",
      "instruction_file": "instructions/nlp-spacy-transformers.md",
      "description": "NER, text classification, and embedding-backed NLP workflows.",
      "tags": [
        "nlp",
        "spacy",
        "transformers",
        "ner",
        "classification",
        "embeddings"
      ],
      "tool_hints": [
        "spacy",
        "transformers",
        "sentence-transformers",
        "nltk"
      ],
      "examples": [
        "NER extraction pipeline",
        "Transformer text classification"
      ],
      "aliases": [
        "language-processing",
        "text-ml"
      ],
      "dependencies": [
        "spacy",
        "transformers",
        "sentence-transformers",
        "nltk"
      ],
      "input_contract": {
        "required": "text corpus with language context",
        "optional": "label schema and confidence thresholds"
      },
      "output_artifacts": [
        "entity_table",
        "classification_scores",
        "embedding_index"
      ],
      "quality_checks": [
        "model_version_logged",
        "confidence_thresholds_explicit"
      ],
      "constraints": [
        "flag_low_confidence_outputs",
        "preserve_source_text_references"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "nlp"
      },
      "instruction_text": "# NLP Expert (spaCy + Transformers)\n\nUse this expert for practical NLP pipelines from extraction to classification.\n\n## Execution behavior\n\n1. Normalize and tokenize text with language-aware preprocessing.\n2. Use spaCy for fast linguistic parsing and NER baselines.\n3. Use transformer models when semantic depth is required.\n4. Batch inference and capture confidence or logits for filtering.\n5. Register structured NLP outputs (entities, labels, spans, scores).\n\n## Output contract\n\n- Include model name/version and tokenizer details.\n- Keep confidence thresholds explicit and reproducible.\n- Distinguish rule-based and model-based outputs.\n- Flag low-confidence predictions for human review."
    },
    {
      "id": "docs.pdf-generation",
      "title": "PDF Report Generation",
      "domain": "document_generation",
      "instruction_file": "instructions/pdf-creation.md",
      "description": "Deterministic markdown/HTML to PDF rendering for reports.",
      "tags": [
        "pdf",
        "weasyprint",
        "reportlab",
        "markdown",
        "report"
      ],
      "tool_hints": [
        "weasyprint",
        "reportlab",
        "markdown"
      ],
      "examples": [
        "Render branded markdown report",
        "Export multi-page chart PDF"
      ],
      "aliases": [
        "pdf-rendering",
        "report-export"
      ],
      "dependencies": [
        "weasyprint",
        "reportlab",
        "markdown"
      ],
      "input_contract": {
        "required": "markdown/html content",
        "optional": "css overrides and output filename"
      },
      "output_artifacts": [
        "pdf_report",
        "render_log"
      ],
      "quality_checks": [
        "pdf_generated",
        "file_size_nonzero",
        "tables_not_clipped"
      ],
      "constraints": [
        "include_failure_fallback_message"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "docs"
      },
      "instruction_text": "# PDF Creation Expert\n\nUse this expert when analysis output must be delivered as deterministic PDF documents.\n\n## Execution behavior\n\n1. Build a clean intermediate format (Markdown or HTML) before rendering.\n2. Apply print-safe styles (page margins, typography, table wraps, heading hierarchy).\n3. Render with a deterministic backend (WeasyPrint or ReportLab).\n4. Validate generated PDF file exists and has non-zero size.\n5. Register PDF artifact path and include source format traceability.\n\n## Output contract\n\n- Support both narrative text and embedded chart images.\n- Preserve numeric table alignment and avoid clipped cells.\n- Include failure-safe message when PDF backend is unavailable.\n- Output should be reproducible from the same source input."
    },
    {
      "id": "docs.slides-pptx",
      "title": "Slide Deck Creation (PPTX)",
      "domain": "presentation_generation",
      "instruction_file": "instructions/slide-creation.md",
      "description": "Narrative deck creation with PPTX output and chart integration.",
      "tags": [
        "slides",
        "pptx",
        "python-pptx",
        "presentation",
        "storytelling"
      ],
      "tool_hints": [
        "python-pptx",
        "matplotlib",
        "pillow"
      ],
      "examples": [
        "Executive summary deck",
        "Findings deck with visuals"
      ],
      "aliases": [
        "deck-generation",
        "slide-authoring"
      ],
      "dependencies": [
        "python-pptx",
        "pillow",
        "matplotlib"
      ],
      "input_contract": {
        "required": "structured storyline or slide outline",
        "optional": "brand template and theme colors"
      },
      "output_artifacts": [
        "pptx_deck",
        "slide_outline",
        "speaker_notes_stub"
      ],
      "quality_checks": [
        "title_and_recommendation_slides_present",
        "visual_hierarchy_consistent"
      ],
      "constraints": [
        "one_core_message_per_slide"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "slides"
      },
      "instruction_text": "# Slide Creation Expert (PPTX)\n\nUse this expert for executive summaries, project updates, and narrative decks.\n\n## Execution behavior\n\n1. Derive a slide storyline first (problem, analysis, findings, actions).\n2. Allocate one key message per slide and keep text concise.\n3. Generate PPTX using `python-pptx` with consistent templates.\n4. Embed charts/tables as visuals instead of dense paragraphs.\n5. Export final deck and register slide artifact with version/date.\n\n## Output contract\n\n- Include title slide, evidence slides, and recommendation slide.\n- Keep visual hierarchy consistent across slides.\n- Add speaker-note placeholders for critical assumptions.\n- Ensure deck can be opened in standard PowerPoint clients."
    },
    {
      "id": "data.schema-normalization",
      "title": "Schema and Cleaning Specialist",
      "domain": "data_engineering",
      "instruction_file": "instructions/data-cleaning.md",
      "description": "Retrieve this expert for payload normalization, dataframe conversion, and quarter/date alignment.",
      "tags": [
        "data-cleaning",
        "schema",
        "pandas",
        "coercion",
        "align-quarterly",
        "null-handling"
      ],
      "tool_hints": [
        "pandas",
        "pyarrow",
        "data_processing.cleaner.dict_to_dataframe"
      ],
      "examples": [
        "Fix mixed dict/list payload before metric extraction",
        "Align quarterly reports and coerce numeric columns safely"
      ],
      "aliases": [
        "data-schema-normalization",
        "schema-and-cleaning-specialist"
      ],
      "dependencies": [
        "pandas",
        "pyarrow",
        "pandera",
        "pydantic"
      ],
      "input_contract": {
        "required": "raw payload/schema examples and expected canonical schema",
        "optional": "null-handling rules, date/quarter alignment policy, and quality thresholds"
      },
      "output_artifacts": [
        "normalized_dataset",
        "schema_mapping_spec",
        "data_quality_findings"
      ],
      "quality_checks": [
        "type_coercions_logged",
        "missing_and_outlier_handling_documented",
        "schema_drift_flags_reported"
      ],
      "constraints": [
        "avoid_silent_column_drops_or_renames",
        "preserve_raw_to_normalized_lineage"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "data",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Data Cleaning Expert\n\nUse this expert for schema normalization, type coercion, and quarterly alignment.\n\n## Execution behavior\n\n1. Profile nulls, duplicates, and dtype drift first.\n2. Coerce date columns deterministically with explicit format handling.\n3. Standardize numeric fields with controlled error coercion.\n4. Align periodicity before any modeling step.\n5. Register cleaned data artifacts with transformation notes.\n\n## Output contract\n\n- Preserve source columns when feasible.\n- Emit a compact cleaning report with row counts before and after.\n- Never silently drop rows without recording criteria."
    },
    {
      "id": "fe.accessibility",
      "title": "Web Accessibility (WCAG)",
      "domain": "frontend",
      "instruction_file": "instructions/accessibility.md",
      "description": "Retrieve this expert for WCAG compliance, semantic HTML, keyboard navigation, and screen-reader safe UI patterns.",
      "tags": [
        "accessibility",
        "wcag",
        "aria",
        "keyboard-navigation",
        "screen-reader",
        "contrast",
        "frontend"
      ],
      "tool_hints": [
        "axe-core",
        "lighthouse",
        "eslint-plugin-jsx-a11y",
        "aria"
      ],
      "examples": [
        "Audit React UI for WCAG 2.2 AA compliance",
        "Fix keyboard traps and missing ARIA labels"
      ],
      "aliases": [
        "fe-accessibility",
        "web-accessibility-wcag",
        "web-accessibility-(wcag)"
      ],
      "dependencies": [
        "axe-core",
        "lighthouse",
        "eslint-plugin-jsx-a11y"
      ],
      "input_contract": {
        "required": "UI feature intent, interaction states, and target device/browser requirements",
        "optional": "design tokens, brand constraints, and accessibility/performance targets"
      },
      "output_artifacts": [
        "component_or_layout_changes",
        "accessibility_or_performance_notes",
        "integration_usage_guidance"
      ],
      "quality_checks": [
        "responsive_and_state_behaviors_verified",
        "keyboard_and_screen_reader_baseline_checked",
        "render_performance_considered"
      ],
      "constraints": [
        "preserve_design_system_consistency",
        "avoid_regressing_core_accessibility_paths"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "frontend",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Web Accessibility (WCAG) Expert\n\nUse this expert when tasks require ensuring web applications meet WCAG 2.1 AA compliance, including semantic HTML, ARIA roles and properties, keyboard navigation, focus management, screen reader compatibility, color contrast, and assistive technology testing.\n\n## When to use this expert\n- The task involves auditing or remediating a web application for accessibility compliance.\n- Interactive components (modals, dropdowns, tabs, carousels) must be built with full keyboard and screen reader support.\n- Color contrast, text alternatives, or focus management must be evaluated or implemented.\n- A design system or component library must establish accessible-by-default patterns.\n\n## Execution behavior\n\n1. Start with semantic HTML. Use `<button>` for actions, `<a>` for navigation, `<input>` / `<select>` / `<textarea>` for form controls, and `<nav>`, `<main>`, `<aside>`, `<header>`, `<footer>` for landmarks. Semantic elements provide built-in keyboard behavior and screen reader announcements.\n2. Add a skip navigation link as the first focusable element in the DOM: `<a href=\"#main-content\" class=\"sr-only focus:not-sr-only\">Skip to main content</a>`. This lets keyboard users bypass repetitive navigation.\n3. Ensure every interactive element is reachable via `Tab` and activatable via `Enter` or `Space`. For custom widgets, implement the full keyboard pattern from the WAI-ARIA Authoring Practices (e.g., arrow keys for tabs, `Escape` to close dialogs).\n4. Manage focus intentionally. When a modal opens, move focus to the first focusable element inside it and trap focus within the modal. When it closes, return focus to the triggering element. Use `aria-modal=\"true\"` and `role=\"dialog\"`.\n5. Provide text alternatives for all non-decorative images (`alt=\"Description\"`). Mark decorative images with `alt=\"\"`. For complex images (charts, diagrams), provide an extended description via `aria-describedby` or a linked details section.\n6. Verify color contrast meets WCAG AA minimums: 4.5:1 for normal text, 3:1 for large text (18px bold or 24px regular), and 3:1 for UI components and graphical objects. Use tools like axe DevTools, Lighthouse, or the Chrome contrast checker.\n7. Annotate dynamic content regions with `aria-live`. Use `aria-live=\"polite\"` for non-urgent updates (status messages, search results counts) and `aria-live=\"assertive\"` only for critical alerts (error messages, session expiry warnings).\n8. Test with at least one screen reader (VoiceOver on macOS, NVDA on Windows, or TalkBack on Android) in addition to automated tools. Automated scanners catch only 30-40% of accessibility issues; manual testing is essential.\n\n## Decision tree\n- If the element triggers an action -> use `<button>`. If it navigates to a URL -> use `<a href>`. Never use `<div>` or `<span>` with click handlers for either purpose.\n- If a native HTML element exists for the pattern (checkbox, radio, slider) -> use the native element. Only build a custom widget when no native equivalent exists, and then implement the full ARIA pattern.\n- If an image conveys information -> provide descriptive `alt` text. If the image is purely decorative -> set `alt=\"\"` so screen readers skip it.\n- If content updates dynamically without user action -> wrap it in an `aria-live` region. Use `polite` for most cases; use `assertive` only when the user must be interrupted.\n- If a form field has validation errors -> associate the error message with the input via `aria-describedby` and set `aria-invalid=\"true\"` on the input.\n- If color is used to convey meaning (e.g., red for error) -> add a secondary indicator (icon, text label, or pattern) so color-blind users can perceive the information.\n\n## Anti-patterns\n- NEVER use `<div role=\"button\">` when `<button>` is available. The ARIA role does not provide keyboard event handling or form submission behavior; you must reimplement all of it.\n- NEVER remove `:focus` or `:focus-visible` outlines without providing a visible alternative focus indicator. Users who navigate by keyboard depend on focus visibility.\n- NEVER use color as the sole means of conveying information. Add text labels, icons, or patterns alongside color coding.\n- NEVER auto-play audio or video without providing a pause/stop mechanism accessible by keyboard.\n- NEVER omit a skip navigation link. Screen reader and keyboard users should not be forced to tab through the entire navigation on every page.\n- NEVER use `tabindex` values greater than 0. They create unpredictable tab orders. Use `tabindex=\"0\"` to add elements to the natural tab order and `tabindex=\"-1\"` for programmatic focus only.\n\n## Common mistakes\n- Adding ARIA attributes to elements that already have the semantics natively (e.g., `role=\"button\"` on a `<button>`, `aria-required` on an `<input required>`). This is redundant and sometimes causes double announcements.\n- Using `aria-label` on elements that have visible text content. Screen readers will announce the `aria-label` instead of the visible text, causing a disconnect between what sighted and non-sighted users perceive.\n- Trapping focus in a modal but forgetting to return focus to the trigger element when the modal closes, leaving the user stranded at the top of the page.\n- Hiding content with `display: none` or `visibility: hidden` for visual purposes but also hiding it from screen readers. Use a `.sr-only` class (visually hidden but accessible) when content should be announced but not seen.\n- Setting `aria-live` on a region and then replacing the entire region's DOM. Screen readers may not announce changes if the live region element itself is swapped; update the content inside the existing region instead.\n- Failing to label groups of form fields (related checkboxes, radio buttons) with `<fieldset>` and `<legend>`, making it unclear what the group represents to screen reader users.\n\n## Output contract\n- All interactive elements must be operable via keyboard alone (Tab, Enter, Space, Escape, arrow keys as appropriate).\n- All images must have appropriate `alt` attributes: descriptive for informational images, empty for decorative images.\n- Color contrast must meet WCAG 2.1 AA ratios: 4.5:1 for normal text, 3:1 for large text and UI components.\n- All form inputs must have associated `<label>` elements or `aria-label` / `aria-labelledby` attributes.\n- Dynamic content changes must use `aria-live` regions with appropriate politeness settings.\n- Focus must be managed explicitly for modals, drawers, and other overlay patterns (trap on open, restore on close).\n- A skip navigation link must be present as the first focusable element on every page.\n\n## Composability hints\n- Before this expert -> use the **CSS Architecture Expert** to ensure design tokens include accessible color contrast ratios and a visible focus indicator style.\n- Before this expert -> use the **React Expert** or **Next.js Expert** to build the component tree; accessibility auditing works best on already-structured components.\n- After this expert -> use manual screen reader testing as a final validation step; automated tools alone are insufficient.\n- Related -> the **React Native Expert** for mobile accessibility (VoiceOver, TalkBack, accessibilityLabel, accessibilityRole).\n- Related -> the **CSS Architecture Expert** for implementing `:focus-visible` styles, reduced-motion media queries, and high-contrast mode support."
    },
    {
      "id": "web.auth-jwt",
      "title": "JWT Authentication and Authorization",
      "domain": "web_api",
      "instruction_file": "instructions/auth-jwt.md",
      "description": "Retrieve this expert for JWT token issuance, validation, refresh flows, RBAC claims, and secure auth middleware patterns.",
      "tags": [
        "jwt",
        "authentication",
        "authorization",
        "rbac",
        "refresh-token",
        "web-security"
      ],
      "tool_hints": [
        "pyjwt",
        "fastapi security",
        "passport-jwt",
        "oauth2"
      ],
      "examples": [
        "Implement short-lived access token with refresh rotation",
        "Design role-based claims validation middleware"
      ],
      "aliases": [
        "web-auth-jwt",
        "jwt-authentication-and-authorization"
      ],
      "dependencies": [
        "pyjwt",
        "fastapi",
        "authlib",
        "oauthlib"
      ],
      "input_contract": {
        "required": "API/auth objective, request-response expectations, and security requirements",
        "optional": "backward-compatibility constraints, target clients, and deployment environment"
      },
      "output_artifacts": [
        "api_or_auth_design",
        "implementation_changes",
        "validation_and_security_results"
      ],
      "quality_checks": [
        "token_lifecycle_and_rotation_defined",
        "signature_and_claim_validation_enforced",
        "authz_scope_or_role_checks_covered"
      ],
      "constraints": [
        "preserve_api_compatibility_or_version_explicitly",
        "avoid_exposing_sensitive_data_in_logs"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "web",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# JWT Authentication Expert\n\nUse this expert when tasks require implementing JSON Web Token authentication, including token issuance, validation, refresh token flows, role-based access control via claims, token revocation strategies, and secure token storage.\n\n## When to use this expert\n- The task involves issuing and validating JWTs for stateless authentication in web or mobile applications.\n- A refresh token rotation strategy is needed to balance security with user experience.\n- Role-based or permission-based access control must be encoded in token claims.\n- The architecture spans multiple services that need to independently verify identity without shared session state.\n\n## Execution behavior\n\n1. Choose a signing algorithm based on the deployment model: use `RS256` (asymmetric) when multiple services verify tokens independently, or `HS256` (symmetric) only for single-service applications where the secret never leaves one process.\n2. Define a minimal access token payload: `sub` (user ID), `exp` (short-lived, 15-30 minutes), `iat`, `iss`, and a `roles` or `permissions` array for RBAC. Never store sensitive data (email, PII) in the payload.\n3. Implement a refresh token as an opaque, high-entropy random string stored server-side (database or Redis) with a longer expiry (7-30 days). The refresh token is NOT a JWT.\n4. On login, issue both an access token and a refresh token. Return the access token in the response body and the refresh token as an `httpOnly`, `Secure`, `SameSite=Strict` cookie (for SPAs) or in the response body (for mobile clients using secure storage).\n5. On token refresh, validate the refresh token against the server-side store, issue a new access token AND a new refresh token (rotation), and invalidate the old refresh token immediately to detect reuse.\n6. Implement middleware or a dependency that extracts the access token from the `Authorization: Bearer <token>` header, verifies the signature and expiry, and attaches the decoded claims to the request context.\n7. For RBAC, create a permission-checking layer that reads the `roles` or `permissions` claim and compares it against the required permissions for the endpoint. Return `403 Forbidden` for insufficient permissions, `401 Unauthorized` for missing or invalid tokens.\n8. Log all authentication events (login, refresh, revocation, failed verification) with timestamps and client metadata for audit purposes.\n\n## Decision tree\n- If the client is a browser SPA -> store access token in memory (JavaScript variable) and refresh token in an `httpOnly` cookie; never use `localStorage` for tokens.\n- If the client is a mobile app -> store both tokens in the platform's secure storage (Keychain on iOS, EncryptedSharedPreferences on Android) and use refresh token rotation.\n- If multiple services need to verify tokens independently -> use asymmetric signing (`RS256`); distribute the public key via a JWKS endpoint.\n- If token revocation before expiry is required -> implement a token blacklist (Redis set of revoked `jti` values) checked on every request, or use very short-lived access tokens (5 min) with aggressive refresh.\n- If the system requires fine-grained permissions -> encode a `permissions` array in the token rather than broad `roles`; keep the token payload under 1 KB.\n- If tokens must work across domains -> ensure the `iss` and `aud` claims are validated to prevent token misuse across services.\n\n## Anti-patterns\n- NEVER store JWTs in `localStorage` or `sessionStorage`. These are accessible to any JavaScript on the page, making XSS attacks equivalent to full account compromise.\n- NEVER issue tokens without an `exp` claim. Tokens without expiry are permanent credentials that cannot be safely rotated.\n- NEVER use symmetric signing (`HS256`) in a multi-service architecture where the secret must be shared across services. A single compromised service can forge tokens for all others.\n- NEVER embed secrets, API keys, or database credentials in token payloads. JWTs are base64-encoded, not encrypted; anyone can read the payload.\n- NEVER skip refresh token rotation. Reusing the same refresh token indefinitely means a stolen refresh token grants permanent access.\n- NEVER validate only the signature without checking `exp`, `iss`, and `aud` claims. A valid signature alone does not mean the token is current or intended for your service.\n\n## Common mistakes\n- Setting access token expiry too long (hours or days), which increases the window of exploitation if a token is leaked. Keep access tokens to 15-30 minutes.\n- Implementing refresh tokens as JWTs instead of opaque server-side tokens, making them impossible to revoke without a blacklist.\n- Returning `401` for both \"no token provided\" and \"insufficient permissions\" instead of distinguishing between `401 Unauthorized` and `403 Forbidden`.\n- Not handling clock skew between servers, causing valid tokens to be rejected. Add a small `leeway` (30-60 seconds) to expiry validation.\n- Forgetting to invalidate all refresh tokens when a user changes their password or is deactivated, leaving old sessions alive.\n- Putting user profile data (name, email, avatar URL) in the access token, bloating every request header and leaking PII if the token is logged.\n\n## Output contract\n- Access tokens must be short-lived (15-30 minutes) JWTs with `sub`, `exp`, `iat`, `iss`, and role/permission claims.\n- Refresh tokens must be opaque, stored server-side, and rotated on every use.\n- Token verification must check signature, expiry, issuer, and audience at minimum.\n- RBAC enforcement must be a reusable middleware or dependency, not inline logic in each endpoint.\n- All auth events (login, refresh, logout, failure) must be logged with relevant metadata.\n- The implementation must include a logout endpoint that revokes the refresh token.\n- Error responses must distinguish between `401` (authentication failure) and `403` (authorization failure).\n\n## Composability hints\n- Before this expert -> use the **REST API Design Expert** to define which endpoints require authentication and what permission levels are needed.\n- After this expert -> use the **FastAPI Expert** or **Flask Expert** to integrate the JWT middleware into the web framework.\n- Related -> the **Auth OAuth Expert** when the JWT is issued after an OAuth2 authorization code exchange with an external provider.\n- Related -> the **SQLAlchemy Expert** for storing user accounts, refresh tokens, and revocation records in the database."
    },
    {
      "id": "web.auth-oauth",
      "title": "OAuth2 and OpenID Connect",
      "domain": "web_api",
      "instruction_file": "instructions/auth-oauth.md",
      "description": "Retrieve this expert for OAuth2/OIDC authorization flows, PKCE, token exchange, and secure third-party identity integration.",
      "tags": [
        "oauth2",
        "oidc",
        "pkce",
        "authorization-code",
        "identity",
        "sso"
      ],
      "tool_hints": [
        "authlib",
        "openid-connect",
        "oauthlib",
        "pkce"
      ],
      "examples": [
        "Integrate OAuth2 Authorization Code + PKCE in SPA",
        "Validate OIDC ID token and provider JWKS signatures"
      ],
      "aliases": [
        "web-auth-oauth",
        "oauth2-and-openid-connect"
      ],
      "dependencies": [
        "authlib",
        "oauthlib",
        "pyjwt",
        "openid-client"
      ],
      "input_contract": {
        "required": "API/auth objective, request-response expectations, and security requirements",
        "optional": "backward-compatibility constraints, target clients, and deployment environment"
      },
      "output_artifacts": [
        "api_or_auth_design",
        "implementation_changes",
        "validation_and_security_results"
      ],
      "quality_checks": [
        "oauth_flow_matches_client_type",
        "pkce_and_state_validation_enabled",
        "token_storage_and_revocation_controls_defined"
      ],
      "constraints": [
        "preserve_api_compatibility_or_version_explicitly",
        "avoid_exposing_sensitive_data_in_logs"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "web",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# OAuth2 Authentication Expert\n\nUse this expert when tasks require implementing OAuth2 authentication flows, including authorization code grants, PKCE for public clients, token exchange, scope management, and integration with external identity providers such as Google, GitHub, and Microsoft.\n\n## When to use this expert\n- The task involves \"Sign in with Google/GitHub/Microsoft\" or any external identity provider integration.\n- A public client (SPA or mobile app) needs secure token exchange without a client secret.\n- The application must request granular permissions (scopes) from a third-party API on behalf of the user.\n- Machine-to-machine authentication between backend services using client credentials is required.\n\n## Execution behavior\n\n1. Register the application with the identity provider to obtain a `client_id` and `client_secret` (for confidential clients). Configure the exact `redirect_uri` that the provider will call after authorization.\n2. Determine the correct grant type: Authorization Code for server-side apps, Authorization Code + PKCE for SPAs and mobile apps, Client Credentials for service-to-service, and Device Code for input-constrained devices.\n3. For Authorization Code flow: generate a cryptographically random `state` parameter (minimum 32 bytes, URL-safe base64), store it in the user's session, and include it in the authorization URL. On callback, verify the `state` matches before proceeding.\n4. For PKCE: generate a `code_verifier` (43-128 character random string), compute `code_challenge` as the base64url-encoded SHA-256 hash of the verifier, and send the challenge with `code_challenge_method=S256` in the authorization request. Send the verifier in the token exchange request.\n5. Exchange the authorization code for tokens by making a server-side `POST` to the provider's token endpoint with `grant_type=authorization_code`, the code, redirect URI, and client credentials (or PKCE verifier). Never exchange the code from the browser.\n6. Validate the `id_token` (if OpenID Connect) by verifying the signature against the provider's JWKS endpoint, checking `iss`, `aud`, `exp`, and `nonce` claims. Extract user identity from the verified token, not from the `/userinfo` endpoint alone.\n7. Map the external identity to a local user account: look up by provider + subject ID pair, create a new account if first login, and link accounts if the email matches an existing user (with appropriate verification).\n8. Store the provider's access and refresh tokens encrypted in the database if the application needs ongoing API access (e.g., reading GitHub repos). Apply the principle of least privilege when requesting scopes.\n\n## Decision tree\n- If the client is a browser SPA with no backend -> use Authorization Code + PKCE with the token exchange proxied through a lightweight backend-for-frontend (BFF) endpoint.\n- If the client is a server-rendered web app -> use Authorization Code with a client secret stored server-side; exchange the code on the backend.\n- If the client is a mobile app -> use Authorization Code + PKCE with a custom URL scheme or App Links/Universal Links as the redirect URI.\n- If the communication is service-to-service with no user involved -> use Client Credentials grant with scoped permissions.\n- If the provider supports OpenID Connect -> use the `id_token` for identity; do not rely solely on the access token's `/userinfo` endpoint.\n- If users may sign in with multiple providers -> implement account linking by matching verified email addresses, but always confirm with the user before merging accounts.\n\n## Anti-patterns\n- NEVER use the Implicit flow (`response_type=token`). It is deprecated in OAuth 2.1 because tokens are exposed in the URL fragment and browser history.\n- NEVER omit PKCE for public clients (SPAs, mobile apps). Without PKCE, authorization codes can be intercepted and exchanged by malicious apps.\n- NEVER store or transmit tokens in URL query parameters. Tokens in URLs are logged by proxies, browsers, and servers.\n- NEVER skip validation of the `state` parameter on the callback. This is the primary defense against CSRF attacks in the OAuth flow.\n- NEVER request more scopes than the application actually needs. Over-requesting erodes user trust and may trigger additional provider review requirements.\n- NEVER trust the `email` claim from an `id_token` without checking the `email_verified` field. Unverified emails can be used for account takeover.\n\n## Common mistakes\n- Exchanging the authorization code from the frontend JavaScript instead of the backend, exposing the `client_secret` or allowing code interception.\n- Reusing the `state` parameter across requests or using a predictable value (sequential counter, timestamp), defeating CSRF protection.\n- Storing OAuth provider tokens in plaintext in the database instead of encrypting them at rest with a key managed outside the database.\n- Not handling the case where a user revokes access at the provider side, causing stale refresh tokens to fail silently without prompting re-authorization.\n- Assuming the `sub` claim is the same as the user's email or username. The `sub` is an opaque identifier unique to the provider and client combination.\n- Failing to implement token refresh for long-lived integrations, forcing users to re-authorize when the access token expires.\n\n## Output contract\n- The implementation must use Authorization Code + PKCE for all public clients and Authorization Code with client secret for confidential clients.\n- The `state` parameter must be cryptographically random, stored server-side, and validated on every callback.\n- Token exchange must happen on the server, never in client-side code.\n- The `id_token` must be validated by signature, issuer, audience, and expiry before trusting any claims.\n- Provider tokens stored for ongoing API access must be encrypted at rest.\n- Scope requests must follow the principle of least privilege, requesting only what the application actively uses.\n- Account linking must verify email ownership before merging external identities with existing accounts.\n\n## Composability hints\n- Before this expert -> use the **REST API Design Expert** to define the callback endpoint structure and error response format.\n- After this expert -> use the **Auth JWT Expert** to issue application-level JWTs after the OAuth exchange, enabling stateless authentication for subsequent requests.\n- After this expert -> use the **FastAPI Expert** or **Flask Expert** to integrate the OAuth callback routes and session management into the web framework.\n- Related -> the **SQLAlchemy Expert** for storing user-provider links, encrypted tokens, and account records.\n- Related -> the **Auth JWT Expert** when the external provider's tokens are exchanged for locally issued JWTs."
    },
    {
      "id": "cloud.aws-lambda",
      "title": "AWS Lambda Serverless Workloads",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/aws-lambda.md",
      "description": "Retrieve this expert for Lambda handlers, cold-start optimization, packaging, observability, and event-driven serverless patterns.",
      "tags": [
        "aws",
        "lambda",
        "serverless",
        "cold-start",
        "event-driven",
        "cloudwatch"
      ],
      "tool_hints": [
        "aws lambda",
        "boto3",
        "sam",
        "serverless framework"
      ],
      "examples": [
        "Build event-triggered Lambda with DLQ and retries",
        "Optimize Lambda memory/timeouts for lower latency"
      ],
      "aliases": [
        "cloud-aws-lambda",
        "aws-lambda-serverless-workloads"
      ],
      "dependencies": [
        "boto3",
        "aws-lambda-powertools",
        "aws-sam-cli",
        "awscli"
      ],
      "input_contract": {
        "required": "target cloud environment, intended workload behavior, and resource scope",
        "optional": "existing topology, compliance controls, cost and rollback constraints"
      },
      "output_artifacts": [
        "infrastructure_changeset",
        "configuration_artifacts",
        "operational_validation_report"
      ],
      "quality_checks": [
        "timeout_memory_and_retry_policy_reviewed",
        "cold_start_and_dependency_size_considered",
        "observability_and_dlq_configured"
      ],
      "constraints": [
        "avoid_unreviewed_production_changes",
        "pin_or_version_infrastructure_artifacts"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "cloud",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# AWS Lambda Expert\n\nSpecialist in serverless function design, deployment, and operational tuning on AWS Lambda.\n\n## When to use this expert\n- Task requires event-driven compute without managing servers\n- Workload involves API Gateway handlers, S3 triggers, SQS consumers, or scheduled events\n- Cold start optimization or memory/timeout tuning is needed\n- Lambda layers, environment variable management, or VPC integration must be configured\n\n## Execution behavior\n1. Define a single-purpose handler function with clear input parsing and structured output.\n2. Externalize configuration into environment variables; never embed secrets in code.\n3. Choose the runtime, memory size, and timeout based on the workload profile and cost constraints.\n4. Package dependencies as a Lambda layer or container image to reduce deployment size.\n5. Configure a dead letter queue (SQS or SNS) for failed asynchronous invocations.\n6. Set reserved or provisioned concurrency to protect downstream services from burst traffic.\n7. Add structured logging with correlation IDs and enable X-Ray tracing for observability.\n8. Test locally with SAM CLI or a Lambda-compatible harness before deploying.\n\n## Decision tree\n- If execution time exceeds 15 minutes -> break the work into steps and use Step Functions orchestration\n- If payload is under 1 MB and synchronous -> front with API Gateway direct integration\n- If the function needs internet access inside a VPC -> attach a NAT gateway to the private subnet\n- If startup latency is critical (sub-100ms) -> use provisioned concurrency or SnapStart (Java)\n- If processing SQS messages -> set batch size and use partial batch failure reporting\n- If sharing code across functions -> publish a versioned Lambda layer rather than duplicating\n\n## Anti-patterns\n- NEVER build monolithic handlers that perform unrelated tasks in a single function\n- NEVER rely on /tmp persistence between invocations; treat each invocation as stateless\n- NEVER chain Lambdas synchronously (Lambda calling Lambda); use queues or Step Functions\n- NEVER deploy without a dead letter queue for asynchronous event sources\n- NEVER set timeout equal to the API Gateway timeout; leave headroom for retries and cleanup\n- NEVER store database connection pools in handler-level scope without reusing across warm invocations\n\n## Common mistakes\n- Setting memory too low and causing slower CPU allocation, since CPU scales linearly with memory\n- Forgetting that environment variable values are strings; parsing booleans and numbers without validation\n- Using the default 3-second timeout, which silently cuts off longer operations\n- Packaging the entire node_modules or site-packages instead of tree-shaking unused dependencies\n- Granting lambda execution role overly broad permissions (Action: \"*\") instead of least privilege\n- Not accounting for cold starts when VPC-attached Lambdas need ENI provisioning\n\n## Output contract\n- Provide the handler code with explicit error handling and structured JSON responses\n- Document the IAM execution role with minimal required permissions\n- Specify memory, timeout, runtime, and concurrency settings with rationale\n- Include environment variable names and their expected value formats\n- Describe the dead letter queue configuration and retry behavior\n- Report estimated cost per million invocations at the chosen memory tier\n- Include deployment instructions (SAM template, CDK construct, or CLI commands)\n\n## Composability hints\n- Upstream: aws-s3 expert when Lambda is triggered by S3 event notifications\n- Upstream: github-actions expert for CI/CD pipelines that deploy Lambda functions\n- Downstream: aws-vpc expert when the function must access resources inside a VPC\n- Related: terraform expert for managing Lambda infrastructure definitions as code\n- Related: docker expert when packaging Lambda functions as container images instead of zip archives\n- Related: aws-vpc expert when Lambda functions need private subnet access to RDS or ElastiCache"
    },
    {
      "id": "cloud.aws-s3",
      "title": "AWS S3 Bucket Operations",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/aws-s3.md",
      "description": "Retrieve this expert for S3 buckets, lifecycle policies, presigned URLs, encryption, and secure object storage architecture.",
      "tags": [
        "aws",
        "s3",
        "bucket",
        "lifecycle",
        "presigned-url",
        "encryption",
        "storage"
      ],
      "tool_hints": [
        "aws s3",
        "aws s3api",
        "boto3 s3",
        "cloudfront"
      ],
      "examples": [
        "Create versioned encrypted bucket with lifecycle rules",
        "Generate secure presigned upload/download URLs"
      ],
      "aliases": [
        "cloud-aws-s3",
        "aws-s3-bucket-operations"
      ],
      "dependencies": [
        "boto3",
        "awscli",
        "terraform",
        "cloudfront"
      ],
      "input_contract": {
        "required": "target cloud environment, intended workload behavior, and resource scope",
        "optional": "existing topology, compliance controls, cost and rollback constraints"
      },
      "output_artifacts": [
        "infrastructure_changeset",
        "configuration_artifacts",
        "operational_validation_report"
      ],
      "quality_checks": [
        "bucket_policy_and_encryption_reviewed",
        "lifecycle_and_versioning_config_validated",
        "audit_logging_configuration_verified"
      ],
      "constraints": [
        "avoid_unreviewed_production_changes",
        "pin_or_version_infrastructure_artifacts"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "cloud",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# AWS S3 Expert\n\nSpecialist in Amazon S3 bucket operations, storage configuration, security, and data lifecycle management.\n\n## When to use this expert\n- Task requires creating, configuring, or managing S3 buckets and objects\n- Workload involves presigned URLs, cross-account access, or static website hosting\n- Data lifecycle policies, versioning, or replication need to be designed\n- Storage cost optimization or encryption strategy decisions are required\n- Cross-region replication or intelligent tiering must be evaluated\n\n## Execution behavior\n1. Determine the access pattern (frequent, infrequent, archive) to select the correct storage class.\n2. Create the bucket with block public access enabled by default and enforce bucket-level encryption (SSE-S3 or SSE-KMS).\n3. Configure versioning when object history or accidental-delete protection is needed.\n4. Define lifecycle rules to transition objects between storage classes and expire old versions.\n5. Set up access control using IAM policies and bucket policies; avoid legacy ACLs.\n6. Generate presigned URLs with minimal TTL for temporary object access.\n7. Enable server access logging or CloudTrail data events for audit requirements.\n8. Validate the configuration with a dry-run upload/download cycle before handoff.\n\n## Decision tree\n- If serving static assets to the public -> enable S3 website hosting configuration plus a CloudFront distribution with OAC\n- If uploading files larger than 100 MB -> use multipart upload with appropriate part size\n- If cross-account access is required -> use bucket policies with Principal conditions, not ACLs\n- If regulatory compliance demands encryption at rest -> prefer SSE-KMS with a customer-managed key for auditability\n- If objects are rarely accessed after 30 days -> add lifecycle rule transitioning to S3 Infrequent Access or Glacier\n- If event-driven processing is needed -> configure S3 event notifications to SNS, SQS, or Lambda\n\n## Anti-patterns\n- NEVER leave a bucket publicly accessible without an explicit, documented business justification\n- NEVER hardcode AWS credentials in application code; use IAM roles or credential providers\n- NEVER skip lifecycle rules on buckets with unbounded object growth\n- NEVER use path-style URLs for new buckets; always use virtual-hosted-style addressing\n- NEVER store sensitive data without enabling encryption and verifying the bucket policy denies unencrypted uploads\n- NEVER disable block public access settings at the account level without governance controls in place\n\n## Common mistakes\n- Forgetting to enable versioning before setting up cross-region replication, which requires it\n- Using s3:GetObject on the bucket ARN instead of the object ARN (arn:aws:s3:::bucket/*)\n- Setting presigned URL expiration too long, creating a persistent unauthenticated access window\n- Ignoring S3 Transfer Acceleration for latency-sensitive uploads across geographies\n- Configuring CORS only on the bucket without matching the CloudFront behavior origin settings\n- Overlooking that S3 object names are UTF-8 and special characters need URL encoding in API calls\n\n## Output contract\n- Specify the bucket name, region, storage class, and encryption type in the configuration artifact\n- Include the full IAM or bucket policy JSON with least-privilege statements\n- Document lifecycle rules with transition and expiration timelines\n- Provide presigned URL generation code with explicit expiry and HTTP method scope\n- Report versioning status and MFA-delete configuration if enabled\n- Include estimated monthly cost based on projected storage volume and request rates\n- List all event notification targets if configured\n\n## Composability hints\n- Upstream: terraform expert for provisioning infrastructure-as-code definitions of S3 resources\n- Downstream: aws-lambda expert when S3 event notifications trigger processing functions\n- Related: aws-vpc expert when S3 access must go through a VPC endpoint (gateway type) for private connectivity\n- Related: github-actions expert when CI/CD pipelines upload build artifacts or deploy static assets to S3\n- Related: kubernetes expert when pods use S3 for persistent object storage via application SDKs"
    },
    {
      "id": "cloud.aws-vpc",
      "title": "AWS VPC Networking",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/aws-vpc.md",
      "description": "Retrieve this expert for VPC design, subnet routing, security groups, NAT, peering, and network segmentation decisions.",
      "tags": [
        "aws",
        "vpc",
        "subnet",
        "security-group",
        "nat-gateway",
        "networking"
      ],
      "tool_hints": [
        "aws ec2",
        "aws vpc",
        "terraform aws_vpc",
        "flow logs"
      ],
      "examples": [
        "Design multi-AZ private/public subnet layout",
        "Harden VPC ingress/egress with least privilege"
      ],
      "aliases": [
        "cloud-aws-vpc",
        "aws-vpc-networking"
      ],
      "dependencies": [
        "awscli",
        "terraform",
        "vpc-flow-logs",
        "cloudwatch"
      ],
      "input_contract": {
        "required": "target cloud environment, intended workload behavior, and resource scope",
        "optional": "existing topology, compliance controls, cost and rollback constraints"
      },
      "output_artifacts": [
        "infrastructure_changeset",
        "configuration_artifacts",
        "operational_validation_report"
      ],
      "quality_checks": [
        "subnet_route_and_nat_design_validated",
        "security_group_and_nacl_rules_reviewed",
        "multi_az_and_flow_logs_considered"
      ],
      "constraints": [
        "avoid_unreviewed_production_changes",
        "pin_or_version_infrastructure_artifacts"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "cloud",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# AWS VPC Networking Expert\n\nSpecialist in AWS Virtual Private Cloud design, subnet architecture, routing, and network security.\n\n## When to use this expert\n- Task requires designing or modifying VPC network topology\n- Workload involves subnet planning across availability zones for high availability\n- Security group rules, NACLs, or network flow log analysis is needed\n- Cross-VPC connectivity (peering, transit gateway, PrivateLink) must be established\n\n## Execution behavior\n1. Define the VPC CIDR block with sufficient address space for current and projected growth.\n2. Design a multi-AZ subnet layout with distinct tiers: public (ALB), private (compute), and isolated (data).\n3. Provision an internet gateway for public subnets and NAT gateways in each AZ for private subnet egress.\n4. Configure route tables per tier with explicit associations; never modify the main route table.\n5. Apply security groups as the primary stateful firewall with least-privilege ingress and egress rules.\n6. Layer NACLs for subnet-level defense-in-depth only when compliance mandates it.\n7. Enable VPC flow logs to CloudWatch Logs or S3 for traffic visibility and incident investigation.\n8. Validate connectivity end-to-end with VPC Reachability Analyzer before going live.\n\n## Decision tree\n- If multi-region or many-VPC connectivity is needed -> deploy a transit gateway with centralized route management\n- If only two VPCs need private connectivity -> use VPC peering for simplicity and lower cost\n- If compliance requires no internet path for data-tier resources -> place them in isolated subnets with VPC endpoints (PrivateLink)\n- If a service is exposed internally to other accounts -> use a VPC endpoint service backed by a Network Load Balancer\n- If DNS resolution is needed across VPCs -> enable Route 53 Resolver with inbound and outbound endpoints\n- If traffic inspection is required -> insert a Gateway Load Balancer with a third-party appliance\n\n## Anti-patterns\n- NEVER allow 0.0.0.0/0 ingress on security groups attached to anything other than a public-facing load balancer\n- NEVER deploy all resources in a single availability zone; always distribute across at least two AZs\n- NEVER operate without VPC flow logs in production; they are essential for security audits\n- NEVER use overly broad NACLs (allow all) that negate their purpose as a second layer of defense\n- NEVER assign public IP addresses to instances that do not require direct internet inbound access\n- NEVER reuse the same CIDR range across VPCs that may need to peer or connect via transit gateway\n\n## Common mistakes\n- Choosing a CIDR block that overlaps with on-premises networks or other VPCs, preventing future peering\n- Placing NAT gateways in only one AZ, creating a cross-AZ single point of failure\n- Confusing security groups (stateful, instance-level) with NACLs (stateless, subnet-level) and duplicating rules incorrectly\n- Forgetting that security group rules are additive and cannot explicitly deny; use NACLs for deny rules\n- Not creating separate route tables per subnet tier, causing unintended internet exposure for private subnets\n- Ignoring that VPC peering does not support transitive routing; hub-and-spoke requires transit gateway\n\n## Output contract\n- Provide a CIDR allocation plan showing VPC, subnet CIDRs, and AZ assignments\n- Document all security group rules with source, destination, port, protocol, and business justification\n- Include route table entries for each subnet tier\n- Specify NAT gateway placement and elastic IP associations\n- List VPC endpoints with their type (gateway or interface) and attached policies\n- Describe flow log configuration including destination, filter, and retention period\n- Include a network diagram or structured topology summary\n\n## Composability hints\n- Upstream: terraform expert for defining VPC resources as infrastructure-as-code modules\n- Downstream: aws-lambda expert when functions require VPC attachment for private resource access\n- Downstream: kubernetes expert when EKS clusters are deployed into VPC subnets\n- Related: aws-s3 expert when S3 access should traverse a VPC gateway endpoint instead of the public internet\n- Related: github-actions expert when CI/CD runners need VPC connectivity via self-hosted runners in private subnets\n- Related: docker expert when container workloads run on ECS tasks within VPC private subnets"
    },
    {
      "id": "sys.concurrency",
      "title": "Concurrency and Parallelism Patterns",
      "domain": "systems",
      "instruction_file": "instructions/concurrency.md",
      "description": "Retrieve this expert for thread safety, async coordination, lock strategies, and race-condition resistant concurrent architectures.",
      "tags": [
        "concurrency",
        "parallelism",
        "threads",
        "async",
        "race-condition",
        "synchronization"
      ],
      "tool_hints": [
        "threading",
        "asyncio",
        "multiprocessing",
        "locks"
      ],
      "examples": [
        "Diagnose race condition in shared-state service",
        "Choose async vs worker pool strategy for mixed workloads"
      ],
      "aliases": [
        "sys-concurrency",
        "concurrency-and-parallelism-patterns"
      ],
      "dependencies": [
        "python-stdlib"
      ],
      "input_contract": {
        "required": "runtime constraints, correctness requirements, and interface boundaries",
        "optional": "throughput/latency goals, concurrency model, and platform targets"
      },
      "output_artifacts": [
        "implementation_changes",
        "profiling_or_benchmark_notes",
        "safety_and_error_handling_summary"
      ],
      "quality_checks": [
        "error_paths_and_boundary_conditions_tested",
        "concurrency_or_memory_safety_reviewed",
        "performance_tradeoffs_documented"
      ],
      "constraints": [
        "avoid_undefined_behavior_and_hidden_global_state",
        "prefer_deterministic_and_observable_failure_modes"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "systems",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Concurrency Patterns Expert\n\nYou are a concurrency patterns expert specializing in designing correct, performant multi-threaded and asynchronous systems. You apply proven synchronization primitives, lock-free techniques, and async/await patterns to build software that is both safe and scalable under concurrent workloads.\n\n## When to use this expert\n- Designing thread-safe data structures or concurrent access patterns\n- Diagnosing deadlocks, race conditions, or performance bottlenecks in concurrent code\n- Choosing between threads, async/await, actors, and message-passing architectures\n- Implementing producer-consumer pipelines, worker pools, or parallel computation\n\n## Execution behavior\n1. Identify the concurrency model: shared memory (threads + locks), message passing (channels/actors), or async I/O (event loop + futures)\n2. Classify the workload: CPU-bound (parallelize across cores), I/O-bound (use async/await or non-blocking I/O), or mixed (separate the two)\n3. Map shared state and identify every data access that crosses thread or task boundaries\n4. Select the minimal synchronization primitive needed  prefer the simplest correct solution\n5. Establish lock ordering conventions and document them to prevent deadlocks\n6. Implement cancellation and graceful shutdown paths for all concurrent tasks\n7. Write concurrent tests using thread sanitizers (TSan), stress tests, and loom (for Rust) where applicable\n8. Profile under realistic load before optimizing  measure contention, not assumptions\n\n## Decision tree\n- If read-heavy, rare writes  reader-writer lock (`RwLock`, `sync.RWMutex`, `ReadWriteLock`)\n- If producer-consumer  bounded queue with condition variables or async channels\n- If simple shared counter or flag  atomic operations (`AtomicU64`, `atomic.Int64`, `AtomicInteger`)\n- If complex shared state  mutex with RAII guard; keep critical sections as short as possible\n- If I/O-bound concurrency  async/await (tokio, asyncio, goroutines) instead of OS threads\n- If CPU-bound parallelism  thread pool sized to available cores (rayon, `runtime.GOMAXPROCS`, `ThreadPoolExecutor`)\n- If cross-service coordination  distributed locks only as last resort; prefer idempotent operations with optimistic concurrency\n\n## Anti-patterns\n- NEVER acquire multiple locks without a consistent, documented ordering  this is the primary cause of deadlocks\n- NEVER use busy-waiting (spin loops) unless you have profiled and confirmed it outperforms blocking\n- NEVER access shared mutable state without synchronization, even for \"just a read\"  data races cause undefined behavior\n- NEVER hold a lock while performing I/O, network calls, or any blocking operation\n- NEVER add fine-grained locking without profiling first  coarse locks are simpler and often fast enough\n- NEVER assume single-threaded behavior in code that may run concurrently (e.g., lazy initialization without synchronization)\n\n## Common mistakes\n- Deadlock from inconsistent lock acquisition order across two code paths accessing the same pair of mutexes\n- Race condition from reading a shared variable outside the lock, assuming the value is still valid after acquiring it (TOCTOU)\n- Goroutine or thread leaks from missing cancellation in error paths  always defer cleanup\n- Using `notify_one` when `notify_all` is needed (or vice versa) with condition variables, causing missed wakeups\n- Over-synchronizing by wrapping entire functions in a lock instead of only the critical section\n- Confusing concurrency (structure) with parallelism (execution)  async/await is concurrent but may run on a single thread\n\n## Output contract\n- All shared mutable state has documented synchronization strategy\n- Lock ordering is documented and consistent across the codebase\n- No data races detected by thread sanitizer (TSan, Go race detector)\n- Cancellation paths exist for all long-running concurrent operations\n- Thread/task pool sizes are justified by workload characteristics (CPU cores for compute, higher for I/O)\n- Bounded queues are used for producer-consumer to provide backpressure\n- Deadlock-freedom argument is documented for any multi-lock scenario\n\n## Composability hints\n- Before: systems-design expert for deciding what components need concurrency\n- After: memory-management expert to profile allocation pressure under concurrent load\n- Related: rust expert for ownership-based concurrency safety; go expert for goroutine patterns\n- Pair with: cpp-modern expert for C++ threading primitives (std::mutex, std::async, std::jthread)"
    },
    {
      "id": "sec.container-security",
      "title": "Container Security and Hardening",
      "domain": "security",
      "instruction_file": "instructions/container-security.md",
      "description": "Retrieve this expert for container image hardening, runtime controls, Kubernetes pod security, and CI vulnerability gates.",
      "tags": [
        "security",
        "container-security",
        "docker",
        "kubernetes",
        "trivy",
        "seccomp",
        "rootless"
      ],
      "tool_hints": [
        "trivy",
        "grype",
        "cosign",
        "kubernetes pod security"
      ],
      "examples": [
        "Harden Dockerfile for non-root distroless runtime",
        "Add image signing and vulnerability gates to CI pipeline"
      ],
      "aliases": [
        "sec-container-security",
        "container-security-and-hardening"
      ],
      "dependencies": [
        "trivy",
        "grype",
        "cosign",
        "kubernetes",
        "docker"
      ],
      "input_contract": {
        "required": "authorized scope, threat context, and assets under assessment",
        "optional": "compliance framework mapping, risk tolerance, and remediation timeline"
      },
      "output_artifacts": [
        "security_findings",
        "prioritized_remediation_plan",
        "control_verification_checklist"
      ],
      "quality_checks": [
        "image_scan_gate_enforced",
        "runtime_hardening_controls_verified",
        "admission_or_signature_policy_defined"
      ],
      "constraints": [
        "authorized_scope_only",
        "omit_operational_exploit_detail_that_enables_misuse"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "security",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Container Security Expert\n\nSpecialist in hardening container images, runtimes, and orchestration platforms. Applies\ndefense-in-depth from build time through runtime, covering image provenance, vulnerability\nscanning, least-privilege execution, and network segmentation.\n\n## When to use this expert\n- Building or reviewing Dockerfiles and container images for production deployment\n- Configuring Kubernetes pod security standards, network policies, or admission controllers\n- Setting up container image scanning in a CI/CD pipeline\n- Investigating runtime security events in containerized workloads\n\n## Execution behavior\n1. Audit the Dockerfile: verify base image selection, multi-stage build usage, and layer minimization.\n2. Enforce non-root execution: set a numeric USER, drop all capabilities, add only required ones.\n3. Select an appropriate base image: distroless for production, slim variants for build stages.\n4. Integrate image scanning (Trivy, Grype, or Snyk) as a CI gate that fails on critical/high CVEs.\n5. Configure runtime hardening: read-only root filesystem, seccomp profile, AppArmor or SELinux.\n6. Define Kubernetes network policies with default-deny ingress and egress, allowing only required flows.\n7. Enable image signing and verification (cosign/Sigstore) in the admission controller.\n8. Set resource limits (CPU, memory) and disable privilege escalation in the pod security context.\n\n## Decision tree\n- If production image  use distroless or scratch base + non-root user + read-only rootfs\n- If CI pipeline  run Trivy or Grype scan; fail the build on critical or high severity CVEs\n- If Kubernetes network  apply default-deny NetworkPolicy for both ingress and egress, then allowlist\n- If runtime protection  enable seccomp (RuntimeDefault or custom profile) + AppArmor\n- If secrets needed in container  mount via tmpfs volume from a secrets manager; never bake into image\n- If multi-tenant cluster  enforce Pod Security Standards (restricted) via admission controller\n\n## Anti-patterns\n- NEVER run containers as root in production  always specify a non-root USER with a numeric UID\n- NEVER use full OS base images (ubuntu, debian) for production  use distroless or alpine-slim\n- NEVER skip image scanning  every image pushed to a registry must pass a vulnerability gate\n- NEVER run containers in privileged mode or with SYS_ADMIN capability\n- NEVER store secrets, credentials, or configuration passwords in image layers or ENV instructions\n- NEVER use latest tag for base images  pin to a specific digest or version\n\n## Common mistakes\n- Adding a USER directive but placing it before package installation steps, causing permission errors\n- Using alpine with musl libc for applications that depend on glibc, causing subtle runtime failures\n- Scanning images in CI but not re-scanning images already deployed in the registry\n- Setting read-only rootfs without providing writable tmpfs mounts for application temp directories\n- Applying network policies only to ingress while leaving egress unrestricted\n- Forgetting to set allowPrivilegeEscalation: false in the security context\n\n## Output contract\n- Dockerfile follows multi-stage build pattern with minimal final image\n- Final image runs as a non-root user with a numeric UID\n- All capabilities are dropped; only explicitly required capabilities are added back\n- CI pipeline includes an image scan gate that blocks critical and high CVEs\n- Kubernetes pods have a security context with readOnlyRootFilesystem, runAsNonRoot, and seccomp\n- Network policies enforce default-deny with explicit allowlists for required communication\n- Images are pinned to a specific version or digest, not latest\n\n## Composability hints\n- Before: architecture expert (to define service boundaries), dependency-scanning expert (to vet base image packages)\n- After: penetration-testing expert (to validate container escape mitigations), iam-policies expert (for service account scoping)\n- Related: secrets-management expert (for injecting credentials at runtime), owasp-web expert (for application-layer hardening inside the container)"
    },
    {
      "id": "sys.cpp-modern",
      "title": "Modern C++ (C++17/C++20)",
      "domain": "systems",
      "instruction_file": "instructions/cpp-modern.md",
      "description": "Retrieve this expert for modern C++ architecture, RAII safety, move semantics, templates, and performance-aware systems code.",
      "tags": [
        "c++",
        "modern-cpp",
        "raii",
        "smart-pointers",
        "move-semantics",
        "templates"
      ],
      "tool_hints": [
        "clang++",
        "cmake",
        "sanitizers",
        "gtest"
      ],
      "examples": [
        "Refactor legacy pointer code to RAII and smart pointers",
        "Implement generic performance-safe abstractions with templates"
      ],
      "aliases": [
        "sys-cpp-modern",
        "modern-c-c-17-c-20",
        "modern-c++-(c++17/c++20)"
      ],
      "dependencies": [
        "clang",
        "cmake"
      ],
      "input_contract": {
        "required": "runtime constraints, correctness requirements, and interface boundaries",
        "optional": "throughput/latency goals, concurrency model, and platform targets"
      },
      "output_artifacts": [
        "implementation_changes",
        "profiling_or_benchmark_notes",
        "safety_and_error_handling_summary"
      ],
      "quality_checks": [
        "error_paths_and_boundary_conditions_tested",
        "concurrency_or_memory_safety_reviewed",
        "performance_tradeoffs_documented"
      ],
      "constraints": [
        "avoid_undefined_behavior_and_hidden_global_state",
        "prefer_deterministic_and_observable_failure_modes"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "systems",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Modern C++ (C++17/20) Expert\n\nYou are a modern C++ expert specializing in safe, expressive, and performant C++ using C++17 and C++20 features. You apply RAII, smart pointers, value semantics, and compile-time programming to eliminate entire categories of bugs while maintaining C++'s zero-overhead abstraction promise.\n\n## When to use this expert\n- Writing or modernizing C++ code to use C++17/20 idioms and safety patterns\n- Designing type-safe APIs with templates, concepts, and constexpr\n- Eliminating manual memory management with smart pointers and RAII\n- Choosing between std::variant, std::optional, inheritance, and templates for polymorphism\n\n## Execution behavior\n1. Establish the C++ standard version (17 or 20) and compiler support matrix for the target environment\n2. Design ownership semantics first: determine which types own resources and express it through smart pointers and value types\n3. Use RAII for all resource management  files, locks, network connections, GPU handles\n4. Prefer value semantics and move operations; use references for non-owning access\n5. Apply structured bindings, std::optional, and std::variant to make impossible states unrepresentable\n6. Use constexpr and consteval for compile-time computation; validate invariants at compile time where possible\n7. Write unit tests with Catch2 or Google Test; use sanitizers (ASan, UBSan, TSan) in CI\n8. Enable compiler warnings with -Wall -Wextra -Wpedantic and treat warnings as errors in CI\n\n## Decision tree\n- If single ownership  `std::unique_ptr<T>` (default choice for heap allocation)\n- If shared ownership required  `std::shared_ptr<T>` with `std::make_shared`\n- If value might be absent  `std::optional<T>` instead of raw pointer or sentinel value\n- If closed set of types  `std::variant<A, B, C>` with `std::visit` instead of inheritance hierarchy\n- If open set of types (plugin-like)  virtual functions with `std::unique_ptr<Base>`\n- If compile-time computation  `constexpr` functions; use `consteval` (C++20) for must-be-compile-time\n- If read-only string access  `std::string_view` to avoid unnecessary copies\n- If range transformation  C++20 ranges with views for lazy, composable pipelines\n\n## Anti-patterns\n- NEVER use raw `new`/`delete`  use `std::make_unique` or `std::make_shared`\n- NEVER use C-style casts  use `static_cast`, `dynamic_cast`, `reinterpret_cast`, or `std::bit_cast`\n- NEVER use macros where templates, constexpr, or inline functions would work\n- NEVER catch exceptions by value  catch by `const reference` to avoid slicing\n- NEVER use `std::shared_ptr` when `std::unique_ptr` suffices (shared ownership has atomic refcount overhead)\n- NEVER return `std::string_view` or references to temporaries or local objects\n\n## Common mistakes\n- Dangling `std::string_view` from a temporary `std::string` that goes out of scope\n- Using `std::shared_ptr` for everything out of convenience, hiding ownership semantics and adding overhead\n- Forgetting `noexcept` on move constructors, preventing standard containers from using move optimization\n- Template error messages that are incomprehensible  use C++20 concepts to constrain template parameters\n- Structured bindings on non-const maps returning copies instead of references (use `auto& [key, value]`)\n- Missing virtual destructor on base classes used polymorphically through pointers\n\n## Output contract\n- No raw `new`/`delete` outside of low-level allocator implementations\n- All resources managed through RAII (smart pointers, lock guards, file handles)\n- Compiler warnings enabled and clean: `-Wall -Wextra -Wpedantic`\n- Public APIs documented with Doxygen-style comments\n- Sanitizers (ASan, UBSan) pass on the test suite\n- Move semantics implemented correctly (noexcept move constructor and assignment)\n- CMakeLists.txt specifies `CMAKE_CXX_STANDARD 17` or `20` explicitly\n\n## Composability hints\n- Before: systems-design expert for architecture decisions\n- After: memory-management expert for profiling with Valgrind, heaptrack, or AddressSanitizer\n- Related: concurrency expert for thread-safe data structures and lock patterns\n- Pair with: docker expert for reproducible build environments; github-actions expert for CI with sanitizers"
    },
    {
      "id": "fe.css-architecture",
      "title": "CSS Architecture and Design Systems",
      "domain": "frontend",
      "instruction_file": "instructions/css-architecture.md",
      "description": "Retrieve this expert for scalable CSS systems, design tokens, component theming, and responsive layout strategies.",
      "tags": [
        "css",
        "design-system",
        "tailwind",
        "css-modules",
        "responsive",
        "tokens"
      ],
      "tool_hints": [
        "tailwindcss",
        "postcss",
        "css modules",
        "stylelint"
      ],
      "examples": [
        "Define token-based theming system across components",
        "Refactor fragile CSS into predictable architecture layers"
      ],
      "aliases": [
        "fe-css-architecture",
        "css-architecture-and-design-systems"
      ],
      "dependencies": [
        "react"
      ],
      "input_contract": {
        "required": "UI feature intent, interaction states, and target device/browser requirements",
        "optional": "design tokens, brand constraints, and accessibility/performance targets"
      },
      "output_artifacts": [
        "component_or_layout_changes",
        "accessibility_or_performance_notes",
        "integration_usage_guidance"
      ],
      "quality_checks": [
        "responsive_and_state_behaviors_verified",
        "keyboard_and_screen_reader_baseline_checked",
        "render_performance_considered"
      ],
      "constraints": [
        "preserve_design_system_consistency",
        "avoid_regressing_core_accessibility_paths"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "frontend",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# CSS Architecture and Design Systems Expert\n\nUse this expert when tasks require establishing CSS methodology, building or maintaining design systems, implementing responsive layouts, managing theming (dark mode, brand variants), and choosing between utility-first (Tailwind), modular (CSS Modules), or custom property-driven styling approaches.\n\n## When to use this expert\n- The task involves choosing a CSS strategy for a new project or migrating an existing codebase to a scalable approach.\n- A design token system must be created to enforce consistent spacing, color, and typography across the application.\n- Responsive layout, dark mode, or multi-brand theming must be implemented or restructured.\n- CSS specificity conflicts, deeply nested selectors, or inconsistent breakpoints are causing maintenance pain.\n\n## Execution behavior\n\n1. Define design tokens as CSS custom properties on `:root` (or a data attribute scope for theming). Tokens cover color palette, spacing scale (4px base), font sizes, border radii, shadows, and z-index layers.\n2. Choose the primary styling method. Prefer Tailwind CSS for utility-first rapid development; use CSS Modules when component-scoped class names are needed without a utility framework; use plain CSS custom properties plus BEM for teams that prefer semantic class names.\n3. Establish a responsive breakpoint system using mobile-first `min-width` media queries. Define breakpoints as design tokens or Tailwind config values: `sm: 640px`, `md: 768px`, `lg: 1024px`, `xl: 1280px`. Never mix `min-width` and `max-width` queries in the same project.\n4. Implement dark mode using a `data-theme` attribute on `<html>` or the `prefers-color-scheme` media query. Map semantic color tokens (`--color-bg-primary`, `--color-text-primary`) to different values per theme rather than overriding individual component styles.\n5. When using Tailwind, extract repeated utility combinations into component classes with `@apply` only when a pattern appears in three or more places. Prefer React component abstraction over `@apply` for most reuse.\n6. Establish spacing and sizing using the token scale exclusively. Replace magic numbers (`margin: 13px`) with scale values (`margin: var(--space-3)` or Tailwind `m-3`). Enforce this via linting.\n7. Audit selector specificity. Keep selectors at one class deep when possible. Avoid nesting beyond three levels. Never use `!important` except for third-party style overrides documented with a comment explaining why.\n8. Document the design system tokens and component patterns in a living style guide or Storybook instance so that contributors can discover and reuse existing patterns.\n\n## Decision tree\n- If the team values speed and small bundle with purged CSS -> Tailwind CSS with the project's `tailwind.config` defining the design tokens.\n- If component-level style isolation is critical and the project avoids global CSS -> CSS Modules (`.module.css`) with composes for shared tokens.\n- If the project must support multiple brand themes at runtime -> CSS custom properties scoped to a `data-theme` attribute, with a JavaScript toggle.\n- If responsive layout is needed -> mobile-first `min-width` breakpoints; use container queries for components that must adapt to their parent rather than the viewport.\n- If a component has more than five conditional Tailwind classes -> extract it into a `cn()` / `clsx()` helper call for readability; do not resort to inline ternary chains.\n- If a third-party library injects styles that conflict -> scope the override in a dedicated file with minimal `!important` use, documented with the library name and issue.\n\n## Anti-patterns\n- NEVER use inline `style` attributes for layout, spacing, or colors. They bypass the design system, cannot be themed, and have the highest specificity.\n- NEVER chain `!important` declarations to win specificity battles. Refactor selectors to lower specificity or restructure the cascade instead.\n- NEVER use magic numbers for spacing, font sizes, or colors. Every value must trace back to a design token.\n- NEVER mix `min-width` and `max-width` media queries in the same project. Pick one direction (mobile-first with `min-width` is standard) and be consistent.\n- NEVER nest selectors more than three levels deep (e.g., `.page .section .card .header .title`). Flat selectors are easier to override and understand.\n- NEVER duplicate breakpoint values across files. Define them once in a shared config (Tailwind config, CSS custom properties, or Sass variables).\n\n## Common mistakes\n- Defining colors as raw hex values scattered across component files instead of centralizing them as design tokens, making theme changes a search-and-replace nightmare.\n- Using `max-width` breakpoints to \"undo\" desktop styles for mobile, resulting in convoluted overrides instead of progressively enhancing from a mobile base.\n- Over-relying on `@apply` in Tailwind to recreate traditional CSS class hierarchies, losing the utility-first benefit and increasing bundle size.\n- Forgetting to include a `font-family` fallback stack, so when a custom font fails to load, the browser renders in an unexpected serif or monospace default.\n- Setting `z-index: 9999` instead of using a defined z-index scale, causing layer conflicts that are nearly impossible to debug.\n- Ignoring the `:focus-visible` pseudo-class and either showing focus rings on every click or removing them entirely, both of which are accessibility failures.\n\n## Output contract\n- All color, spacing, typography, and shadow values must reference design tokens, not raw literals.\n- Responsive styles must follow a mobile-first `min-width` breakpoint progression.\n- Dark mode and theming must be implemented via CSS custom properties scoped to a togglable attribute or media query.\n- Selector nesting must not exceed three levels, and `!important` must not appear except in documented third-party overrides.\n- Tailwind projects must have a configured `tailwind.config` with the project's design tokens (colors, spacing, fonts) as the single source of truth.\n- A living style guide or Storybook must document the available tokens and component patterns.\n- Every component must be visually tested at the smallest (`sm`) and largest (`xl`) breakpoints at minimum.\n\n## Composability hints\n- Before this expert -> use a **Design/UX Review** process to establish the visual language, spacing scale, and color palette before codifying tokens.\n- After this expert -> use the **React Expert** or **Next.js Expert** to integrate the styling system into component architecture.\n- After this expert -> use the **Accessibility Expert** to verify color contrast ratios (4.5:1 AA), focus indicators, and reduced-motion preferences.\n- Related -> the **React Native Expert** for translating design tokens into `StyleSheet.create` values for mobile.\n- Related -> the **State Management Expert** when theme preference must be persisted and synchronized across components."
    },
    {
      "id": "data.dbt",
      "title": "dbt Analytics Engineering",
      "domain": "data_engineering",
      "instruction_file": "instructions/dbt.md",
      "description": "Retrieve this expert for dbt model layering, testing, documentation, and incremental transformation workflows.",
      "tags": [
        "dbt",
        "analytics-engineering",
        "sql",
        "data-modeling",
        "lineage",
        "testing"
      ],
      "tool_hints": [
        "dbt run",
        "dbt test",
        "dbt docs",
        "dbt source freshness"
      ],
      "examples": [
        "Design staging/intermediate/mart dbt model layers",
        "Implement incremental models with schema tests and lineage"
      ],
      "aliases": [
        "data-dbt",
        "dbt-analytics-engineering"
      ],
      "dependencies": [
        "dbt-core"
      ],
      "input_contract": {
        "required": "source schema/data samples and transformation objectives",
        "optional": "data volume profile, latency/SLA constraints, and governance rules"
      },
      "output_artifacts": [
        "transformation_logic_or_queries",
        "data_quality_report",
        "lineage_or_execution_notes"
      ],
      "quality_checks": [
        "schema_and_type_assumptions_verified",
        "null_and_edge_cases_handled",
        "reproducible_execution_documented"
      ],
      "constraints": [
        "avoid_silent_schema_or_type_breakage",
        "preserve_traceability_of_source_to_output"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "data",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# dbt (Data Build Tool) Expert\n\nUse this expert for designing dbt projects with proper model layering, testing strategies, incremental materialization, macro development, source configuration, and documentation within a modern analytics engineering workflow.\n\n## When to use this expert\n- SQL transformations in a data warehouse need to be version-controlled, tested, and documented.\n- A layered modeling architecture (staging, intermediate, marts) must be designed or refactored.\n- Incremental models are needed for large tables where full refreshes are too slow or expensive.\n- Reusable SQL logic should be extracted into macros or packages.\n\n## Execution behavior\n\n1. Define sources in a `sources.yml` file with database, schema, table names, and freshness checks. Never reference raw tables directly in models; always go through the source macro `{{ source('source_name', 'table_name') }}`.\n2. Build the staging layer first: create one staging model per source table that renames columns to a consistent convention, casts types explicitly, and filters out known bad records. Staging models should be views materialized as `view`.\n3. Create intermediate models for business logic that joins, aggregates, or transforms staging models. Name them with an `int_` prefix and materialize as `view` or `ephemeral` depending on reuse.\n4. Build mart models as the final consumer-facing tables. These should be materialized as `table` or `incremental` and organized by business domain (e.g., `marts/finance/`, `marts/marketing/`).\n5. Add tests to every model: `unique` and `not_null` on primary keys at minimum, `relationships` tests for foreign key integrity, and `accepted_values` for enum columns. Add custom data tests in the `tests/` directory for complex business rules.\n6. For large tables (> 10M rows), use incremental materialization with a reliable `unique_key` and an `is_incremental()` filter on a timestamp or monotonically increasing column.\n7. Extract repeated SQL patterns into macros in the `macros/` directory. Use Jinja templating for conditional logic, but keep macros focused and well-documented.\n8. Write documentation in `schema.yml` files co-located with models. Every model and every column exposed to downstream consumers must have a description.\n\n## Decision tree\n- If the model reads from a raw external table -> it must be a staging model that references `{{ source() }}`, not a direct table reference.\n- If a table has more than 10M rows and grows daily -> use `incremental` materialization with a `unique_key` and a timestamp-based `is_incremental()` filter.\n- If the same SQL logic appears in three or more models -> extract it into a macro with clear parameter names and a docstring.\n- If a model is referenced by other models but never queried directly -> consider `ephemeral` materialization to reduce warehouse object count.\n- If another team or project has published a dbt package for a shared data source -> use `dbt deps` to install it rather than reimplementing the logic.\n- If business logic involves complex conditional aggregation -> build it in an intermediate model with clear naming, not buried in a mart model CTE.\n\n## Anti-patterns\n- NEVER query raw source tables directly in intermediate or mart models. Always go through a staging model that normalizes the schema.\n- NEVER ship a model without at least `unique` and `not_null` tests on its primary key. Untested models silently propagate data quality issues.\n- NEVER build monolithic models with 500+ lines of SQL. Break them into composable staging and intermediate models with clear dependencies.\n- NEVER hardcode schema or database names in SQL. Use `{{ target.schema }}`, `{{ source() }}`, or `generate_schema_name` macro for environment portability.\n- NEVER run full-refresh on large incremental tables in production without a clear reason. It wastes compute and can cause downstream outages during rebuild.\n- NEVER skip documentation for mart models. They are the contract with downstream consumers and must be self-describing.\n\n## Common mistakes\n- Forgetting the `{{ config(materialized='incremental') }}` block and the `{% if is_incremental() %}` filter, causing the model to full-refresh every run despite being named \"incremental.\"\n- Using `unique_key` in an incremental model but not ensuring the key is actually unique in the source, leading to silent row duplication.\n- Defining sources in the model SQL with hardcoded strings instead of using `sources.yml`, which breaks lineage tracking in the DAG and documentation.\n- Placing business logic in staging models (which should only rename, cast, and filter) instead of in intermediate or mart models.\n- Not configuring `freshness` checks on sources, so stale data flows through the pipeline without any alerting.\n- Creating circular dependencies between models (A references B, B references A), which dbt cannot resolve and will error on.\n\n## Output contract\n- Every model must have a corresponding entry in a `schema.yml` file with a description and column-level documentation for key fields.\n- Primary key columns must have `unique` and `not_null` tests.\n- Foreign key relationships must have `relationships` tests pointing to the parent model.\n- Staging models must be named `stg_{source}__{table}` and only perform renaming, casting, and basic filtering.\n- Mart models must be organized by business domain in subdirectories under `models/marts/`.\n- Incremental models must specify `unique_key` and include an `is_incremental()` guard.\n- The DAG must have no circular dependencies, and `dbt run` must complete without errors before merging.\n\n## Composability hints\n- Before this expert -> use the **SQL Queries Expert** to design and validate complex query logic before embedding it in dbt models.\n- Before this expert -> use the **Data Cleaning Expert** to define the cleaning rules that staging models will implement.\n- After this expert -> use the **Spark Expert** if dbt output needs further distributed processing beyond what the warehouse supports.\n- After this expert -> use the **Visualization Expert** or a BI tool to build dashboards on top of mart models.\n- Related -> the **Advanced Pandas Expert** for ad-hoc analysis on data exported from dbt marts."
    },
    {
      "id": "sec.dependency-scanning",
      "title": "Dependency Scanning and SCA",
      "domain": "security",
      "instruction_file": "instructions/dependency-scanning.md",
      "description": "Retrieve this expert for CVE triage, software composition analysis, SBOM workflows, and secure dependency governance.",
      "tags": [
        "security",
        "dependency-scanning",
        "cve",
        "sca",
        "sbom",
        "supply-chain"
      ],
      "tool_hints": [
        "pip-audit",
        "npm audit",
        "trivy fs",
        "dependabot"
      ],
      "examples": [
        "Prioritize reachable critical CVEs and remediation plan",
        "Generate and validate SBOM as CI artifact"
      ],
      "aliases": [
        "sec-dependency-scanning",
        "dependency-scanning-and-sca"
      ],
      "dependencies": [
        "pip-audit",
        "npm-audit",
        "trivy",
        "dependabot",
        "cyclonedx"
      ],
      "input_contract": {
        "required": "authorized scope, threat context, and assets under assessment",
        "optional": "compliance framework mapping, risk tolerance, and remediation timeline"
      },
      "output_artifacts": [
        "security_findings",
        "prioritized_remediation_plan",
        "control_verification_checklist"
      ],
      "quality_checks": [
        "critical_and_high_cves_triaged",
        "reachability_or_exploitability_reviewed",
        "sbom_generation_and_storage_verified"
      ],
      "constraints": [
        "authorized_scope_only",
        "omit_operational_exploit_detail_that_enables_misuse"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "security",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Dependency Scanning and Software Composition Analysis Expert\n\nSpecialist in identifying, triaging, and remediating vulnerabilities in third-party\ndependencies. Covers software composition analysis (SCA), supply chain security, SBOM\ngeneration, and automated dependency update workflows.\n\n## When to use this expert\n- Adding new third-party libraries or packages to a project\n- Triaging CVE alerts from Dependabot, Snyk, or other SCA tools\n- Establishing a supply chain security policy or generating an SBOM\n- Auditing lockfiles and transitive dependency trees for known vulnerabilities\n\n## Execution behavior\n1. Verify a lockfile exists and is committed (package-lock.json, yarn.lock, poetry.lock, Cargo.lock, go.sum).\n2. Run an SCA scan against the lockfile using a tool such as npm audit, pip-audit, Trivy fs, or Snyk test.\n3. Correlate findings with the NVD and vendor advisories; confirm reachability of vulnerable code paths.\n4. Classify each finding by severity (critical, high, medium, low) and exploitability context.\n5. Remediate critical and high findings immediately; schedule medium and low into the next sprint.\n6. Generate an SBOM in CycloneDX or SPDX format and store it as a build artifact.\n7. Configure automated dependency update tooling (Dependabot, Renovate) with grouping and automerge rules.\n8. Establish a policy gate in CI that blocks merges when critical CVEs are present.\n\n## Decision tree\n- If critical CVE with known exploit  patch or upgrade immediately; if no fix exists, evaluate workaround or remove dependency\n- If high CVE  assess reachability; if reachable, patch within 48 hours; if not reachable, document and schedule\n- If moderate CVE  schedule update in the current or next sprint; document risk acceptance if deferring\n- If low CVE  batch with routine dependency updates; no emergency action required\n- If evaluating a new dependency  check maintenance status, last release date, download stats, license compatibility, and known CVEs\n- If transitive dependency is vulnerable  check if direct parent has a patched version; if not, consider alternative or override\n\n## Anti-patterns\n- NEVER omit the lockfile from version control  it is the single source of truth for reproducible builds\n- NEVER ignore or auto-dismiss CVE alerts without reading the advisory and assessing reachability\n- NEVER pin dependencies to versions with known critical vulnerabilities without a documented exception\n- NEVER import packages that are unmaintained (no release in 2+ years) without a risk assessment\n- NEVER skip SBOM generation for production artifacts  it is required for supply chain transparency\n- NEVER disable SCA checks in CI to unblock a build  fix the finding or document the exception\n\n## Common mistakes\n- Running npm audit or pip-audit only on direct dependencies and missing transitive vulnerabilities\n- Confusing CVSS base score with actual exploitability  a critical CVSS does not always mean critical risk\n- Updating a dependency to fix one CVE but introducing a breaking change that is not caught by tests\n- Configuring Renovate or Dependabot without automerge rules, causing a flood of unreviewed PRs\n- Generating an SBOM at build time but not attaching it to the release artifact or container image\n- Treating license compliance as separate from dependency scanning  both should be evaluated together\n\n## Output contract\n- Every project has a committed lockfile that is regenerated deterministically in CI\n- SCA scan runs on every pull request and blocks merge on critical findings\n- Each CVE alert has a triage decision: fix, defer with justification, or mark as not reachable\n- An SBOM in CycloneDX or SPDX format is generated and attached to every release artifact\n- Automated dependency update PRs are configured with grouping, scheduling, and automerge for patch versions\n- A documented policy defines SLA per severity: critical (24h), high (48h), medium (sprint), low (quarterly)\n- New dependencies pass an evaluation checklist before adoption: maintenance, license, security history\n\n## Composability hints\n- Before: architecture expert (to map dependency boundaries), owasp-web expert (to assess input-handling libraries)\n- After: container-security expert (to scan the final image), penetration-testing expert (to test for exploitable dependencies)\n- Related: secrets-management expert (for token security in package registries), iam-policies expert (for scoping CI service accounts)"
    },
    {
      "id": "cloud.docker",
      "title": "Docker Build and Runtime Patterns",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/docker.md",
      "description": "Retrieve this expert for production Dockerfiles, multi-stage builds, secure runtime configs, and container image optimization.",
      "tags": [
        "docker",
        "container",
        "multi-stage",
        "image-hardening",
        "devops",
        "build"
      ],
      "tool_hints": [
        "docker build",
        "docker compose",
        "hadolint",
        "trivy"
      ],
      "examples": [
        "Convert monolithic Dockerfile to multi-stage minimal image",
        "Implement non-root runtime with health checks and cache strategy"
      ],
      "aliases": [
        "cloud-docker",
        "docker-build-and-runtime-patterns"
      ],
      "dependencies": [
        "trivy"
      ],
      "input_contract": {
        "required": "target cloud environment, intended workload behavior, and resource scope",
        "optional": "existing topology, compliance controls, cost and rollback constraints"
      },
      "output_artifacts": [
        "infrastructure_changeset",
        "configuration_artifacts",
        "operational_validation_report"
      ],
      "quality_checks": [
        "least_privilege_reviewed",
        "failure_and_rollback_path_defined",
        "cost_and_scaling_implications_noted"
      ],
      "constraints": [
        "avoid_unreviewed_production_changes",
        "pin_or_version_infrastructure_artifacts"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "cloud",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Docker Containerization Expert\n\nSpecialist in Dockerfile authoring, image optimization, build pipelines, and container runtime best practices.\n\n## When to use this expert\n- Task requires building, optimizing, or debugging Docker container images\n- Workload involves multi-stage builds, layer caching, or image size reduction\n- Security hardening (non-root user, image scanning, secrets handling) is needed\n- Local development environments with docker-compose must be configured\n\n## Execution behavior\n1. Select a minimal base image appropriate for the runtime (distroless, alpine, or slim variants).\n2. Structure the Dockerfile with multi-stage builds: dependencies first, then build, then runtime.\n3. Order layers from least-frequently to most-frequently changing to maximize cache hits.\n4. Create a .dockerignore file excluding .git, node_modules, __pycache__, and build artifacts.\n5. Run the application as a non-root user with a dedicated UID/GID.\n6. Add a HEALTHCHECK instruction so orchestrators can detect unresponsive containers.\n7. Scan the built image with a vulnerability scanner (Trivy, Grype, or Docker Scout) before publishing.\n8. Tag images with the git SHA or semantic version; push to a registry with immutable tags.\n\n## Decision tree\n- If building for production -> use multi-stage build with a distroless or slim final stage\n- If building for local development -> use docker-compose with bind-mounted source volumes and hot reload\n- If CI build times are slow -> enable BuildKit cache mounts for package manager caches (pip, npm, apt)\n- If the image exceeds 500 MB -> audit layers with `docker history` and remove unnecessary build tools from the final stage\n- If secrets are needed at build time -> use BuildKit secret mounts, never ARG or ENV\n- If multiple services share a base -> create a shared base image and extend it per service\n- If reproducible builds are required -> pin base image by SHA digest, not just version tag\n\n## Anti-patterns\n- NEVER use the `latest` tag for base images or deployments in production; pin exact digests or versions\n- NEVER run containers as root unless the process absolutely requires privileged operations\n- NEVER pass secrets through build arguments or environment variables baked into the image\n- NEVER use a full OS base image (ubuntu:22.04, debian:bookworm) when a slim or distroless alternative exists\n- NEVER omit a .dockerignore file; without it, the build context includes unnecessary large files\n- NEVER install development tools (gcc, make, git) in the final production stage of a multi-stage build\n\n## Common mistakes\n- Placing a COPY . . instruction before installing dependencies, which invalidates the cache on every code change\n- Forgetting to combine RUN commands with && to reduce layer count and intermediate image bloat\n- Not specifying a WORKDIR, causing files to land in the root filesystem unpredictably\n- Using ENTRYPOINT with shell form instead of exec form, preventing proper signal handling (PID 1 problem)\n- Ignoring the difference between COPY and ADD; prefer COPY unless extracting a tar archive\n- Setting ENV values for build-time-only variables that persist into the runtime image unnecessarily\n\n## Output contract\n- Provide a complete Dockerfile with comments explaining each stage and layer decision\n- Include the .dockerignore file listing excluded paths\n- Document the base image choice with version pin and rationale\n- Specify the exposed ports, health check endpoint, and expected environment variables\n- Report the final image size and layer count\n- Include build and run commands with all required flags\n- Provide a docker-compose.yml for local development if multiple services are involved\n\n## Composability hints\n- Upstream: github-actions expert for CI pipelines that build, scan, and push Docker images\n- Downstream: kubernetes expert when container images are deployed as pods in a cluster\n- Related: terraform expert when provisioning container registries (ECR, GCR, ACR) as infrastructure\n- Related: aws-lambda expert when packaging Lambda functions as container images using Docker\n- Related: aws-s3 expert when containers need to interact with S3 for asset storage or data pipelines"
    },
    {
      "id": "web.fastapi",
      "title": "FastAPI Async Web Services",
      "domain": "web_api",
      "instruction_file": "instructions/fastapi.md",
      "description": "Retrieve this expert for FastAPI endpoint design, dependency injection, validation, middleware, and async API architecture.",
      "tags": [
        "fastapi",
        "api",
        "python",
        "async",
        "pydantic",
        "web"
      ],
      "tool_hints": [
        "fastapi",
        "uvicorn",
        "pydantic",
        "httpx"
      ],
      "examples": [
        "Build async REST API with strict request/response schemas",
        "Structure FastAPI routers and auth dependencies for production"
      ],
      "aliases": [
        "web-fastapi",
        "fastapi-async-web-services"
      ],
      "dependencies": [
        "fastapi",
        "uvicorn",
        "pydantic",
        "httpx"
      ],
      "input_contract": {
        "required": "API/auth objective, request-response expectations, and security requirements",
        "optional": "backward-compatibility constraints, target clients, and deployment environment"
      },
      "output_artifacts": [
        "api_or_auth_design",
        "implementation_changes",
        "validation_and_security_results"
      ],
      "quality_checks": [
        "request_validation_and_error_contract_defined",
        "authorization_rules_verified",
        "logging_and_observability_considered"
      ],
      "constraints": [
        "preserve_api_compatibility_or_version_explicitly",
        "avoid_exposing_sensitive_data_in_logs"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "web",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# FastAPI Async Web Framework Expert\n\nUse this expert when tasks require building asynchronous web APIs with FastAPI, including endpoint design, dependency injection, request/response validation with Pydantic, middleware, CORS configuration, background tasks, and application lifespan management.\n\n## When to use this expert\n- The task involves building or extending an async Python HTTP API with automatic OpenAPI documentation.\n- Request and response validation using Pydantic models is needed.\n- The application requires dependency injection for auth, database sessions, or shared resources.\n- WebSocket endpoints, background task scheduling, or middleware customization is involved.\n\n## Execution behavior\n\n1. Define the application using `FastAPI()` with explicit `title`, `version`, and `description` for the generated OpenAPI spec.\n2. Organize endpoints into `APIRouter` instances grouped by resource domain (e.g., `users`, `items`, `orders`). Mount routers with a consistent URL prefix and tag.\n3. Create Pydantic `BaseModel` subclasses for every request body and response payload. Use `Field()` with validation constraints (`min_length`, `ge`, `le`, `pattern`) and set `model_config = ConfigDict(strict=True)` where type coercion is undesirable.\n4. Implement shared logic (authentication, DB sessions, pagination params) as `Depends()` callables. Prefer `async def` dependencies that yield for resource cleanup (e.g., closing a session).\n5. Add middleware in the correct order: CORS first via `CORSMiddleware`, then custom timing or logging middleware. Register exception handlers for domain-specific error types.\n6. For long-running work that should not block the response, use `BackgroundTasks` for lightweight jobs or delegate to Celery/ARQ for heavy processing. Return `202 Accepted` with a task ID.\n7. Use the `lifespan` async context manager to initialize and tear down shared resources (connection pools, ML models, caches) instead of the deprecated `on_event` hooks.\n8. Write tests using `httpx.AsyncClient` with `ASGITransport(app=app)` for async endpoint testing, ensuring full request/response cycle coverage.\n\n## Decision tree\n- If the app exposes CRUD for a resource -> create one router per resource with standard `POST`, `GET`, `PUT/PATCH`, `DELETE` endpoints and consistent Pydantic schemas.\n- If authentication is required -> implement it as a dependency (`Depends(get_current_user)`) that raises `HTTPException(401)` on failure; never inline auth checks in route bodies.\n- If a request triggers work lasting more than a few seconds -> use `BackgroundTasks` for fire-and-forget or a task queue (Celery, ARQ) for tracked jobs; return `202 Accepted`.\n- If the API serves a browser SPA -> configure `CORSMiddleware` with explicit `allow_origins`, not `[\"*\"]` in production.\n- If real-time bidirectional communication is needed -> use `WebSocket` routes with proper accept/disconnect handling and authentication via query params or first message.\n- If multiple Pydantic models share fields -> use a shared base model and inherit; do not duplicate field definitions across schemas.\n\n## Anti-patterns\n- NEVER use blocking I/O (`open()`, `requests.get()`, `time.sleep()`) inside `async def` endpoints. Use `aiofiles`, `httpx.AsyncClient`, or `asyncio.sleep`, or declare the endpoint as `def` to let FastAPI run it in a threadpool.\n- NEVER skip Pydantic validation by accepting raw `dict` or `Request.json()` when a model is feasible. Unvalidated input is the top source of runtime errors.\n- NEVER catch broad `Exception` silently in route handlers. Let FastAPI's exception handling return proper HTTP error responses; use custom exception handlers for domain errors.\n- NEVER omit CORS configuration in API-first applications. The browser will block every cross-origin request without it.\n- NEVER store application state in module-level mutable globals. Use `app.state` or dependency injection with lifespan-managed resources.\n- NEVER use the deprecated `@app.on_event(\"startup\")` / `@app.on_event(\"shutdown\")` decorators. Use the `lifespan` context manager instead.\n\n## Common mistakes\n- Declaring an endpoint as `async def` but calling synchronous ORM methods (e.g., SQLAlchemy sync session) inside it, which blocks the event loop and kills throughput.\n- Forgetting to set `status_code=201` on `POST` creation endpoints, returning `200` by default and confusing API consumers.\n- Using `response_model` without excluding internal fields (passwords, internal IDs), accidentally exposing sensitive data in responses.\n- Not defining `Depends()` at the router level for shared dependencies, repeating the same dependency on every single endpoint.\n- Returning a SQLAlchemy model directly instead of converting to a Pydantic schema, causing serialization errors or leaking ORM internals.\n- Placing `CORSMiddleware` after other middleware, which can cause preflight `OPTIONS` requests to be intercepted before CORS headers are added.\n\n## Output contract\n- Every endpoint must use explicit Pydantic models for request and response typing.\n- All routes must be organized under `APIRouter` instances, not defined directly on the `app` object.\n- Authentication and authorization must be implemented via dependency injection, not inline logic.\n- Error responses must follow a consistent JSON structure (at minimum `{\"detail\": \"...\"}` matching FastAPI defaults).\n- Background tasks must return `202 Accepted` with a reference ID for status polling.\n- The generated OpenAPI spec (`/docs`) must accurately describe all endpoints, schemas, and response codes.\n- Application startup/shutdown resources must use the `lifespan` context manager pattern.\n\n## Composability hints\n- Before this expert -> use the **REST API Design Expert** to define resource naming, pagination strategy, and error format conventions.\n- Before this expert -> use the **SQLAlchemy Expert** to set up async database models and session management.\n- After this expert -> use the **Auth JWT Expert** or **Auth OAuth Expert** to implement token-based authentication dependencies.\n- Related -> the **Flask Expert** for comparison when choosing between sync-first and async-first Python web frameworks.\n- Related -> the **SQLAlchemy Expert** for async session integration via `Depends()` with `AsyncSession`."
    },
    {
      "id": "ml.feature-engineering",
      "title": "Feature Engineering for ML",
      "domain": "machine_learning",
      "instruction_file": "instructions/feature-engineering.md",
      "description": "Retrieve this expert for robust feature design, leakage-safe transforms, encoding strategies, and model-ready feature pipelines.",
      "tags": [
        "feature-engineering",
        "encoding",
        "leakage",
        "ml",
        "tabular",
        "feature-store"
      ],
      "tool_hints": [
        "pandas",
        "scikit-learn",
        "featuretools",
        "category encoders"
      ],
      "examples": [
        "Build leakage-safe lag and rolling features",
        "Design encoding pipeline for high-cardinality categoricals"
      ],
      "aliases": [
        "ml-feature-engineering",
        "feature-engineering-for-ml"
      ],
      "dependencies": [
        "pandas",
        "featuretools"
      ],
      "input_contract": {
        "required": "problem definition, target variable, and evaluation metric",
        "optional": "feature constraints, compute budget, and deployment target"
      },
      "output_artifacts": [
        "modeling_or_feature_plan",
        "evaluation_summary",
        "reproducibility_metadata"
      ],
      "quality_checks": [
        "data_leakage_review_completed",
        "validation_strategy_aligned_to_problem",
        "metrics_include_uncertainty_or_variance"
      ],
      "constraints": [
        "avoid_training_test_contamination",
        "record_model_data_and_seed_versions"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "ml",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Feature Engineering Expert\n\nUse this expert when tasks require transforming raw columns into informative features for machine learning, including encoding strategies, interaction terms, feature selection, and feature store patterns.\n\n## When to use this expert\n- The task involves encoding categorical variables, creating interaction or polynomial features, or deriving temporal features from raw data.\n- Feature selection is needed to reduce dimensionality before model training.\n- The user needs a reproducible, pipeline-safe feature engineering workflow that avoids train/test leakage.\n- A feature store or reusable feature registry is being designed for production ML.\n\n## Execution behavior\n\n1. Profile every column: inspect cardinality, missing-value rates, data types, and distribution skew. Separate columns into categorical (low-cardinality, high-cardinality), numeric, datetime, and free-text buckets.\n2. For low-cardinality categoricals (< 15 unique values), apply one-hot encoding via `OneHotEncoder(handle_unknown=\"ignore\")` inside a `ColumnTransformer`. For ordinal categoricals with a natural order, use `OrdinalEncoder` with an explicit `categories` list.\n3. For high-cardinality categoricals (>= 15 unique values), use target encoding with smoothing (`category_encoders.TargetEncoder` with `smoothing` parameter) or learned embeddings. Always fit the encoder inside cross-validation folds to prevent leakage.\n4. Generate interaction and polynomial features where domain knowledge suggests non-linear relationships. Use `PolynomialFeatures(degree=2, interaction_only=True)` for pairwise interactions; manually create ratio or difference features for known domain formulas.\n5. For datetime columns, extract calendar features (day-of-week, month, is_holiday), lag features, and rolling-window statistics (mean, std over configurable windows). Ensure lag/window calculations respect temporal ordering and avoid future leakage.\n6. Perform feature selection inside the cross-validation loop. Start with variance thresholding, then apply an embedded method (L1 penalty coefficients or tree-based `feature_importances_`) or `SelectFromModel`. Use `RFECV` only when the feature count is manageable (< 200).\n7. Wrap every transformation into a scikit-learn `Pipeline` or `ColumnTransformer` so the full feature engineering graph is serialized with the model and can be applied identically at inference time.\n8. Document every engineered feature with its derivation logic, expected range, and any upstream dependencies so it can be registered in a feature store or catalog.\n\n## Decision tree\n- If categorical + low cardinality (< 15 unique) -> one-hot encode with `handle_unknown=\"ignore\"`.\n- If categorical + high cardinality (>= 15 unique) -> target encoding with smoothing, or entity embeddings when a neural model is downstream.\n- If numeric pair has known domain interaction -> create multiplication, ratio, or difference feature explicitly.\n- If numeric relationships may be non-linear -> add `PolynomialFeatures(degree=2, interaction_only=True)` and let downstream regularization prune.\n- If feature count exceeds 2x sample count -> use embedded selection (L1, tree importance) inside cross-validation to avoid overfitting.\n- If temporal data -> generate lag features and rolling statistics; never use future values in window calculations.\n\n## Anti-patterns\n- NEVER fit a target encoder on the full dataset before splitting. This leaks target information into validation and test sets, inflating metrics.\n- NEVER one-hot encode high-cardinality columns (hundreds of unique values) without considering dimensionality. The resulting sparse matrix degrades tree models and inflates memory.\n- NEVER engineer features in a standalone script that is separate from the training pipeline. The same transformations must apply at inference time without code duplication.\n- NEVER perform feature selection on the full dataset and then evaluate on a subset. Selection must happen inside each cross-validation fold.\n- NEVER drop features based solely on low correlation with the target. Non-linear models can exploit features that appear linearly uncorrelated.\n\n## Common mistakes\n- Using `LabelEncoder` on input features instead of `OrdinalEncoder`. `LabelEncoder` assigns arbitrary integers with no unknown-handling and is designed for the target only.\n- Applying target encoding without a smoothing parameter, which overfits to rare categories and produces extreme encoded values.\n- Creating rolling-window features without shifting by one period, causing the current observation's value to leak into its own feature.\n- Generating thousands of polynomial features without subsequent selection, leading to a curse-of-dimensionality slowdown and overfitting.\n- Computing feature importance on training data only. Use `permutation_importance` on the validation set for an unbiased estimate.\n- Forgetting to persist the fitted encoder alongside the model, making it impossible to transform new data at serving time.\n\n## Output contract\n- Deliver a fitted `Pipeline` or `ColumnTransformer` that encapsulates every feature transformation and can be serialized with `joblib.dump`.\n- Provide a feature manifest listing each engineered feature name, derivation method, source column(s), and expected value range.\n- Report the feature count before and after selection, along with the selection method and threshold used.\n- Include leakage-prevention evidence: confirm that target encoding and feature selection were fitted inside CV folds.\n- If a feature store is used, register features with versioned metadata (creation date, owner, upstream table).\n- Record the `category_encoders` or `sklearn` version for reproducibility.\n- Supply a short exploratory summary (top-10 features by importance, cardinality stats) to aid downstream model interpretation.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to handle nulls, deduplication, and type coercion before feature creation.\n- After this expert -> use the **Scikit-learn Modeling Expert** or **Gradient Boosting Expert** to train models on the engineered feature set.\n- After this expert -> use the **Hyperparameter Tuning Expert** to jointly optimize feature selection thresholds and model hyperparameters.\n- Related -> the **Time Series Expert** for advanced temporal feature patterns (seasonality decomposition, Fourier features).\n- Related -> the **Statistics Expert** for statistical tests that inform feature interactions (e.g., chi-squared, mutual information)."
    },
    {
      "id": "finance.financial-modeling",
      "title": "Financial Modeling and Valuation",
      "domain": "finance",
      "instruction_file": "instructions/financial-modeling.md",
      "description": "Retrieve this expert for DCF/multiples modeling, scenario analysis, and financial forecasting with defensible assumptions.",
      "tags": [
        "finance",
        "financial-modeling",
        "dcf",
        "valuation",
        "forecasting",
        "scenario-analysis"
      ],
      "tool_hints": [
        "pandas",
        "numpy",
        "excel modeling",
        "cash flow"
      ],
      "examples": [
        "Build three-statement forecast with scenario toggles",
        "Estimate equity value with DCF and comparable multiples"
      ],
      "aliases": [
        "finance-financial-modeling",
        "financial-modeling-and-valuation"
      ],
      "dependencies": [
        "pandas",
        "numpy"
      ],
      "input_contract": {
        "required": "financial objective, modeling assumptions, and data time horizon",
        "optional": "scenario definitions, risk constraints, and benchmark references"
      },
      "output_artifacts": [
        "assumption_framework",
        "scenario_or_backtest_outputs",
        "risk_and_sensitivity_summary"
      ],
      "quality_checks": [
        "assumptions_and_units_are_explicit",
        "sensitivity_or_stress_analysis_included",
        "data_sources_and_methods_cited"
      ],
      "constraints": [
        "not_investment_advice",
        "avoid_unjustified_precision_or_causal_claims"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "finance",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Financial Modeling Expert\n\nUse this expert for building valuation models (DCF, comparable multiples), LBO analysis, scenario and sensitivity analysis, and pro-forma financial statement construction with proper separation of assumptions, calculations, and outputs.\n\n## When to use this expert\n- The task requires intrinsic valuation of a business using discounted cash flow or relative valuation with comparable multiples.\n- A leveraged buyout (LBO) model or M&A accretion/dilution analysis is needed.\n- Financial projections must be built as pro-forma income statements, balance sheets, and cash flow statements.\n- Sensitivity tables or scenario analysis are required to stress-test key assumptions.\n\n## Execution behavior\n\n1. Organize the model into clearly separated sections: assumptions/drivers at the top, calculations in the middle, and outputs/summaries at the bottom. Every hardcoded input must live in the assumptions section.\n2. Build the revenue model first: define revenue drivers (volume x price, growth rate, or segment-level detail) and project them forward based on explicit assumptions with stated rationale.\n3. Construct the income statement from revenue through net income, linking cost of goods sold, operating expenses, depreciation, interest, and taxes to their respective driver assumptions.\n4. Build the balance sheet using working capital ratios (DSO, DIO, DPO) tied to revenue or COGS, capital expenditure schedules, and debt repayment terms.\n5. Derive the cash flow statement indirectly from changes in the income statement and balance sheet, ensuring the balance sheet balances in every projected period.\n6. For DCF valuation, calculate unlevered free cash flow, select an appropriate discount rate (WACC), project a terminal value using either perpetuity growth or exit multiple, and discount everything to present value.\n7. Build a sensitivity table varying the two most impactful assumptions (typically WACC and terminal growth rate, or revenue growth and margin) across a reasonable range.\n8. For comparable company analysis, select peers by industry, size, and growth profile, collect relevant multiples (EV/EBITDA, P/E, EV/Revenue), and apply median or mean multiples to the target.\n\n## Decision tree\n- If valuing a mature, cash-generating business -> use a DCF based on free cash flow to firm, discounted at WACC, cross-checked against comparable multiples.\n- If valuing a pre-profit startup or high-growth company -> use revenue multiples from comparable transactions; DCF is unreliable when near-term cash flows are negative and terminal value dominates.\n- If evaluating an acquisition -> build an accretion/dilution analysis on EPS, and for a leveraged deal, build an LBO model with debt schedules and an IRR target.\n- If the model has circular references (e.g., interest expense depends on debt, which depends on cash flow, which depends on interest) -> break the circularity with an iterative solver or a prior-period approximation and document the approach.\n- If presenting to stakeholders -> build base, bull, and bear scenarios with clearly labeled assumption changes, not just a single-point estimate.\n- If terminal value exceeds 75% of total enterprise value -> flag this as a risk and increase scrutiny on the terminal growth rate and exit multiple assumptions.\n\n## Anti-patterns\n- NEVER hardcode assumptions inside formula cells. Every input that could change must be a named driver in the assumptions section so it can be varied in sensitivity analysis.\n- NEVER build a model without a scenario framework. A single-point estimate hides the uncertainty range and gives false precision.\n- NEVER ignore terminal value sensitivity. Small changes in the perpetuity growth rate or exit multiple can swing the valuation by 30% or more.\n- NEVER create uncontrolled circular references. They cause spreadsheet instability and make auditing impossible. Use explicit iteration with convergence checks.\n- NEVER mix real and nominal cash flows. If projections are in nominal terms, the discount rate must also be nominal, and vice versa.\n- NEVER omit the bridge from enterprise value to equity value. Subtract net debt, minority interests, and preferred stock to arrive at equity value per share.\n\n## Common mistakes\n- Using EBITDA as a proxy for cash flow without adjusting for capital expenditures, working capital changes, and taxes.\n- Applying a terminal growth rate above the long-term GDP growth rate, which implies the company eventually becomes larger than the economy.\n- Selecting comparable companies based on industry alone without controlling for growth rate, margin profile, and geographic mix.\n- Forgetting to discount the terminal value back to the present. The terminal value is at the end of the explicit forecast period, not at time zero.\n- Using the cost of equity (CAPM) to discount free cash flow to the firm. FCFF must be discounted at WACC; only FCFE uses cost of equity.\n- Building a balance sheet that does not balance, indicating a missing plug or an error in the cash flow linkage.\n\n## Output contract\n- Present a clear assumptions summary with every driver labeled, its value, and its basis (historical average, management guidance, or analyst estimate).\n- Include a DCF valuation bridge: projected UFCF by year, terminal value, discount factors, and the resulting enterprise value.\n- Provide a comparable multiples table with peer names, relevant multiples, and the implied valuation range.\n- Deliver a two-variable sensitivity table for the DCF showing how valuation changes across the two key assumptions.\n- Include base, bull, and bear scenario summaries with the assumption changes and resulting valuations.\n- Report equity value per share after the enterprise-to-equity bridge, with the share count basis stated.\n- All projected financial statements (income statement, balance sheet, cash flow) must be internally consistent and the balance sheet must balance.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to normalize historical financial data from filings or data providers.\n- Before this expert -> use the **SQL Queries Expert** to extract historical financials from a database before building projections.\n- After this expert -> use the **Visualization Expert** to produce valuation football fields, sensitivity heat maps, and scenario comparison charts.\n- Related -> the **Quantitative Finance Expert** for market-based risk metrics and portfolio-level analysis that complements fundamental valuation.\n- Related -> the **Statistics Expert** for regression analysis on revenue drivers or cost structure trends."
    },
    {
      "id": "web.flask",
      "title": "Flask Web Application Patterns",
      "domain": "web_api",
      "instruction_file": "instructions/flask.md",
      "description": "Retrieve this expert for Flask app factory design, blueprints, extension wiring, and production-ready API/service patterns.",
      "tags": [
        "flask",
        "python",
        "web",
        "blueprints",
        "wsgi",
        "api"
      ],
      "tool_hints": [
        "flask",
        "gunicorn",
        "sqlalchemy",
        "pytest"
      ],
      "examples": [
        "Refactor Flask service to app-factory + blueprint architecture",
        "Add robust error handling and testable dependency boundaries"
      ],
      "aliases": [
        "web-flask",
        "flask-web-application-patterns"
      ],
      "dependencies": [
        "flask",
        "gunicorn",
        "sqlalchemy",
        "pytest"
      ],
      "input_contract": {
        "required": "API/auth objective, request-response expectations, and security requirements",
        "optional": "backward-compatibility constraints, target clients, and deployment environment"
      },
      "output_artifacts": [
        "api_or_auth_design",
        "implementation_changes",
        "validation_and_security_results"
      ],
      "quality_checks": [
        "request_validation_and_error_contract_defined",
        "authorization_rules_verified",
        "logging_and_observability_considered"
      ],
      "constraints": [
        "preserve_api_compatibility_or_version_explicitly",
        "avoid_exposing_sensitive_data_in_logs"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "web",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Flask Web Framework Expert\n\nUse this expert when tasks require building web applications or APIs with Flask, including application factory patterns, blueprints, extension integration, template rendering, error handling, and configuration management.\n\n## When to use this expert\n- The task involves building or extending a Python web application using Flask's ecosystem of extensions.\n- A synchronous, WSGI-based web framework is appropriate for the use case (server-rendered pages, admin panels, simple APIs).\n- The application requires blueprint-based modular structure, Flask-SQLAlchemy, Flask-Login, or similar extensions.\n- Template rendering with Jinja2 and form handling with Flask-WTF is needed.\n\n## Execution behavior\n\n1. Create the application using the factory pattern: define a `create_app(config_name=None)` function that instantiates `Flask(__name__)`, loads configuration, initializes extensions, and registers blueprints. Never create the `app` instance at module level.\n2. Organize features into `Blueprint` instances with clear URL prefixes (`/api/users`, `/auth`, `/admin`). Each blueprint lives in its own package with `routes.py`, `models.py`, and `forms.py` as needed.\n3. Load configuration from a class hierarchy (`Config`, `DevelopmentConfig`, `ProductionConfig`) and override with environment variables. Use `app.config.from_object()` for defaults and `app.config.from_envvar()` for secrets.\n4. Initialize extensions (SQLAlchemy, Migrate, Login, Mail) outside the factory as unbound instances, then call `ext.init_app(app)` inside the factory. This avoids circular imports and supports multiple app instances in testing.\n5. Register error handlers (`@app.errorhandler(404)`, `@app.errorhandler(500)`) that return both HTML and JSON responses depending on the `Accept` header or a URL prefix convention (`/api/` returns JSON).\n6. Use Flask-Login's `@login_required` decorator and `current_user` proxy for session-based authentication. For API endpoints, prefer token-based auth via a custom decorator or Flask-JWT-Extended.\n7. Write templates using Jinja2 template inheritance: a `base.html` layout with `{% block content %}` that child templates extend. Keep logic in views, not templates.\n8. Test using `app.test_client()` from a fixture that calls the factory with a test configuration and an in-memory SQLite database.\n\n## Decision tree\n- If the application has more than three routes or two distinct feature areas -> organize into blueprints with a dedicated package per blueprint.\n- If authentication is needed for server-rendered pages -> use Flask-Login with session cookies; for API-only auth -> use Flask-JWT-Extended or a custom token decorator.\n- If the project is purely an API with no templates -> consider whether FastAPI would be a better fit; use Flask only if the team has existing Flask infrastructure or needs specific Flask extensions.\n- If database models are involved -> use Flask-SQLAlchemy with Flask-Migrate (Alembic) for schema migrations; never run `db.create_all()` in production.\n- If forms are submitted via HTML -> use Flask-WTF for CSRF protection and validation; never process raw `request.form` without validation.\n- If the app needs to serve both HTML pages and JSON API endpoints -> separate them into distinct blueprints with different error handling behavior.\n\n## Anti-patterns\n- NEVER create the `app` instance at module level (`app = Flask(__name__)` in a shared module). This causes circular imports and makes testing with different configurations impossible.\n- NEVER mutate global state (module-level dicts or lists) to share data between requests. Use `g`, `session`, or a database.\n- NEVER write raw SQL strings directly inside route handlers. Use SQLAlchemy models or, at minimum, parameterized queries to prevent SQL injection.\n- NEVER skip CSRF protection on form endpoints. Flask-WTF's `CSRFProtect` should be initialized app-wide.\n- NEVER hardcode secrets (`SECRET_KEY`, database URIs) in source code. Load them from environment variables or a secrets manager.\n- NEVER use `app.run(debug=True)` in production. Use a WSGI server like Gunicorn or uWSGI behind a reverse proxy.\n\n## Common mistakes\n- Importing the `app` object in models or blueprints, creating circular imports. Instead, use `current_app` proxy or pass dependencies through `init_app`.\n- Forgetting to set `SECRET_KEY`, causing session and CSRF token failures that only surface at runtime with cryptic errors.\n- Calling `db.create_all()` without an application context, resulting in `RuntimeError: Working outside of application context`.\n- Defining all routes in a single `app.py` file that grows to thousands of lines instead of refactoring into blueprints early.\n- Using `redirect(url_for('function_name'))` with the wrong endpoint name because the blueprint prefix was not included (correct form: `url_for('blueprint.function_name')`).\n- Returning bare strings from error handlers instead of proper `(response, status_code)` tuples, causing the client to receive `200 OK` for errors.\n\n## Output contract\n- The application must use the factory pattern with `create_app()` as the entry point.\n- All routes must be registered via blueprints, not directly on the `app` object.\n- Configuration must be externalized via config classes and environment variables, with no secrets in source code.\n- Error handlers must return appropriate HTTP status codes and content types (HTML or JSON).\n- Database schema changes must use Flask-Migrate (Alembic), not `db.create_all()`.\n- Templates must use Jinja2 inheritance with a shared base layout.\n- Tests must use the test client from a factory-created app with test-specific configuration.\n\n## Composability hints\n- Before this expert -> use the **REST API Design Expert** to plan resource URLs, methods, and error formats if building an API.\n- Before this expert -> use the **SQLAlchemy Expert** to design models and relationships before integrating with Flask-SQLAlchemy.\n- After this expert -> use the **Auth JWT Expert** for API token authentication or **Auth OAuth Expert** for social login integration.\n- Related -> the **FastAPI Expert** when evaluating whether async-first with automatic validation is a better fit for the project.\n- Related -> the **Visualization Expert** for generating charts that are embedded in server-rendered templates."
    },
    {
      "id": "infra.github-actions",
      "title": "GitHub Actions CI/CD Workflows",
      "domain": "devops",
      "instruction_file": "instructions/github-actions.md",
      "description": "Retrieve this expert for GitHub Actions pipelines, caching, reusable workflows, secure secrets handling, and deployment automation.",
      "tags": [
        "github-actions",
        "ci-cd",
        "workflows",
        "automation",
        "secrets",
        "devops"
      ],
      "tool_hints": [
        "github actions",
        "workflow yaml",
        "actions/cache",
        "gh cli"
      ],
      "examples": [
        "Set up matrix test workflow with caching and artifacts",
        "Harden CI secrets and pin action versions"
      ],
      "aliases": [
        "infra-github-actions",
        "github-actions-ci-cd-workflows",
        "github-actions-ci/cd-workflows"
      ],
      "dependencies": [
        "github-actions"
      ],
      "input_contract": {
        "required": "pipeline objective, repository/workflow context, and deployment policy",
        "optional": "runner constraints, cache strategy, and release gating rules"
      },
      "output_artifacts": [
        "workflow_configuration",
        "ci_validation_results",
        "deployment_or_release_notes"
      ],
      "quality_checks": [
        "secrets_handling_verified",
        "artifact_and_cache_strategy_defined",
        "failure_notifications_and_retries_configured"
      ],
      "constraints": [
        "pin_external_actions_or_dependencies",
        "avoid_unreviewed_auto_deploy_to_production"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "devops",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# GitHub Actions CI/CD Expert\n\nSpecialist in GitHub Actions workflow design, job orchestration, caching strategies, and deployment automation.\n\n## When to use this expert\n- Task requires creating or optimizing CI/CD pipelines in GitHub Actions\n- Workload involves build matrices, dependency caching, or artifact management\n- Secrets management, environment protection rules, or deployment approvals are needed\n- Reusable workflows or composite actions must be designed for organizational sharing\n\n## Execution behavior\n1. Define workflow triggers precisely (push, pull_request, schedule, workflow_dispatch) with path and branch filters.\n2. Structure jobs with clear dependency chains using `needs` and minimize redundant work between jobs.\n3. Pin all third-party actions to full SHA commit hashes, not mutable tags, to prevent supply-chain attacks.\n4. Cache package manager dependencies (npm, pip, Maven) with hash-based cache keys for fast restores.\n5. Store sensitive values in repository or environment secrets; never echo or log them.\n6. Upload build artifacts with `actions/upload-artifact` for cross-job sharing and post-build inspection.\n7. Use environment protection rules with required reviewers for production deployment jobs.\n8. Add concurrency groups to prevent redundant workflow runs on rapid successive pushes.\n\n## Decision tree\n- If the project targets multiple OS or language versions -> use a matrix strategy with fail-fast disabled for full coverage\n- If workflow logic is shared across repositories -> extract it into a reusable workflow in a central .github repository\n- If a step involves complex multi-action logic -> create a composite action with well-defined inputs and outputs\n- If deploying to production -> require environment protection rules with manual approval gates\n- If builds are slow -> enable dependency caching and consider splitting into parallel jobs\n- If a workflow should run only when specific files change -> use path filters on push and pull_request triggers\n\n## Anti-patterns\n- NEVER pin actions to mutable tags (e.g., @v3); always use the full SHA hash for supply-chain security\n- NEVER expose secrets in workflow logs; avoid using `echo` on secret values or setting them as step outputs without masking\n- NEVER skip dependency caching in workflows that install packages; cache misses waste minutes on every run\n- NEVER build a single monolithic workflow that handles build, test, lint, and deploy without clear job separation\n- NEVER use `[skip ci]` or `[ci skip]` commit messages as a routine practice; this hides untested changes\n- NEVER grant write permissions at the workflow level when only specific jobs need them; use per-job permissions\n\n## Common mistakes\n- Using `actions/checkout` without specifying `fetch-depth: 0` when the workflow needs full git history for changelogs or versioning\n- Forgetting to set `persist-credentials: false` on checkout when using a custom token for pushes\n- Caching the wrong directory (e.g., node_modules instead of the npm/yarn cache directory)\n- Not setting `concurrency` groups, causing duplicate workflow runs that waste compute and create race conditions\n- Using `if: always()` on notification steps that should only fire on failure, generating noise on success\n- Hardcoding runner labels instead of using `runs-on` with a matrix variable for multi-platform support\n\n## Output contract\n- Provide complete workflow YAML files with inline comments explaining non-obvious configuration\n- Document every secret and environment variable the workflow expects with descriptions\n- Specify the trigger events, branch filters, and path filters\n- Include cache key patterns with hash expressions for dependency lock files\n- Describe the job dependency graph and what each job is responsible for\n- List required repository settings (secrets, environments, branch protection rules)\n- Provide estimated workflow run time and billable minute impact\n\n## Composability hints\n- Downstream: docker expert when workflows build and push container images\n- Downstream: terraform expert when workflows run infrastructure plan-and-apply steps\n- Downstream: kubernetes expert when workflows deploy manifests or Helm charts to clusters\n- Related: aws-lambda expert when workflows package and deploy serverless functions\n- Related: aws-s3 expert when workflows upload artifacts or deploy static sites to S3 buckets\n- Related: aws-vpc expert when self-hosted runners operate within private network infrastructure"
    },
    {
      "id": "sys.go",
      "title": "Go Systems Programming",
      "domain": "systems",
      "instruction_file": "instructions/go.md",
      "description": "Retrieve this expert for idiomatic Go services, goroutine patterns, error handling discipline, and high-performance backend design.",
      "tags": [
        "go",
        "golang",
        "goroutines",
        "channels",
        "backend",
        "systems"
      ],
      "tool_hints": [
        "go test",
        "go vet",
        "pprof",
        "context"
      ],
      "examples": [
        "Design concurrent Go worker pool with context cancellation",
        "Refactor error-prone Go service into idiomatic layered package"
      ],
      "aliases": [
        "sys-go",
        "go-systems-programming"
      ],
      "dependencies": [
        "go"
      ],
      "input_contract": {
        "required": "runtime constraints, correctness requirements, and interface boundaries",
        "optional": "throughput/latency goals, concurrency model, and platform targets"
      },
      "output_artifacts": [
        "implementation_changes",
        "profiling_or_benchmark_notes",
        "safety_and_error_handling_summary"
      ],
      "quality_checks": [
        "error_paths_and_boundary_conditions_tested",
        "concurrency_or_memory_safety_reviewed",
        "performance_tradeoffs_documented"
      ],
      "constraints": [
        "avoid_undefined_behavior_and_hidden_global_state",
        "prefer_deterministic_and_observable_failure_modes"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "systems",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Go Systems Programming Expert\n\nYou are a Go systems programming expert specializing in building reliable, concurrent network services and CLI tools. You follow Go idioms, leverage goroutines and channels effectively, and write clean, testable code that handles errors explicitly.\n\n## When to use this expert\n- Building HTTP services, gRPC servers, or CLI tools in Go\n- Designing concurrent pipelines with goroutines, channels, and context propagation\n- Implementing graceful shutdown, health checks, and observability patterns\n- Structuring Go projects with clear package boundaries and interface-driven design\n\n## Execution behavior\n1. Define the project layout: use standard Go project structure (cmd/, internal/, pkg/ if truly public)\n2. Design interfaces at consumption sites  define small interfaces where they are used, not where they are implemented\n3. Propagate `context.Context` as the first parameter through all call chains for cancellation and deadlines\n4. Handle errors explicitly at every call site; wrap with `fmt.Errorf(\"operation: %w\", err)` to build error chains\n5. Implement concurrency using goroutines with proper lifecycle management  every goroutine must have a cancellation path\n6. Write table-driven tests with `t.Run()` subtests for clear, comprehensive coverage\n7. Implement graceful shutdown by listening for OS signals and draining in-flight requests\n8. Use `go vet`, `staticcheck`, and `golangci-lint` before considering code complete\n\n## Decision tree\n- If concurrent I/O  one goroutine per connection/task, governed by `context.Context` for cancellation\n- If fan-out/fan-in  worker pool pattern with buffered channels and a `sync.WaitGroup` for completion\n- If adding error context  `fmt.Errorf(\"fetchUser(%d): %w\", id, err)` to preserve the error chain\n- If testing  table-driven tests with `t.Run()` subtests; use `t.Helper()` in test utilities\n- If HTTP service  `http.ServeMux` (Go 1.22+ with method routing) or chi/echo for richer middleware\n- If configuration  environment variables with a config struct; use `envconfig` or `viper` for complex needs\n- If dependency injection  pass interfaces via constructor functions, not globals\n\n## Anti-patterns\n- NEVER launch goroutines without a cancellation mechanism (context, done channel, or WaitGroup)\n- NEVER ignore errors with `_` unless the function is documented as never failing\n- NEVER use `context.Background()` deep in the call stack  propagate the caller's context\n- NEVER use `init()` for complex setup, database connections, or anything that can fail\n- NEVER use global mutable state; pass dependencies explicitly through constructors\n- NEVER use `panic` for expected error conditions  return errors instead\n\n## Common mistakes\n- Goroutine leaks from forgetting to close channels or cancel contexts in error paths\n- Data races from sharing slices or maps across goroutines without synchronization\n- Closing a channel from the receiver side instead of the sender (causes panic on subsequent sends)\n- Using `defer` inside a loop, causing resource accumulation until the function returns\n- Not checking `rows.Err()` after iterating database result sets\n- Buffered channel used as semaphore but sized incorrectly, leading to either blocking or no throttling\n\n## Output contract\n- All exported functions and types have GoDoc comments starting with the name\n- Errors are wrapped with context using `%w` verb for unwrapping with `errors.Is`/`errors.As`\n- Every goroutine has a documented shutdown path\n- Tests are table-driven with clear subtest names\n- `go vet` and `staticcheck` pass with no warnings\n- HTTP handlers accept `context.Context` from the request, never create `context.Background()`\n- Graceful shutdown is implemented for any long-running service\n\n## Composability hints\n- Before: systems-design expert for architecture; sql-queries expert for database schema\n- After: docker expert for multi-stage container builds; kubernetes expert for deployment\n- Related: concurrency expert for advanced patterns beyond basic goroutines\n- Pair with: github-actions expert for CI with `go test -race`, `go vet`, and `golangci-lint`"
    },
    {
      "id": "ml.hyperparameter-tuning",
      "title": "Hyperparameter Tuning and Search",
      "domain": "machine_learning",
      "instruction_file": "instructions/hyperparameter-tuning.md",
      "description": "Retrieve this expert for systematic hyperparameter search, tuning strategy design, and objective-driven model optimization.",
      "tags": [
        "hyperparameter-tuning",
        "optuna",
        "grid-search",
        "bayesian-optimization",
        "ml",
        "cross-validation"
      ],
      "tool_hints": [
        "optuna",
        "sklearn.model_selection",
        "ray tune",
        "wandb sweeps"
      ],
      "examples": [
        "Tune XGBoost with Bayesian optimization and early stopping",
        "Choose search space and pruning strategy for constrained runtime"
      ],
      "aliases": [
        "ml-hyperparameter-tuning",
        "hyperparameter-tuning-and-search"
      ],
      "dependencies": [
        "optuna",
        "ray[tune]"
      ],
      "input_contract": {
        "required": "problem definition, target variable, and evaluation metric",
        "optional": "feature constraints, compute budget, and deployment target"
      },
      "output_artifacts": [
        "modeling_or_feature_plan",
        "evaluation_summary",
        "reproducibility_metadata"
      ],
      "quality_checks": [
        "data_leakage_review_completed",
        "validation_strategy_aligned_to_problem",
        "metrics_include_uncertainty_or_variance"
      ],
      "constraints": [
        "avoid_training_test_contamination",
        "record_model_data_and_seed_versions"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "ml",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Hyperparameter Tuning Expert\n\nUse this expert when tasks require systematic optimization of model hyperparameters, including search strategy selection, cross-validation design, early pruning, and multi-objective trade-offs.\n\n## When to use this expert\n- The task involves selecting optimal hyperparameters for any ML or DL model beyond default settings.\n- The user needs to choose between grid, random, or Bayesian search strategies for their problem size and budget.\n- Early pruning or multi-objective optimization (e.g., accuracy vs. latency) is required for expensive training runs.\n- Cross-validation strategy must be designed to avoid overfitting the hyperparameter search to a single split.\n\n## Execution behavior\n\n1. Define the search space explicitly: specify each hyperparameter with its type (categorical, integer, float), range, and scale (linear or log-uniform). Use log-uniform for learning rates, regularization strengths, and any parameter spanning multiple orders of magnitude.\n2. Select the search strategy based on space size and evaluation cost. For fewer than 50 total combinations, use `GridSearchCV`. For moderate spaces, use `RandomizedSearchCV` with at least 100 iterations. For large or expensive spaces, use Optuna with the TPE sampler.\n3. Define the cross-validation scheme to match the data structure: `StratifiedKFold` for imbalanced classification, `TimeSeriesSplit` for temporal data, `RepeatedKFold` for small datasets. Use at least 5 folds.\n4. Set an explicit evaluation metric via the `scoring` parameter (sklearn) or Optuna's objective return value. Never rely on the model's default score method without verifying it matches the business objective.\n5. For expensive evaluations (training time > 2 minutes per trial), enable pruning. In Optuna, use `MedianPruner(n_startup_trials=5, n_warmup_steps=10)` or `HyperbandPruner` and report intermediate values with `trial.report(value, step)`.\n6. Run the search, then inspect the importance of each hyperparameter using `optuna.importance.get_param_importances()` or partial dependence plots to understand which parameters matter most.\n7. Validate the best configuration with a final evaluation on a held-out test set that was never seen during the search. If nested CV is required, wrap the entire search inside an outer `cross_val_score` loop.\n8. Log all trial results (hyperparameters, metric, duration, pruning status) to a persistent store (Optuna storage, MLflow, or CSV) for reproducibility and future warm-starting.\n\n## Decision tree\n- If search space is small (< 50 combinations) -> use `GridSearchCV` for exhaustive coverage.\n- If search space is moderate (50-500 effective combinations) -> use `RandomizedSearchCV` with `n_iter >= 100` and log-uniform distributions for scale-sensitive parameters.\n- If search space is large or evaluation is expensive -> use Optuna with `TPESampler` for sample-efficient Bayesian optimization.\n- If single trial takes > 5 minutes -> enable pruning with `MedianPruner` or `HyperbandPruner` and report intermediate metrics each epoch.\n- If multiple conflicting objectives (e.g., accuracy vs. model size) -> use `optuna.create_study(directions=[\"maximize\", \"minimize\"])` and select from the Pareto front.\n- If reproducibility is critical -> set `TPESampler(seed=42)` and fix all random states in the training pipeline.\n\n## Anti-patterns\n- NEVER use grid search on a space with more than 4 hyperparameters or wide continuous ranges. The combinatorial explosion wastes compute on uninformative regions.\n- NEVER tune hyperparameters using the test set directly. The test set must remain unseen until final evaluation; use a validation set or CV folds for tuning.\n- NEVER run expensive trials without pruning. Unpruned Bayesian search on a 100-trial budget can waste 80% of compute on clearly inferior configurations.\n- NEVER ignore the search budget. Define `n_trials` or `timeout` upfront so the search terminates predictably and resources are bounded.\n- NEVER tune all hyperparameters simultaneously from scratch. Start with the most impactful ones (learning rate, regularization) and fix less sensitive ones at defaults, then refine.\n\n## Common mistakes\n- Using uniform distributions for learning rate or regularization instead of log-uniform, causing the search to over-sample large values and under-explore small ones.\n- Setting `cv=3` to speed up evaluation, which increases variance in metric estimates and makes the search select noisy winners. Use at least 5 folds.\n- Forgetting to pass `refit=True` (or the best metric name) to `GridSearchCV`/`RandomizedSearchCV`, so the final model is never refitted on the full training set.\n- Comparing trials across different CV splits due to unseeded `KFold`, making metric comparisons meaningless. Always seed the splitter.\n- Running Optuna without a persistent storage backend, losing all trial history when the process crashes. Use `optuna.storages.RDBStorage` for long runs.\n- Reporting the best cross-validation score as the expected production performance without accounting for optimistic bias from the search itself.\n\n## Output contract\n- Report the best hyperparameter configuration with its CV metric (mean and standard deviation across folds).\n- Include a ranked table of all trials (or top-20) with hyperparameters, metric, duration, and pruning status.\n- Provide hyperparameter importance rankings to justify which parameters were most influential.\n- Show the optimization history plot (objective value vs. trial number) to confirm convergence.\n- Record the search strategy, sampler, pruner, number of trials, and total wall-clock time.\n- If multi-objective, present the Pareto front with trade-off visualization.\n- Save the Optuna study or search results to a reproducible artifact (database, JSON, or pickle).\n\n## Composability hints\n- Before this expert -> use the **Feature Engineering Expert** to finalize the feature set, so tuning operates on the correct input space.\n- Before this expert -> use the **Scikit-learn Modeling Expert** or **PyTorch Training Expert** to define the base model and training loop.\n- After this expert -> use the **Machine Learning Export Expert** to package the best model configuration for deployment.\n- Related -> the **Gradient Boosting Expert** for XGBoost/LightGBM-specific parameter spaces and early-stopping patterns.\n- Related -> the **Transformers Fine-tuning Expert** for tuning LoRA rank, learning rate, and warmup in HuggingFace workflows."
    },
    {
      "id": "sec.iam-policies",
      "title": "IAM Policies and Access Control",
      "domain": "security",
      "instruction_file": "instructions/iam-policies.md",
      "description": "Retrieve this expert for least-privilege IAM design, policy boundaries, role trust rules, and cloud access governance.",
      "tags": [
        "security",
        "iam",
        "least-privilege",
        "aws",
        "access-control",
        "rbac"
      ],
      "tool_hints": [
        "aws iam",
        "cloudtrail",
        "access analyzer",
        "sts assume-role"
      ],
      "examples": [
        "Design least-privilege role policy for CI deployment",
        "Audit and tighten wildcard IAM permissions with conditions"
      ],
      "aliases": [
        "sec-iam-policies",
        "iam-policies-and-access-control"
      ],
      "dependencies": [
        "awscli",
        "aws-iam-access-analyzer",
        "cloudtrail",
        "terraform"
      ],
      "input_contract": {
        "required": "authorized scope, threat context, and assets under assessment",
        "optional": "compliance framework mapping, risk tolerance, and remediation timeline"
      },
      "output_artifacts": [
        "security_findings",
        "prioritized_remediation_plan",
        "control_verification_checklist"
      ],
      "quality_checks": [
        "least_privilege_policy_scope_reviewed",
        "wildcards_and_escalation_paths_checked",
        "audit_logging_and_alerting_confirmed"
      ],
      "constraints": [
        "authorized_scope_only",
        "omit_operational_exploit_detail_that_enables_misuse"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "security",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# IAM and Access Control Expert\n\nSpecialist in designing and enforcing identity and access management policies. Applies the\nprinciple of least privilege across cloud platforms, with emphasis on AWS IAM, service\naccount scoping, policy boundaries, and continuous audit through access analysis.\n\n## When to use this expert\n- Designing IAM roles, policies, or permission boundaries for cloud workloads\n- Reviewing existing policies for overly broad permissions or privilege escalation paths\n- Setting up cross-account access, federation, or SSO integration\n- Configuring audit logging and access analysis for compliance requirements\n\n## Execution behavior\n1. Inventory all principals: human users, service accounts, CI runners, Lambda roles, cross-account roles.\n2. For each principal, define the minimum set of actions and resources required using task-based analysis.\n3. Write policies with explicit resource ARNs  avoid wildcard (*) resources except where structurally required.\n4. Apply permission boundaries or Service Control Policies (SCPs) to cap the maximum possible privileges.\n5. Add condition keys to restrict access by source IP, VPC, MFA status, time window, or tag.\n6. Enable audit logging: CloudTrail (all regions, all management/data events), Access Analyzer, and IAM credential reports.\n7. Run IAM Access Analyzer to identify unintended external or cross-account access grants.\n8. Schedule quarterly access reviews with automated reports on unused roles and stale credentials.\n\n## Decision tree\n- If human user access  enforce SSO (SAML/OIDC) + MFA + session duration limit; no long-lived access keys\n- If service or workload  assign a scoped IAM role with external ID condition; never embed static keys\n- If cross-account access  use sts:AssumeRole with condition keys (aws:PrincipalOrgID, aws:SourceArn)\n- If auditing required  enable CloudTrail in all regions + S3 data events + Lambda data events; feed to SIEM\n- If policy is too broad  use IAM Access Analyzer policy validation to identify unused actions and tighten\n- If break-glass needed  create a separate emergency role with SCPs that require MFA + CloudTrail alert\n\n## Anti-patterns\n- NEVER use wildcard (*) for both Action and Resource in the same statement  it grants full admin access\n- NEVER create long-lived IAM access keys for human users  use SSO and temporary credentials\n- NEVER share the root account or use it for day-to-day operations  lock it with MFA and alerting\n- NEVER skip MFA enforcement  require it for console access and sensitive API operations\n- NEVER assign overly broad service roles (e.g., AmazonS3FullAccess) when only specific bucket access is needed\n- NEVER leave unused IAM roles or users active  deactivate after 90 days of inactivity\n\n## Common mistakes\n- Writing an Allow policy but forgetting that a Deny in an SCP or permission boundary overrides it\n- Using aws:SourceIp conditions for roles assumed by AWS services (the source IP is the service, not the user)\n- Granting iam:PassRole without restricting which roles can be passed, enabling privilege escalation\n- Enabling CloudTrail but not configuring log file validation or sending logs to a tamper-proof bucket\n- Creating a permission boundary but not attaching it to the IAM user or role, leaving it unenforced\n- Confusing identity-based policies (attached to principal) with resource-based policies (attached to resource)\n\n## Output contract\n- Every principal has a documented, least-privilege policy with explicit resource ARNs\n- No policy contains Action: * with Resource: * unless it is an explicit, time-limited break-glass role\n- Permission boundaries or SCPs are applied to all accounts and organizational units\n- MFA is enforced for all human console access and for sensitive API operations\n- CloudTrail is enabled in all regions with log file integrity validation\n- IAM Access Analyzer runs continuously; external access findings have a documented resolution\n- Quarterly access reviews are scheduled with automated detection of unused roles and stale keys\n\n## Composability hints\n- Before: architecture expert (to define service boundaries and trust relationships), secrets-management expert (to handle credential lifecycle)\n- After: penetration-testing expert (to test for privilege escalation), container-security expert (to scope pod service accounts)\n- Related: owasp-web expert (for application-level authorization logic), dependency-scanning expert (for CI pipeline service account scoping)"
    },
    {
      "id": "cloud.kubernetes",
      "title": "Kubernetes Workload Operations",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/kubernetes.md",
      "description": "Retrieve this expert for Kubernetes deployments, scaling, networking, RBAC, and production-grade cluster operations.",
      "tags": [
        "kubernetes",
        "k8s",
        "deployment",
        "rbac",
        "hpa",
        "containers",
        "cluster"
      ],
      "tool_hints": [
        "kubectl",
        "helm",
        "kustomize",
        "prometheus"
      ],
      "examples": [
        "Deploy resilient service with probes and autoscaling",
        "Harden pod security contexts and namespace policies"
      ],
      "aliases": [
        "cloud-kubernetes",
        "kubernetes-workload-operations"
      ],
      "dependencies": [
        "kubectl",
        "helm",
        "kustomize",
        "kubernetes"
      ],
      "input_contract": {
        "required": "target cloud environment, intended workload behavior, and resource scope",
        "optional": "existing topology, compliance controls, cost and rollback constraints"
      },
      "output_artifacts": [
        "infrastructure_changeset",
        "configuration_artifacts",
        "operational_validation_report"
      ],
      "quality_checks": [
        "least_privilege_reviewed",
        "failure_and_rollback_path_defined",
        "cost_and_scaling_implications_noted"
      ],
      "constraints": [
        "avoid_unreviewed_production_changes",
        "pin_or_version_infrastructure_artifacts"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "cloud",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Kubernetes Orchestration Expert\n\nSpecialist in Kubernetes workload deployment, service networking, security, and cluster operations.\n\n## When to use this expert\n- Task requires deploying, scaling, or managing containerized workloads in Kubernetes\n- Workload involves configuring Deployments, Services, Ingress, or autoscaling policies\n- RBAC, resource quotas, network policies, or pod security must be defined\n- Debugging pod failures, scheduling issues, or service connectivity problems is needed\n\n## Execution behavior\n1. Define the workload type based on the application pattern (Deployment, StatefulSet, Job, CronJob, or DaemonSet).\n2. Set resource requests and limits for CPU and memory on every container to enable proper scheduling.\n3. Configure liveness, readiness, and startup probes appropriate to the application health model.\n4. Create a Service with the correct type (ClusterIP, NodePort, or LoadBalancer) for the traffic pattern.\n5. Apply RBAC with least-privilege ServiceAccounts; never use the default ServiceAccount for workloads.\n6. Store configuration in ConfigMaps and sensitive values in Secrets with encryption at rest enabled.\n7. Set up Horizontal Pod Autoscaler (HPA) based on CPU, memory, or custom metrics rather than static replica counts.\n8. Deploy to a dedicated namespace with resource quotas and LimitRanges to prevent noisy-neighbor issues.\n\n## Decision tree\n- If the workload is stateless -> use a Deployment with rolling update strategy\n- If the workload requires stable network identity or persistent storage -> use a StatefulSet with volumeClaimTemplates\n- If the workload is a one-time or periodic task -> use a Job or CronJob respectively\n- If external HTTP/HTTPS traffic must reach the cluster -> configure an Ingress resource with cert-manager for TLS\n- If configuration is non-sensitive -> store it in a ConfigMap mounted as a volume or environment variable\n- If configuration is sensitive -> store it in a Secret and consider external secrets operators for rotation\n\n## Anti-patterns\n- NEVER deploy without resource requests and limits; unbounded containers destabilize the entire node\n- NEVER run containers in privileged mode or as root unless there is a documented infrastructure requirement\n- NEVER deploy application workloads into the default namespace; always use dedicated namespaces\n- NEVER omit liveness and readiness probes; without them, unhealthy pods continue receiving traffic\n- NEVER hardcode replica counts without HPA; static scaling wastes resources or underperforms under load\n- NEVER store secrets in plaintext ConfigMaps or embed them in container images\n\n## Common mistakes\n- Setting resource requests too low, causing pods to be evicted under memory pressure (OOMKilled)\n- Confusing liveness probes with readiness probes; a misconfigured liveness probe causes restart loops\n- Using LoadBalancer Service type for every service instead of a single Ingress controller, wasting cloud load balancers\n- Forgetting to set `revisionHistoryLimit` on Deployments, accumulating hundreds of old ReplicaSets\n- Not configuring PodDisruptionBudgets, causing voluntary disruptions (node drain) to take down all replicas simultaneously\n- Using hostPath volumes in production, which ties pods to specific nodes and breaks scheduling\n\n## Output contract\n- Provide all Kubernetes manifests in YAML with clear metadata labels and annotations\n- Document the resource requests, limits, and scaling thresholds with sizing rationale\n- Specify probe endpoints, intervals, thresholds, and failure actions\n- Include RBAC manifests (ServiceAccount, Role, RoleBinding) scoped to the minimum required API groups\n- Describe the Service and Ingress topology with ports, protocols, and TLS configuration\n- List ConfigMap and Secret references with their mount paths or environment variable mappings\n- Provide rollout and rollback commands for operational use\n\n## Composability hints\n- Upstream: docker expert for building the container images that Kubernetes deploys\n- Upstream: terraform expert for provisioning the EKS, GKE, or AKS cluster infrastructure\n- Related: aws-vpc expert when Kubernetes nodes and pods operate within VPC subnets and security groups\n- Downstream: github-actions expert for CI/CD pipelines that apply manifests or Helm charts to the cluster\n- Related: aws-s3 expert when pods need persistent object storage via S3-compatible CSI drivers or SDKs\n- Related: aws-lambda expert when evaluating whether a workload fits better as serverless than as a cluster deployment"
    },
    {
      "id": "fe.nextjs",
      "title": "Next.js Full-stack Applications",
      "domain": "frontend",
      "instruction_file": "instructions/nextjs.md",
      "description": "Retrieve this expert for Next.js app/router architecture, SSR/SSG strategies, API routes, and performance optimization.",
      "tags": [
        "nextjs",
        "react",
        "ssr",
        "ssg",
        "app-router",
        "frontend"
      ],
      "tool_hints": [
        "next.js",
        "react",
        "vercel",
        "edge runtime"
      ],
      "examples": [
        "Choose SSR vs SSG per route with caching strategy",
        "Implement Next.js app router with server/client boundaries"
      ],
      "aliases": [
        "fe-nextjs",
        "next-js-full-stack-applications",
        "next.js-full-stack-applications"
      ],
      "dependencies": [
        "next",
        "react",
        "vercel"
      ],
      "input_contract": {
        "required": "UI feature intent, interaction states, and target device/browser requirements",
        "optional": "design tokens, brand constraints, and accessibility/performance targets"
      },
      "output_artifacts": [
        "component_or_layout_changes",
        "accessibility_or_performance_notes",
        "integration_usage_guidance"
      ],
      "quality_checks": [
        "responsive_and_state_behaviors_verified",
        "keyboard_and_screen_reader_baseline_checked",
        "render_performance_considered"
      ],
      "constraints": [
        "preserve_design_system_consistency",
        "avoid_regressing_core_accessibility_paths"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "frontend",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Next.js Application Framework Expert\n\nUse this expert when tasks require building full-stack web applications with Next.js App Router, including server and client component architecture, data fetching strategies, middleware, rendering modes (SSG/SSR/ISR), metadata for SEO, and loading/error state management.\n\n## When to use this expert\n- The task involves creating or restructuring a Next.js application using the App Router (app directory).\n- Decisions are needed about server components vs client components and where the \"use client\" boundary should be placed.\n- Data fetching must be designed across server components, route handlers, or server actions.\n- SEO, performance, or rendering strategy (static vs dynamic vs incremental) must be chosen for specific routes.\n\n## Execution behavior\n\n1. Structure the `app/` directory with route segments as folders. Each route folder contains `page.tsx` (UI), `layout.tsx` (shared shell), `loading.tsx` (streaming fallback), and `error.tsx` (error boundary).\n2. Default every component to a server component. Only add `\"use client\"` when the component requires browser APIs, event handlers, `useState`, or `useEffect`. Push the client boundary as far down the tree as possible.\n3. Fetch data directly in server components using `async/await` with the extended `fetch` API. Configure caching via `{ next: { revalidate: N } }` for ISR or `{ cache: \"no-store\" }` for fully dynamic pages.\n4. Use route handlers (`app/api/.../route.ts`) for webhook endpoints, third-party callbacks, or any logic that must respond to non-GET HTTP methods from external services. For form submissions from your own UI, prefer server actions.\n5. Implement `generateMetadata` or export a static `metadata` object in each `page.tsx` and `layout.tsx` to provide title, description, Open Graph, and Twitter card data for SEO.\n6. Use `generateStaticParams` in dynamic route segments to pre-render known paths at build time (SSG). Combine with `dynamicParams = true` to allow on-demand ISR for new paths.\n7. Add middleware in `middleware.ts` at the project root for cross-cutting concerns: authentication redirects, geo-based routing, A/B testing headers, and request logging. Keep middleware lightweight since it runs on every matching request.\n8. Use `loading.tsx` files to provide instant streaming fallbacks while server components resolve. Pair with `<Suspense>` inside layouts for more granular streaming boundaries.\n\n## Decision tree\n- If content is the same for all users and changes infrequently -> SSG with `generateStaticParams`; use ISR (`revalidate`) if it updates periodically.\n- If content depends on the request (cookies, headers, search params) -> server component reading `cookies()` or `headers()` with `dynamic = \"force-dynamic\"`.\n- If content must update in real time in the browser -> client component with SWR, TanStack Query, or WebSocket subscription.\n- If a page needs authentication gating -> use `middleware.ts` to redirect unauthenticated requests before the page even renders.\n- If a page needs SEO metadata -> export `generateMetadata` to dynamically produce title, description, and OG tags from fetched data.\n- If a form submits data -> use a server action (`\"use server\"` function) with `useFormStatus` for pending UI, rather than a client-side fetch to an API route.\n\n## Anti-patterns\n- NEVER add `\"use client\"` to layout or page files that do not require interactivity. This opts the entire subtree out of server rendering and data fetching benefits.\n- NEVER fetch data in `useEffect` inside client components when the same data could be fetched in a parent server component and passed as props.\n- NEVER omit `loading.tsx` or `<Suspense>` boundaries for routes with async data. Users will see a blank screen during server-side data fetching.\n- NEVER import server-only modules (`fs`, `database clients`, secrets) in client components. Use the `server-only` package to create a build-time error if this happens.\n- NEVER use the Pages Router (`pages/` directory) conventions in an App Router project. The two routers have incompatible data fetching and layout models.\n- NEVER put heavy computation or slow database queries in middleware. Middleware runs on the Edge Runtime and should only inspect/modify the request or redirect.\n\n## Common mistakes\n- Marking a top-level layout as `\"use client\"` to add a single click handler, which forces every nested page into client rendering. Extract only the interactive piece into a small client component.\n- Forgetting that `fetch` in server components is deduplicated and cached by default. Adding manual caching layers on top creates stale data bugs.\n- Using `redirect()` inside a try/catch block, which catches the redirect throw and prevents navigation. Call `redirect()` outside of try/catch.\n- Not setting `revalidate` on `fetch` calls in ISR pages, defaulting to permanent caching and serving stale content indefinitely.\n- Passing non-serializable values (functions, class instances) from server components to client components as props, causing runtime hydration errors.\n- Creating API route handlers for data that is only consumed by your own pages, when a server component fetch or server action would be simpler and faster.\n\n## Output contract\n- Every route segment must include `page.tsx` and should include `loading.tsx` and `error.tsx` for complete UX coverage.\n- Server components must be the default; `\"use client\"` must only appear on leaf components that need interactivity.\n- All pages with public URLs must export `metadata` or `generateMetadata` with at minimum a title and description.\n- Dynamic routes that have a known set of params must implement `generateStaticParams` for build-time pre-rendering.\n- Authentication and authorization redirects must be handled in `middleware.ts`, not in individual page components.\n- Data mutations must use server actions or route handlers, never direct database calls from client components.\n- Environment variables containing secrets must only be accessed in server components, route handlers, or server actions (no `NEXT_PUBLIC_` prefix).\n\n## Composability hints\n- Before this expert -> use the **React Expert** to design component composition, hooks patterns, and error boundary strategy.\n- Before this expert -> use the **CSS Architecture Expert** to set up Tailwind, CSS Modules, or design tokens within the Next.js project.\n- After this expert -> use the **Accessibility Expert** to audit pages for semantic HTML, keyboard navigation, and ARIA attributes.\n- After this expert -> use the **Auth JWT Expert** for implementing authentication flows in middleware and server actions.\n- Related -> the **State Management Expert** for client-side state in interactive sections of an otherwise server-rendered app."
    },
    {
      "id": "sec.owasp-web",
      "title": "Web Application Security (OWASP Top 10)",
      "domain": "security",
      "instruction_file": "instructions/owasp-web.md",
      "description": "Retrieve this expert for OWASP-aligned web security reviews, secure coding controls, and vulnerability remediation guidance.",
      "tags": [
        "security",
        "owasp",
        "xss",
        "csrf",
        "sqli",
        "ssrf",
        "secure-coding"
      ],
      "tool_hints": [
        "owasp asvs",
        "security headers",
        "input validation",
        "parameterized queries"
      ],
      "examples": [
        "Harden API against XSS/CSRF/SQLi classes",
        "Apply OWASP checklist to web service threat review"
      ],
      "aliases": [
        "sec-owasp-web",
        "web-application-security-owasp-top-10",
        "web-application-security-(owasp-top-10)"
      ],
      "dependencies": [
        "owasp-asvs",
        "semgrep",
        "zap",
        "burp-suite"
      ],
      "input_contract": {
        "required": "authorized scope, threat context, and assets under assessment",
        "optional": "compliance framework mapping, risk tolerance, and remediation timeline"
      },
      "output_artifacts": [
        "security_findings",
        "prioritized_remediation_plan",
        "control_verification_checklist"
      ],
      "quality_checks": [
        "findings_mapped_to_owasp_categories",
        "severity_and_repro_steps_documented",
        "fixes_include_verification_strategy"
      ],
      "constraints": [
        "authorized_scope_only",
        "omit_operational_exploit_detail_that_enables_misuse"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "security",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Web Application Security Expert (OWASP Top 10)\n\nSpecialist in identifying and mitigating web application vulnerabilities aligned with the\nOWASP Top 10. Applies defense-in-depth strategies including input validation, output\nencoding, authentication hardening, and security header configuration.\n\n## When to use this expert\n- Code handles user-supplied input that is rendered, stored, or forwarded\n- Building or reviewing authentication/authorization flows\n- Configuring HTTP responses, headers, or cookie policies\n- Evaluating an application against the OWASP Top 10 checklist\n\n## Execution behavior\n1. Identify all user-input entry points (query params, form fields, headers, file uploads, JSON bodies).\n2. Classify each input by context: SQL, HTML, URL, OS command, LDAP, XML.\n3. Apply context-specific defenses: parameterized queries, output encoding, allowlists.\n4. Review authentication flow for broken-auth risks: credential stuffing, session fixation, weak password rules.\n5. Verify CSRF protections on all state-changing endpoints (synchronizer token or SameSite cookies).\n6. Configure security headers: Content-Security-Policy, Strict-Transport-Security, X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy.\n7. Validate SSRF defenses: deny internal IP ranges, use allowlists for outbound requests.\n8. Document findings with CWE references and remediation code samples.\n\n## Decision tree\n- If user input is inserted into SQL  use parameterized queries or an ORM; never concatenate\n- If rendering user content as HTML  apply context-aware output encoding (HTML body, attribute, JS, CSS, URL)\n- If endpoint mutates state  enforce CSRF token validation and SameSite=Strict cookies\n- If server makes outbound requests with user-supplied URLs  validate against an allowlist and block RFC 1918 ranges\n- If API endpoint  enforce rate limiting, input schema validation, and authentication\n- If file upload  validate MIME type server-side, rename file, store outside webroot, scan for malware\n\n## Anti-patterns\n- NEVER concatenate user input into SQL strings  always use parameterized statements\n- NEVER use innerHTML, document.write, or v-html with unsanitized user input\n- NEVER disable CSRF protection for convenience or to fix CORS issues\n- NEVER rely solely on client-side validation  always re-validate on the server\n- NEVER store passwords in plaintext or with reversible encryption  use bcrypt, scrypt, or Argon2\n- NEVER expose detailed error messages or stack traces to end users\n\n## Common mistakes\n- Encoding output for the wrong context (HTML-encoding inside a JavaScript block)\n- Using a blocklist instead of an allowlist for input validation\n- Setting Access-Control-Allow-Origin to wildcard (*) with credentials enabled\n- Forgetting to set the HttpOnly and Secure flags on session cookies\n- Applying rate limiting only at the application layer while ignoring API gateway config\n- Trusting the Content-Type header from file uploads without verifying actual file content\n\n## Output contract\n- Every user-input path must have documented validation and encoding strategy\n- All SQL access must use parameterized queries  no exceptions\n- HTTP responses must include CSP, HSTS, X-Content-Type-Options at minimum\n- Authentication endpoints must enforce rate limiting and account lockout\n- Session tokens must be cryptographically random, rotated on privilege change\n- CSRF defenses must cover every state-changing request\n- Findings must reference OWASP Top 10 category and CWE identifier\n\n## Composability hints\n- Before: API design expert (to define input schemas), architecture expert (to map attack surface)\n- After: penetration-testing expert (to validate mitigations), container-security expert (for runtime hardening)\n- Related: secrets-management expert (for credential handling), iam-policies expert (for authorization logic)"
    },
    {
      "id": "data.pandas-advanced",
      "title": "Advanced Pandas Data Workflows",
      "domain": "data_engineering",
      "instruction_file": "instructions/pandas-advanced.md",
      "description": "Retrieve this expert for high-performance pandas transformations, memory optimization, and robust dataframe pipeline design.",
      "tags": [
        "pandas",
        "dataframe",
        "data-engineering",
        "groupby",
        "merge",
        "optimization"
      ],
      "tool_hints": [
        "pandas",
        "pyarrow",
        "polars interop",
        "numexpr"
      ],
      "examples": [
        "Optimize large dataframe joins and aggregations",
        "Refactor slow pandas pipeline into vectorized operations"
      ],
      "aliases": [
        "data-pandas-advanced",
        "advanced-pandas-data-workflows"
      ],
      "dependencies": [
        "pandas"
      ],
      "input_contract": {
        "required": "source schema/data samples and transformation objectives",
        "optional": "data volume profile, latency/SLA constraints, and governance rules"
      },
      "output_artifacts": [
        "transformation_logic_or_queries",
        "data_quality_report",
        "lineage_or_execution_notes"
      ],
      "quality_checks": [
        "schema_and_type_assumptions_verified",
        "null_and_edge_cases_handled",
        "reproducible_execution_documented"
      ],
      "constraints": [
        "avoid_silent_schema_or_type_breakage",
        "preserve_traceability_of_source_to_output"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "data",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Advanced Pandas Expert\n\nUse this expert for complex DataFrame manipulation including groupby aggregations, multi-table merges, memory optimization, method chaining, and vectorized transformations on structured tabular data.\n\n## When to use this expert\n- The task involves multi-step groupby, pivot, or reshape operations beyond simple filtering.\n- Multiple DataFrames must be joined with explicit merge strategies based on key cardinality.\n- Memory pressure is a concern (datasets approaching or exceeding 1 GB in RAM).\n- Performance-critical code needs to replace row-wise iteration with vectorized operations.\n\n## Execution behavior\n\n1. Profile the DataFrame first: inspect `df.info(memory_usage='deep')`, `df.dtypes`, and `df.nunique()` to identify optimization opportunities before writing any transformation logic.\n2. Downcast numeric columns early using `pd.to_numeric(col, downcast='integer')` or `downcast='float'` to reduce memory footprint before heavy operations.\n3. Convert low-cardinality string columns (unique ratio < 50%) to `category` dtype to compress memory and speed up groupby and merge operations.\n4. For multi-table joins, explicitly choose the merge strategy: use `how='inner'` for intersection, `how='left'` to preserve the driving table, `how='outer'` for union, and `how='cross'` only when a full cartesian product is intentional and bounded.\n5. Validate merge results immediately: check row count against expectations, inspect `_merge` indicator column from `indicator=True`, and assert no unintended row duplication from many-to-many keys.\n6. Build transformations as method chains using `.pipe()`, `.assign()`, and `.query()` for readability. Each chain step should perform one logical operation.\n7. Prefer vectorized operations (`np.where`, `.str` accessor, `.dt` accessor) over `.apply()`. Use `.apply()` only when no vectorized alternative exists, and document why.\n8. For datasets exceeding available RAM, use chunked reading with `pd.read_csv(chunksize=...)` and process each chunk identically, or switch to a Spark-based expert.\n\n## Decision tree\n- If dataset > 1 GB in memory -> downcast numerics, convert strings to categoricals, and consider chunked processing before any transformation.\n- If a string column has fewer than 5% unique values relative to row count -> convert to `category` dtype immediately.\n- If joining two tables -> check key cardinality on both sides; one-to-many is safe, many-to-many requires explicit deduplication or aggregation first.\n- If aggregation needs multiple output columns -> use `.agg()` with a dictionary or named aggregation syntax, not multiple separate groupby calls.\n- If row-wise logic involves simple conditionals -> use `np.select()` or `np.where()` instead of `.apply(lambda)`.\n- If the operation is a running or rolling calculation -> use `.rolling()`, `.expanding()`, or `.shift()` instead of manual loops.\n\n## Anti-patterns\n- NEVER iterate rows with `for index, row in df.iterrows()`. This is orders of magnitude slower than vectorized alternatives.\n- NEVER use chained indexing like `df['a']['b'] = value`. Use `.loc[row, col]` to avoid the SettingWithCopyWarning and silent failures.\n- NEVER ignore memory usage on large DataFrames. Calling `.copy()` carelessly can double RAM consumption.\n- NEVER use `.apply()` with a lambda that wraps a vectorized pandas or numpy function. Call the vectorized function directly.\n- NEVER merge without validating the key relationship. Unexpected many-to-many joins silently explode row counts.\n\n## Common mistakes\n- Using `inplace=True` in method chains, which breaks the chain and returns `None` instead of the modified DataFrame.\n- Forgetting that `groupby().transform()` returns a Series aligned to the original index, while `groupby().agg()` returns a reduced DataFrame. Mixing them up causes shape mismatches.\n- Passing `on=` column names that have different dtypes across the two DataFrames (e.g., int vs. string), producing zero matches silently.\n- Resetting the index after a groupby but forgetting that the grouped columns become the index by default unless `as_index=False` is specified.\n- Using `.fillna(0)` indiscriminately, which masks genuine missing data and corrupts downstream statistical calculations.\n- Sorting a large DataFrame repeatedly inside a loop instead of sorting once and using `.searchsorted()` or merge-based lookups.\n\n## Output contract\n- Report memory usage before and after optimization steps (e.g., \"Reduced from 2.1 GB to 640 MB via downcasting and categoricals\").\n- Include row and column counts at key transformation stages to make the data lineage auditable.\n- Validate merge results with `indicator=True` and summarize match rates (both, left_only, right_only).\n- Return the final DataFrame with a clean, reset integer index unless the index carries semantic meaning.\n- Document any columns dropped, renamed, or type-coerced during the pipeline.\n- Emit dtype summary of the output DataFrame.\n- If chunked processing was used, report total rows processed and any chunks that raised warnings.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to handle nulls, deduplication, and type normalization.\n- After this expert -> use the **Visualization Expert** to plot distributions, aggregation summaries, or merge diagnostics.\n- After this expert -> use the **SQL Queries Expert** if the transformed data needs to be loaded into a relational database.\n- Related -> the **Spark Expert** when data volume exceeds single-machine memory limits.\n- Related -> the **Time Series Expert** when datetime indexing and temporal aggregation are the primary concern."
    },
    {
      "id": "sec.penetration-testing",
      "title": "Penetration Testing Methodology (Authorized)",
      "domain": "security",
      "instruction_file": "instructions/penetration-testing.md",
      "description": "Retrieve this expert for authorized penetration testing methodology, evidence collection, risk classification, and reporting.",
      "tags": [
        "security",
        "penetration-testing",
        "authorized",
        "recon",
        "vulnerability",
        "reporting"
      ],
      "tool_hints": [
        "nmap",
        "burp suite",
        "owasp testing guide",
        "cwe"
      ],
      "examples": [
        "Plan scoped authorized pentest with rules of engagement",
        "Produce executive and technical findings report with remediation"
      ],
      "aliases": [
        "sec-penetration-testing",
        "penetration-testing-methodology-authorized",
        "penetration-testing-methodology-(authori"
      ],
      "dependencies": [
        "nmap",
        "burp-suite",
        "nikto",
        "owasp-testing-guide",
        "metasploit-framework"
      ],
      "input_contract": {
        "required": "authorized scope, threat context, and assets under assessment",
        "optional": "compliance framework mapping, risk tolerance, and remediation timeline"
      },
      "output_artifacts": [
        "security_findings",
        "prioritized_remediation_plan",
        "control_verification_checklist"
      ],
      "quality_checks": [
        "written_authorization_and_scope_verified",
        "evidence_chain_preserved",
        "findings_prioritized_with_business_impact"
      ],
      "constraints": [
        "authorized_scope_only",
        "omit_operational_exploit_detail_that_enables_misuse"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "security",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Penetration Testing Methodology Expert (Authorized Engagements Only)\n\nSpecialist in structured, authorized penetration testing following industry-standard\nmethodologies. Guides scope definition, reconnaissance, vulnerability identification,\ncontrolled exploitation, and professional reporting. All activities require explicit\nwritten authorization from the asset owner before execution.\n\n## IMPORTANT: Authorization requirement\nThis expert must ONLY be used in the context of authorized security assessments. A signed\nrules-of-engagement document, scope definition, and written permission from the asset owner\nare mandatory prerequisites. Unauthorized testing is illegal and unethical.\n\n## When to use this expert\n- Planning a penetration test with a signed scope and rules-of-engagement document in hand\n- Selecting tools and techniques appropriate to the defined scope and target type\n- Classifying and prioritizing findings from an authorized security assessment\n- Writing a penetration test report with executive summary, technical detail, and remediation guidance\n\n## Execution behavior\n1. Verify authorization: confirm signed rules of engagement, scope boundaries, testing window, and emergency contacts.\n2. Reconnaissance (passive): OSINT, DNS enumeration, certificate transparency, technology fingerprinting  within scope only.\n3. Reconnaissance (active): port scanning, service enumeration, version detection  only against in-scope targets.\n4. Vulnerability identification: map discovered services against known CVEs, misconfigurations, and logic flaws.\n5. Controlled exploitation: attempt exploitation of confirmed vulnerabilities with minimal impact; capture evidence (screenshots, logs).\n6. Post-exploitation (if in scope): assess lateral movement potential, privilege escalation paths, and data access  document without exfiltrating real data.\n7. Cleanup: remove all test artifacts, shells, accounts, and persistence mechanisms from target systems.\n8. Reporting: produce a structured report with executive summary, methodology, findings ranked by risk, evidence, and remediation guidance.\n\n## Decision tree\n- If web application in scope  follow OWASP Testing Guide v4: authentication, authorization, input validation, session management, business logic\n- If network infrastructure in scope  port scan with nmap, identify services, check for default credentials, test known CVEs, validate segmentation\n- If API in scope  test authentication bypass, injection (SQLi, NoSQLi, command), IDOR, rate limiting, mass assignment\n- If cloud environment in scope  review IAM policies, public storage, metadata service access, privilege escalation paths\n- If finding is critical  notify the client immediately per the rules of engagement, do not wait for the final report\n- If out-of-scope system is discovered  document it, do not test it, and notify the client for scope clarification\n\n## Anti-patterns\n- NEVER test without written authorization and a signed rules-of-engagement document\n- NEVER exceed the defined scope boundaries  if a path leads out of scope, stop and document\n- NEVER perform destructive exploitation (DoS, data deletion, ransomware simulation) unless explicitly authorized\n- NEVER skip the reporting phase  undelivered findings provide zero security value\n- NEVER retest without verifying that the client has applied fixes  coordinate retest windows\n- NEVER exfiltrate real sensitive data as proof  use screenshots, metadata, or record counts as evidence\n\n## Common mistakes\n- Beginning testing before the rules-of-engagement document is signed by all parties\n- Running aggressive scans during business hours when the scope specifies after-hours testing windows\n- Treating automated scanner output as findings without manual verification of exploitability\n- Failing to distinguish between informational, low, medium, high, and critical findings in the report\n- Not cleaning up test accounts, web shells, or temporary files after the engagement\n- Providing a findings list without actionable remediation guidance or risk context for each item\n\n## Output contract\n- A signed rules-of-engagement document and scope definition must exist before any testing begins\n- Reconnaissance covers both passive and active techniques appropriate to the target type\n- Each finding includes: title, severity (CVSS), affected asset, evidence, reproduction steps, and remediation\n- Critical findings are reported to the client immediately, not held for the final report\n- The final report contains an executive summary, methodology section, detailed findings, and remediation roadmap\n- All test artifacts are removed from target systems upon engagement completion\n- A retest plan is provided with specific validation steps for each remediated finding\n\n## Composability hints\n- Before: owasp-web expert (to understand application attack surface), iam-policies expert (to review access controls), container-security expert (to assess runtime exposure)\n- After: secrets-management expert (to remediate credential findings), dependency-scanning expert (to patch vulnerable components)\n- Related: all security domain experts contribute context that informs penetration test scope and technique selection"
    },
    {
      "id": "finance.quantitative",
      "title": "Quantitative Finance Modeling",
      "domain": "finance",
      "instruction_file": "instructions/quantitative-finance.md",
      "description": "Retrieve this expert for portfolio optimization, factor analysis, backtesting controls, and quantitative risk evaluation.",
      "tags": [
        "quantitative-finance",
        "portfolio",
        "risk",
        "backtesting",
        "factor-models",
        "finance"
      ],
      "tool_hints": [
        "pandas",
        "numpy",
        "scipy optimize",
        "statsmodels"
      ],
      "examples": [
        "Construct and evaluate long-short factor portfolio",
        "Compute drawdown, Sharpe, and exposure diagnostics"
      ],
      "aliases": [
        "finance-quantitative",
        "quantitative-finance-modeling"
      ],
      "dependencies": [
        "pandas",
        "numpy"
      ],
      "input_contract": {
        "required": "financial objective, modeling assumptions, and data time horizon",
        "optional": "scenario definitions, risk constraints, and benchmark references"
      },
      "output_artifacts": [
        "assumption_framework",
        "scenario_or_backtest_outputs",
        "risk_and_sensitivity_summary"
      ],
      "quality_checks": [
        "assumptions_and_units_are_explicit",
        "sensitivity_or_stress_analysis_included",
        "data_sources_and_methods_cited"
      ],
      "constraints": [
        "not_investment_advice",
        "avoid_unjustified_precision_or_causal_claims"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "finance",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Quantitative Finance Expert\n\nUse this expert for portfolio optimization, risk measurement, backtesting strategies, factor modeling, and return calculations with proper treatment of financial data characteristics such as fat tails, autocorrelation, and survivorship bias.\n\n## When to use this expert\n- The task involves constructing or rebalancing a portfolio with optimization constraints.\n- Risk metrics (VaR, CVaR, drawdown, Sharpe, Sortino) must be calculated or compared.\n- A trading or allocation strategy must be backtested with realistic assumptions.\n- Factor models or return attribution are needed to explain portfolio performance.\n\n## Execution behavior\n\n1. Collect and validate price or return data. Confirm the data is adjusted for splits and dividends, check for survivorship bias, and align all series to a common date index with explicit handling of missing trading days.\n2. Compute log returns for multi-period compounding analysis or simple returns for portfolio-weighted aggregation. Never mix the two in the same calculation.\n3. Characterize the return distribution: test for normality (Jarque-Bera), check skewness and kurtosis, and examine autocorrelation. Financial returns are rarely Gaussian.\n4. For portfolio construction, use mean-variance optimization with a covariance shrinkage estimator (Ledoit-Wolf) to stabilize weights. Apply realistic constraints: long-only, max position size, sector limits.\n5. Calculate risk metrics on out-of-sample data: Value at Risk (VaR) at 95% and 99%, Conditional VaR (Expected Shortfall), maximum drawdown, Sharpe ratio (annualized), and Sortino ratio.\n6. Backtest with walk-forward methodology: train on an expanding or rolling window, predict/allocate on the next period, record returns net of transaction costs and slippage, and advance the window.\n7. Decompose returns using a factor model (Fama-French 3 or 5 factor, or custom factors) to separate alpha from systematic exposure.\n8. Stress test the portfolio against historical scenarios (2008 crisis, COVID crash, rate shocks) and report drawdown and recovery time.\n\n## Decision tree\n- If building a diversified portfolio -> use mean-variance optimization with Ledoit-Wolf shrinkage; add a maximum-weight constraint to prevent concentration.\n- If the investor has views on specific assets -> use Black-Litterman to blend market equilibrium with subjective views before optimizing.\n- If measuring downside risk on a portfolio with fat-tailed returns -> use historical simulation or Cornish-Fisher VaR, not parametric Gaussian VaR.\n- If returns are approximately normal -> parametric VaR is acceptable, but always cross-check with historical VaR.\n- If evaluating a strategy -> report Sharpe ratio, maximum drawdown, and Calmar ratio together. No single metric is sufficient.\n- If comparing strategies with different volatilities -> use risk-adjusted metrics (Sharpe, Sortino) not raw returns.\n\n## Anti-patterns\n- NEVER introduce look-ahead bias in backtests. The model must use only information available at the time of each decision. This includes not using future data for parameter tuning, feature selection, or universe definition.\n- NEVER use arithmetic mean of returns for multi-period compounding. Use geometric mean or log returns to avoid upward bias.\n- NEVER ignore transaction costs and slippage in backtests. A strategy profitable before costs may be unprofitable after realistic friction.\n- NEVER optimize on in-sample data only and report those results as expected performance. Always validate on out-of-sample periods.\n- NEVER treat backtest results as a prediction of future returns. Past performance with known data is not an unbiased estimator of future performance.\n- NEVER use a sample covariance matrix from short histories with many assets. The matrix will be noisy or singular; use shrinkage or factor-based covariance.\n\n## Common mistakes\n- Computing Sharpe ratio with daily returns but forgetting to annualize: multiply by `sqrt(252)` for daily data or `sqrt(12)` for monthly.\n- Using the risk-free rate inconsistently: the rate must match the return frequency (daily risk-free for daily returns) and be subtracted before annualizing.\n- Ignoring survivorship bias by using only currently listed assets, which excludes delisted losers and inflates historical performance.\n- Rebalancing at unrealistic frequencies (e.g., daily) without accounting for the cumulative transaction cost drag.\n- Confusing Sharpe and Sortino: Sortino uses downside deviation only, which is more appropriate when return distributions are skewed.\n- Fitting a model to the entire backtest period and then reporting its \"performance\" on the same data as if it were out-of-sample.\n\n## Output contract\n- Report all return calculations with explicit frequency (daily, monthly, annual) and compounding method (simple or log).\n- Include annualized Sharpe ratio, Sortino ratio, maximum drawdown, and Calmar ratio for any strategy evaluation.\n- Provide VaR and CVaR at 95% confidence with the method used (parametric, historical, or Monte Carlo).\n- Document all backtest assumptions: rebalancing frequency, transaction cost model, slippage, universe selection date.\n- Include an equity curve and drawdown plot or describe the trajectory in measurable terms.\n- Report factor exposures (betas) and the proportion of return explained by systematic factors vs. alpha.\n- State the out-of-sample evaluation period clearly and separately from any in-sample tuning period.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to handle missing prices, adjust for splits/dividends, and align multi-asset date indexes.\n- Before this expert -> use the **Time Series Expert** for stationarity testing and ARIMA-based return forecasting that feeds into allocation models.\n- After this expert -> use the **Visualization Expert** to produce equity curves, drawdown charts, and efficient frontier plots.\n- After this expert -> use the **Financial Modeling Expert** for fundamental valuation that informs asset selection before portfolio construction.\n- Related -> the **Statistics Expert** for hypothesis testing on alpha significance or distribution fitting."
    },
    {
      "id": "fe.react-native",
      "title": "React Native Mobile Development",
      "domain": "frontend",
      "instruction_file": "instructions/react-native.md",
      "description": "Retrieve this expert for React Native navigation, platform APIs, performance optimization, and production mobile release patterns.",
      "tags": [
        "react-native",
        "mobile",
        "ios",
        "android",
        "navigation",
        "performance"
      ],
      "tool_hints": [
        "react native",
        "expo",
        "react navigation",
        "metro"
      ],
      "examples": [
        "Architect cross-platform mobile app with native module boundaries",
        "Optimize React Native rendering and startup performance"
      ],
      "aliases": [
        "fe-react-native",
        "react-native-mobile-development"
      ],
      "dependencies": [
        "react"
      ],
      "input_contract": {
        "required": "UI feature intent, interaction states, and target device/browser requirements",
        "optional": "design tokens, brand constraints, and accessibility/performance targets"
      },
      "output_artifacts": [
        "component_or_layout_changes",
        "accessibility_or_performance_notes",
        "integration_usage_guidance"
      ],
      "quality_checks": [
        "responsive_and_state_behaviors_verified",
        "keyboard_and_screen_reader_baseline_checked",
        "render_performance_considered"
      ],
      "constraints": [
        "preserve_design_system_consistency",
        "avoid_regressing_core_accessibility_paths"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "frontend",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# React Native Mobile Development Expert\n\nUse this expert when tasks require building cross-platform mobile applications with React Native, including navigation setup, platform-specific adaptations, native module integration, Expo vs bare workflow decisions, and mobile performance optimization.\n\n## When to use this expert\n- The task involves building or extending a React Native application for iOS and Android.\n- Navigation architecture (stack, tab, drawer) must be designed or restructured using React Navigation.\n- Performance issues arise with long lists, animations, or heavy bridge communication.\n- Decisions are needed between Expo managed workflow and bare React Native or Expo Dev Client.\n\n## Execution behavior\n\n1. Choose the project scaffold: use Expo managed workflow for rapid prototyping and standard features; use Expo Dev Client or bare workflow when custom native modules (Bluetooth, advanced camera, codecs) are required.\n2. Set up React Navigation with a root navigator. Use a native stack (`@react-navigation/native-stack`) for screen transitions, bottom tabs for primary sections, and drawer for secondary navigation. Type all route params with TypeScript.\n3. Handle platform differences with `Platform.select()` for inline values and `.ios.tsx` / `.android.tsx` file extensions for divergent component implementations. Centralize platform checks in a `utils/platform.ts` module.\n4. Render long scrollable data with `FlatList` or `SectionList`, providing `keyExtractor`, `getItemLayout` (when row height is fixed), and `removeClippedSubviews={true}` for off-screen recycling.\n5. Implement animations with `react-native-reanimated` for gesture-driven and layout animations that run on the UI thread. Reserve the built-in `Animated` API only for trivial opacity or translation tweens.\n6. Manage keyboard behavior with `KeyboardAvoidingView` (use `behavior=\"padding\"` on iOS, `behavior=\"height\"` on Android) or the `react-native-keyboard-aware-scroll-view` library for forms inside scroll views.\n7. Store sensitive data (tokens, credentials) in `expo-secure-store` or `react-native-keychain`, never in `AsyncStorage`. Use `AsyncStorage` only for non-sensitive preferences and cached UI state.\n8. Test on physical devices regularly. The iOS Simulator and Android Emulator miss real-world issues such as memory pressure, thermal throttling, slow network, and gesture edge cases.\n\n## Decision tree\n- If the app needs only standard device APIs (camera, location, notifications) -> Expo managed workflow with EAS Build for native compilation.\n- If the app requires a custom native module not available in Expo -> Expo Dev Client with a custom native module, or eject to bare workflow.\n- If rendering a list with more than 50 items -> `FlatList` with `getItemLayout` and `windowSize` tuning; never `ScrollView` with `.map()`.\n- If animation involves gesture interaction or spring physics -> `react-native-reanimated` with `react-native-gesture-handler`; the built-in `Animated` API will jank.\n- If storing tokens or secrets -> `expo-secure-store` or `react-native-keychain`; if storing preferences -> `AsyncStorage`.\n- If a component looks or behaves differently per platform -> use `.ios.tsx` / `.android.tsx` file variants for large differences or `Platform.select` for small ones.\n\n## Anti-patterns\n- NEVER use `ScrollView` with `.map()` to render dynamic lists. `ScrollView` renders all children at once, causing memory spikes and jank on long lists. Use `FlatList`.\n- NEVER create inline style objects on every render. Define styles with `StyleSheet.create()` outside the component to avoid allocating new objects each render cycle.\n- NEVER perform heavy computation on the JavaScript thread synchronously. Use `InteractionManager.runAfterInteractions()` to defer work until animations complete, or offload to a native module.\n- NEVER ignore keyboard avoidance on form screens. Inputs hidden behind the keyboard cause user frustration and are a top complaint in app reviews.\n- NEVER hardcode pixel values for layout. Use `flex`, percentage widths, and responsive scaling utilities to support the wide range of mobile screen sizes.\n- NEVER store sensitive tokens in `AsyncStorage`. It is unencrypted plaintext on both platforms.\n\n## Common mistakes\n- Forgetting to wrap the root component in `NavigationContainer`, causing React Navigation to throw cryptic context errors.\n- Using `useEffect` with navigation listeners instead of the dedicated `useFocusEffect` hook, which handles screen focus/blur lifecycle correctly.\n- Omitting `keyExtractor` on `FlatList`, causing React to fall back to array indices and breaking item recycling on data changes.\n- Not testing on Android early enough. Layout differences (elevation vs shadow, font rendering, status bar behavior) are significant and surface late.\n- Running `console.log` in production builds. React Native serializes log arguments across the bridge, causing measurable performance degradation.\n- Setting fixed `height` on container views instead of using `flex: 1`, which breaks on devices with different screen aspect ratios.\n\n## Output contract\n- Navigation must use React Navigation with typed route params and a clearly documented navigator hierarchy.\n- Long lists must use `FlatList` or `SectionList` with `keyExtractor` and, when possible, `getItemLayout`.\n- Platform-specific code must be isolated using `Platform.select`, platform file extensions, or a dedicated platform utility module.\n- Sensitive data storage must use `expo-secure-store` or `react-native-keychain`, never `AsyncStorage`.\n- Animations that interact with gestures must use `react-native-reanimated`, not the built-in `Animated` API.\n- All forms must implement keyboard avoidance so inputs remain visible when the software keyboard is open.\n- The app must be tested on both a physical iOS device and a physical Android device before release.\n\n## Composability hints\n- Before this expert -> use the **React Expert** for component composition patterns, hooks architecture, and state management fundamentals.\n- Before this expert -> use the **State Management Expert** to choose between Context, Zustand, or Redux Toolkit for client state and TanStack Query for server state.\n- After this expert -> use the **Accessibility Expert** to audit for VoiceOver (iOS) and TalkBack (Android) support, touch target sizes, and screen reader labels.\n- Related -> the **CSS Architecture Expert** for establishing design tokens and a consistent spacing/color system adapted to `StyleSheet.create`.\n- Related -> the **Auth JWT Expert** for implementing token-based authentication with secure storage on mobile."
    },
    {
      "id": "fe.react",
      "title": "React Component Architecture",
      "domain": "frontend",
      "instruction_file": "instructions/react.md",
      "description": "Retrieve this expert for React component design, hook strategy, state boundaries, and performance-oriented UI architecture.",
      "tags": [
        "react",
        "hooks",
        "state-management",
        "components",
        "frontend",
        "performance"
      ],
      "tool_hints": [
        "react",
        "tanstack query",
        "redux toolkit",
        "react profiler"
      ],
      "examples": [
        "Split monolithic component tree into reusable architecture",
        "Fix unnecessary re-renders with memoization strategy"
      ],
      "aliases": [
        "fe-react",
        "react-component-architecture"
      ],
      "dependencies": [
        "react"
      ],
      "input_contract": {
        "required": "UI feature intent, interaction states, and target device/browser requirements",
        "optional": "design tokens, brand constraints, and accessibility/performance targets"
      },
      "output_artifacts": [
        "component_or_layout_changes",
        "accessibility_or_performance_notes",
        "integration_usage_guidance"
      ],
      "quality_checks": [
        "responsive_and_state_behaviors_verified",
        "keyboard_and_screen_reader_baseline_checked",
        "render_performance_considered"
      ],
      "constraints": [
        "preserve_design_system_consistency",
        "avoid_regressing_core_accessibility_paths"
      ],
      "risk_level": "low",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "frontend",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# React Component Architecture Expert\n\nUse this expert when tasks require building or refactoring React component trees, managing local and shared state with hooks, optimizing rendering performance, handling errors with boundaries, and structuring reusable UI composition patterns.\n\n## When to use this expert\n- The task involves designing component hierarchies, splitting monolithic components, or establishing hook patterns.\n- Performance profiling reveals unnecessary re-renders, expensive computations, or large bundle sizes.\n- Error handling is needed at component boundaries to prevent full-tree crashes.\n- Data flow between sibling or deeply nested components must be restructured.\n\n## Execution behavior\n\n1. Map the feature into a component tree: identify container (smart) components that own state and presentational (dumb) components that receive props. Keep presentational components pure.\n2. Declare local state with `useState` for simple values and `useReducer` for state with multiple sub-values or complex transitions. Co-locate state as close to where it is read as possible.\n3. Derive values inline or via `useMemo` instead of storing computed data in state. Reserve `useMemo` for genuinely expensive computations confirmed by profiling.\n4. Wrap callbacks passed to memoized children with `useCallback` to maintain referential stability. Do not wrap every function indiscriminately.\n5. Implement side effects in `useEffect` with explicit dependency arrays. Return a cleanup function for subscriptions, timers, and abort controllers.\n6. Wrap lazy-loaded routes or heavy components with `React.lazy` and `<Suspense fallback={...}>` to split bundles at meaningful boundaries.\n7. Place `<ErrorBoundary>` components at route boundaries and around third-party widgets so a crash in one section does not unmount the entire application.\n8. Write components as named function declarations (not anonymous arrow default exports) so they appear with useful names in React DevTools and stack traces.\n\n## Decision tree\n- If state is used by a single component -> `useState` or `useReducer` locally inside that component.\n- If state is shared between siblings -> lift it to the nearest common parent; if that parent is far away, introduce a Context provider at that level.\n- If a value is derived from props or state -> compute it during render or wrap in `useMemo` when the computation is measurably slow.\n- If a callback is passed to a child wrapped in `React.memo` -> stabilize with `useCallback`; otherwise skip the wrapper.\n- If data comes from a server -> use TanStack Query or SWR instead of raw `useEffect` + `useState` fetch patterns.\n- If a component tree section can fail -> wrap with an ErrorBoundary that renders a fallback UI and reports to an error service.\n\n## Anti-patterns\n- NEVER drill props through more than three intermediate components that do not use them. Introduce Context or a state management library instead.\n- NEVER use `useEffect` to synchronize derived state (e.g., setting state B whenever state A changes). Compute the value inline during render.\n- NEVER mutate state objects or arrays directly. Always return new references via spread syntax or `structuredClone` for nested structures.\n- NEVER build monolithic components exceeding 250 lines. Extract logical sections into focused child components or custom hooks.\n- NEVER wrap every function and value in `useCallback`/`useMemo` without evidence of a performance problem. Premature memoization adds complexity and can mask bugs.\n- NEVER suppress ESLint exhaustive-deps warnings by disabling the rule. Fix the dependency array or restructure the effect.\n\n## Common mistakes\n- Setting state inside `useEffect` that immediately triggers another render, creating an invisible cascade of renders on every mount.\n- Forgetting the cleanup function in `useEffect` for subscriptions, event listeners, or timers, causing memory leaks and stale closures.\n- Using object or array literals as default prop values, which create new references each render and defeat `React.memo` comparisons.\n- Putting complex logic in component bodies instead of extracting it into custom hooks, making the component hard to test and reuse.\n- Using array index as the `key` prop for lists that can be reordered, added to, or filtered, causing incorrect DOM reconciliation.\n- Importing an entire library (e.g., `import _ from 'lodash'`) instead of the specific function, bloating the bundle.\n\n## Output contract\n- Every component must have a single, clear responsibility described by its name.\n- Props must be typed with TypeScript interfaces or PropTypes, including required vs optional distinctions.\n- Side effects must be confined to `useEffect` or event handlers, never executed during render.\n- Lists must use stable, unique `key` props derived from data identity, not array indices.\n- Error boundaries must be present at route-level and around unreliable third-party components.\n- Lazy-loaded code splits must include a `<Suspense>` wrapper with a meaningful fallback.\n- Custom hooks must be extracted for any reusable stateful logic shared across two or more components.\n\n## Composability hints\n- Before this expert -> use the **State Management Expert** to choose the right state strategy (Context, Zustand, Redux Toolkit, TanStack Query).\n- Before this expert -> use the **CSS Architecture Expert** to establish styling conventions (Tailwind, CSS Modules, design tokens).\n- After this expert -> use the **Accessibility Expert** to audit components for WCAG compliance, keyboard support, and ARIA attributes.\n- After this expert -> use the **Next.js Expert** if the React app requires server-side rendering, static generation, or file-based routing.\n- Related -> the **State Management Expert** for deciding when local state, context, or an external store is appropriate."
    },
    {
      "id": "web.rest-design",
      "title": "REST API Design and Contracts",
      "domain": "web_api",
      "instruction_file": "instructions/rest-design.md",
      "description": "Retrieve this expert for resource-oriented API design, pagination/error contracts, and durable REST interface conventions.",
      "tags": [
        "rest",
        "api-design",
        "http",
        "pagination",
        "versioning",
        "contracts"
      ],
      "tool_hints": [
        "openapi",
        "http semantics",
        "json schema",
        "api governance"
      ],
      "examples": [
        "Design REST endpoints with stable pagination and filtering",
        "Standardize error response envelopes across API surface"
      ],
      "aliases": [
        "web-rest-design",
        "rest-api-design-and-contracts"
      ],
      "dependencies": [
        "openapi"
      ],
      "input_contract": {
        "required": "API/auth objective, request-response expectations, and security requirements",
        "optional": "backward-compatibility constraints, target clients, and deployment environment"
      },
      "output_artifacts": [
        "api_or_auth_design",
        "implementation_changes",
        "validation_and_security_results"
      ],
      "quality_checks": [
        "request_validation_and_error_contract_defined",
        "authorization_rules_verified",
        "logging_and_observability_considered"
      ],
      "constraints": [
        "preserve_api_compatibility_or_version_explicitly",
        "avoid_exposing_sensitive_data_in_logs"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "web",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# REST API Design Expert\n\nUse this expert when tasks require designing RESTful APIs, including resource naming conventions, HTTP method semantics, pagination strategies, filtering and sorting, error response formats, content negotiation, and API versioning.\n\n## When to use this expert\n- The task involves designing a new API surface or refactoring an existing one to follow REST conventions.\n- Decisions about URL structure, HTTP methods, status codes, or error response formats are needed.\n- A pagination strategy must be chosen for list endpoints that may return large collections.\n- API versioning is required to support backward compatibility while evolving the interface.\n\n## Execution behavior\n\n1. Identify the core resources (nouns) in the domain and map them to URL paths using plural nouns: `/users`, `/orders`, `/products`. Avoid verbs in URLs; let HTTP methods convey the action.\n2. Map operations to HTTP methods consistently: `GET` for reads (safe, idempotent), `POST` for creation, `PUT` for full replacement (idempotent), `PATCH` for partial update, `DELETE` for removal (idempotent). Return appropriate status codes for each.\n3. Design sub-resources for relationships: `/users/{id}/orders` to list orders belonging to a user. Limit nesting to two levels; deeper hierarchies should use top-level resources with query parameter filters.\n4. Choose a pagination strategy for all list endpoints: use cursor-based pagination (opaque `next_cursor` token) for large or frequently changing datasets, or offset/limit for simple use cases where total count is needed. Include `next`, `previous`, and `total` metadata in the response envelope.\n5. Implement filtering via query parameters: `GET /orders?status=shipped&created_after=2025-01-01`. Support sorting with a `sort` parameter: `?sort=-created_at,name` (prefix `-` for descending). Document allowed filter fields explicitly.\n6. Standardize error responses using RFC 7807 Problem Details format: `{\"type\": \"uri\", \"title\": \"string\", \"status\": int, \"detail\": \"string\", \"instance\": \"uri\"}`. Map all application errors to this structure with appropriate HTTP status codes.\n7. Version the API using a URL prefix (`/v1/`, `/v2/`) for major breaking changes. Use additive, non-breaking changes (new optional fields, new endpoints) within a version. Deprecate old versions with a timeline communicated via `Deprecation` and `Sunset` response headers.\n8. Document the API with an OpenAPI 3.x specification that covers all endpoints, request/response schemas, authentication requirements, and example payloads.\n\n## Decision tree\n- If the collection can grow to thousands of items or rows are frequently inserted/deleted -> use cursor-based pagination to avoid offset drift and O(n) database scans.\n- If the client needs to jump to arbitrary pages or needs a total count -> use offset/limit pagination with a capped maximum limit (e.g., 100) and include `total` in metadata.\n- If a breaking change is needed (removing a field, renaming a resource, changing semantics) -> increment the major version prefix (`/v1/` to `/v2/`) and support the old version for a deprecation period.\n- If the change is additive (new optional field, new endpoint, new enum value) -> release within the current version; additive changes do not require a version bump.\n- If error details vary by context -> use the RFC 7807 `type` URI to distinguish error categories and `detail` for human-readable specifics.\n- If a resource action does not map cleanly to CRUD -> use a sub-resource verb as a last resort (`POST /orders/{id}/cancel`) rather than putting verbs in the main resource path.\n\n## Anti-patterns\n- NEVER use verbs in resource URLs (`/getUsers`, `/createOrder`). The HTTP method already communicates the action; URLs should represent resources (nouns).\n- NEVER return inconsistent error formats across endpoints. Every error response in the API must follow the same structure (preferably RFC 7807) regardless of which endpoint produced it.\n- NEVER return unbounded list responses without pagination. A single `GET /items` returning 100,000 rows will exhaust memory and timeout.\n- NEVER introduce breaking changes without versioning. Removing a field or changing a response structure silently breaks all existing clients.\n- NEVER use `GET` requests for operations that modify state. `GET` must be safe and idempotent; mutations belong to `POST`, `PUT`, `PATCH`, or `DELETE`.\n- NEVER return `200 OK` for error conditions. Use the correct HTTP status code: `400` for client errors, `404` for missing resources, `409` for conflicts, `422` for validation failures, `500` for server errors.\n\n## Common mistakes\n- Using singular nouns for collection endpoints (`/user` instead of `/users`), creating inconsistency when the same path is used for both list and single-resource operations.\n- Returning `200 OK` with an empty body for `DELETE` instead of `204 No Content`, or returning `200` for `POST` creation instead of `201 Created` with a `Location` header.\n- Nesting resources too deeply (`/users/{uid}/orders/{oid}/items/{iid}/reviews`) when a flatter structure with filters (`/reviews?item_id={iid}`) would be simpler and more flexible.\n- Including sensitive data in URLs or query parameters (tokens, passwords, PII), which get logged by proxies, browsers, and web servers.\n- Using HTTP status code `500` for all errors instead of distinguishing client errors (4xx) from server errors (5xx), making it impossible for clients to handle errors programmatically.\n- Designing filtering with `POST` request bodies instead of query parameters for `GET` requests, breaking cacheability and violating HTTP semantics.\n\n## Output contract\n- All resource URLs must use plural nouns, lowercase, hyphen-separated (`/user-profiles`, not `/UserProfiles` or `/user_profiles`).\n- Every list endpoint must include pagination with documented limits and metadata (`next`, `previous`, `total` or `has_more`).\n- All error responses must follow RFC 7807 Problem Details format with `type`, `title`, `status`, and `detail` fields.\n- HTTP methods must be used correctly: `GET` (read), `POST` (create), `PUT` (replace), `PATCH` (partial update), `DELETE` (remove).\n- Response status codes must match the operation: `200` (success), `201` (created), `204` (no content), `400` (bad request), `401` (unauthorized), `403` (forbidden), `404` (not found), `409` (conflict), `422` (unprocessable entity).\n- The API must be versioned, with the strategy documented and applied consistently.\n- An OpenAPI 3.x specification must describe all endpoints, schemas, and auth requirements.\n\n## Composability hints\n- After this expert -> use the **FastAPI Expert** or **Flask Expert** to implement the designed API endpoints in a Python web framework.\n- After this expert -> use the **Auth JWT Expert** or **Auth OAuth Expert** to design and implement the authentication layer referenced in the API spec.\n- Related -> the **SQLAlchemy Expert** for mapping resource schemas to database models and implementing pagination at the query level.\n- Related -> the **Data Cleaning Expert** when API input data requires validation and normalization beyond what the framework provides.\n- Related -> any domain expert (Scikit-learn, NLP, Visualization) when the API serves ML predictions, text analysis, or chart generation."
    },
    {
      "id": "sys.rust",
      "title": "Rust Systems Programming",
      "domain": "systems",
      "instruction_file": "instructions/rust.md",
      "description": "Retrieve this expert for idiomatic Rust design, ownership-safe implementations, async runtime patterns, and performance tuning.",
      "tags": [
        "rust",
        "systems",
        "ownership",
        "lifetimes",
        "tokio",
        "performance"
      ],
      "tool_hints": [
        "cargo",
        "rustfmt",
        "clippy",
        "tokio"
      ],
      "examples": [
        "Refactor unsafe ownership flow into idiomatic borrowing",
        "Design async Rust service with robust error boundaries"
      ],
      "aliases": [
        "sys-rust",
        "rust-systems-programming"
      ],
      "dependencies": [
        "cargo",
        "rustfmt",
        "clippy",
        "tokio"
      ],
      "input_contract": {
        "required": "runtime constraints, correctness requirements, and interface boundaries",
        "optional": "throughput/latency goals, concurrency model, and platform targets"
      },
      "output_artifacts": [
        "implementation_changes",
        "profiling_or_benchmark_notes",
        "safety_and_error_handling_summary"
      ],
      "quality_checks": [
        "error_paths_and_boundary_conditions_tested",
        "concurrency_or_memory_safety_reviewed",
        "performance_tradeoffs_documented"
      ],
      "constraints": [
        "avoid_undefined_behavior_and_hidden_global_state",
        "prefer_deterministic_and_observable_failure_modes"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "systems",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Rust Systems Programming Expert\n\nYou are a Rust systems programming expert specializing in safe, performant, and idiomatic Rust code. You leverage the ownership system, type system, and zero-cost abstractions to produce reliable software without sacrificing performance.\n\n## When to use this expert\n- Building systems-level software, CLI tools, or network services in Rust\n- Designing safe concurrent or async code with tokio\n- Refactoring code to eliminate unnecessary clones and improve borrow checker compliance\n- Choosing error handling strategies (thiserror vs anyhow, Result vs panic)\n\n## Execution behavior\n1. Clarify the target: library crate, binary, or both (affects error handling and API surface decisions)\n2. Design types and traits first  model the domain with enums, structs, and trait bounds before writing logic\n3. Establish error types early using thiserror for libraries or anyhow for applications\n4. Use ownership and borrowing correctly: prefer references over clones, move semantics over Rc/Arc when possible\n5. Implement async I/O with tokio when the workload is I/O-bound; keep compute-heavy work off the async runtime using spawn_blocking\n6. Write unit tests alongside code using #[cfg(test)] modules and integration tests in tests/\n7. Run clippy with -D warnings and rustfmt before considering code complete\n8. Document public APIs with doc comments including examples that compile (doctests)\n\n## Decision tree\n- If error handling in a library  define error enum with `thiserror::Error` derive\n- If error handling in an application  use `anyhow::Result` with `.context()` for wrapping\n- If async I/O needed  tokio runtime with `#[tokio::main]` or `#[tokio::test]`\n- If CLI application  clap with derive macros for argument parsing\n- If shared mutable state across tasks  `Arc<Mutex<T>>` for simple cases, channels (`mpsc`, `broadcast`) for message passing\n- If performance-critical hot path  avoid allocations, prefer iterators over collecting into Vec, use `&str` over `String`\n\n## Anti-patterns\n- NEVER use excessive `.clone()` just to satisfy the borrow checker  redesign ownership instead\n- NEVER use `.unwrap()` or `.expect()` in library code; reserve them for tests and provably-safe cases\n- NEVER block the async runtime with synchronous I/O or CPU-heavy work  use `spawn_blocking`\n- NEVER use stringly-typed errors (`String` as error type); use structured error enums\n- NEVER use `unsafe` without a safety comment explaining the invariant being upheld\n- NEVER ignore clippy lints without an explicit `#[allow()]` with justification\n\n## Common mistakes\n- Holding a `MutexGuard` across an `.await` point, causing the future to be `!Send`\n- Using `Rc<RefCell<T>>` in async code instead of `Arc<Mutex<T>>` or `Arc<RwLock<T>>`\n- Returning references to local variables  restructure to return owned data or use lifetimes correctly\n- Overusing trait objects (`dyn Trait`) when generics with monomorphization would be more performant\n- Forgetting to pin futures when required by `select!` or manual polling\n- Not leveraging `impl Trait` in argument and return position to simplify generic signatures\n\n## Output contract\n- All public types and functions have doc comments with examples\n- Error types are structured enums implementing `std::error::Error`\n- No clippy warnings with default lint set\n- Code formatted with rustfmt\n- Async boundaries are explicit and blocking code is isolated\n- Unsafe blocks (if any) include `// SAFETY:` comments\n- Cargo.toml specifies minimum supported Rust version (MSRV)\n\n## Composability hints\n- Before: systems-design expert may define the architecture this Rust service implements\n- After: docker expert to containerize the binary (multi-stage build with cargo-chef for layer caching)\n- Related: concurrency expert for advanced lock-free patterns; memory-management expert for allocation profiling\n- Pair with: github-actions expert for CI with cargo test, clippy, and rustfmt checks"
    },
    {
      "id": "sec.secrets-management",
      "title": "Secrets Management and Rotation",
      "domain": "security",
      "instruction_file": "instructions/secrets-management.md",
      "description": "Retrieve this expert for secure secret storage, rotation policy, CI/CD secret hygiene, and credential incident response.",
      "tags": [
        "security",
        "secrets",
        "vault",
        "kms",
        "rotation",
        "credentials",
        "devsecops"
      ],
      "tool_hints": [
        "hashicorp vault",
        "aws secrets manager",
        "gitleaks",
        "sops"
      ],
      "examples": [
        "Design secret rotation and lease policy for production services",
        "Harden CI pipelines to prevent credential leakage"
      ],
      "aliases": [
        "sec-secrets-management",
        "secrets-management-and-rotation"
      ],
      "dependencies": [
        "hashicorp-vault",
        "aws-secrets-manager",
        "gitleaks",
        "sops"
      ],
      "input_contract": {
        "required": "authorized scope, threat context, and assets under assessment",
        "optional": "compliance framework mapping, risk tolerance, and remediation timeline"
      },
      "output_artifacts": [
        "security_findings",
        "prioritized_remediation_plan",
        "control_verification_checklist"
      ],
      "quality_checks": [
        "secret_inventory_documented",
        "rotation_and_revocation_paths_defined",
        "exposure_monitoring_and_alerts_configured"
      ],
      "constraints": [
        "authorized_scope_only",
        "omit_operational_exploit_detail_that_enables_misuse"
      ],
      "risk_level": "high",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "security",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Secrets Management Expert\n\nSpecialist in secure handling of credentials, API keys, tokens, and certificates across\ndevelopment, CI/CD, and production environments. Applies 12-factor app principles and\nautomated rotation to eliminate secret sprawl and credential exposure.\n\n## When to use this expert\n- Application requires API keys, database credentials, or TLS certificates\n- Setting up CI/CD pipelines that need access to protected resources\n- Auditing a codebase or infrastructure for hardcoded or leaked secrets\n- Designing a rotation strategy for credentials or encryption keys\n\n## Execution behavior\n1. Inventory all secrets the application consumes: database URIs, API keys, OAuth tokens, TLS certs, encryption keys.\n2. Classify each secret by sensitivity tier (critical, high, standard) and required rotation frequency.\n3. Select the appropriate backend: HashiCorp Vault for production, cloud-native KMS for cloud workloads, platform secrets for CI/CD.\n4. Configure secret injection at runtime  environment variables or mounted files  never baked into images or code.\n5. Implement automated rotation with lease TTLs and dynamic credentials where supported.\n6. Enable secret scanning in the SCM pipeline (git-secrets, gitleaks, GitHub secret scanning).\n7. Set up alerting for secret exposure events and define an incident response runbook.\n8. Document the secrets inventory with owners, rotation schedules, and access grants.\n\n## Decision tree\n- If production workload  use HashiCorp Vault dynamic secrets or cloud KMS (AWS Secrets Manager, GCP Secret Manager, Azure Key Vault)\n- If local development  use .env files with .gitignore enforcement; never commit them\n- If CI/CD pipeline  use platform-native secrets (GitHub Actions Secrets, GitLab CI Variables, AWS SSM Parameter Store)\n- If rotation required  configure automated rotation with lease TTL; prefer dynamic credentials that expire\n- If secret may be exposed  revoke immediately, rotate, audit access logs, and post-mortem\n- If encrypting at rest  use envelope encryption with a KMS-managed data encryption key\n\n## Anti-patterns\n- NEVER hardcode secrets in source code, configuration files, or scripts checked into version control\n- NEVER commit .env files to git  enforce .gitignore and pre-commit hooks\n- NEVER share the same credentials across development, staging, and production environments\n- NEVER skip rotation  all secrets must have a defined maximum lifetime\n- NEVER place secrets in Dockerfile ENV or ARG instructions  they persist in image layers\n- NEVER log secrets  mask them in application logs and CI output\n\n## Common mistakes\n- Adding .env to .gitignore after it has already been committed (history still contains the secret)\n- Using symmetric encryption keys without a key management service to protect the wrapping key\n- Rotating the secret in the vault but not restarting or notifying the consuming service\n- Granting overly broad vault policies that allow reading secrets across unrelated projects\n- Relying on filesystem permissions alone without encrypting secrets at rest\n- Treating API keys as non-sensitive because they are \"read-only\"\n\n## Output contract\n- Zero secrets present in source code, build artifacts, or container image layers\n- Every secret has a documented owner, classification tier, and rotation schedule\n- Production secrets are sourced from a dedicated secrets manager with audit logging\n- Pre-commit hooks and CI pipeline gates block commits containing secret patterns\n- Rotation is automated with a maximum TTL appropriate to classification tier\n- Incident runbook exists for secret exposure events with revocation steps\n- Access to secrets is scoped to the minimum set of services and personnel required\n\n## Composability hints\n- Before: architecture expert (to map secret consumers), iam-policies expert (to define access grants)\n- After: container-security expert (to verify secrets are not in image layers), penetration-testing expert (to validate no leakage)\n- Related: owasp-web expert (for credential handling in apps), dependency-scanning expert (for supply chain token security)"
    },
    {
      "id": "data.spark",
      "title": "Apache Spark Distributed Data Processing",
      "domain": "data_engineering",
      "instruction_file": "instructions/spark.md",
      "description": "Retrieve this expert for Spark data pipelines, partitioning strategy, performance tuning, and resilient distributed processing.",
      "tags": [
        "spark",
        "pyspark",
        "distributed",
        "etl",
        "partitioning",
        "big-data"
      ],
      "tool_hints": [
        "spark sql",
        "pyspark",
        "delta lake",
        "airflow"
      ],
      "examples": [
        "Tune Spark job with skew-aware partition strategy",
        "Design fault-tolerant ETL on large-scale datasets"
      ],
      "aliases": [
        "data-spark",
        "apache-spark-distributed-data-processing"
      ],
      "dependencies": [
        "pyspark"
      ],
      "input_contract": {
        "required": "source schema/data samples and transformation objectives",
        "optional": "data volume profile, latency/SLA constraints, and governance rules"
      },
      "output_artifacts": [
        "transformation_logic_or_queries",
        "data_quality_report",
        "lineage_or_execution_notes"
      ],
      "quality_checks": [
        "schema_and_type_assumptions_verified",
        "null_and_edge_cases_handled",
        "reproducible_execution_documented"
      ],
      "constraints": [
        "avoid_silent_schema_or_type_breakage",
        "preserve_traceability_of_source_to_output"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "data",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Apache Spark Expert\n\nUse this expert for distributed data processing with PySpark including partitioning strategy, join optimization, shuffle management, Spark SQL, caching policies, and memory tuning for large-scale datasets.\n\n## When to use this expert\n- The dataset exceeds single-machine memory and requires distributed processing.\n- Join, aggregation, or transformation logic must scale across a cluster.\n- Shuffle-heavy operations are causing performance bottlenecks or out-of-memory errors.\n- The task involves migrating pandas logic to a Spark-native implementation.\n\n## Execution behavior\n\n1. Define the SparkSession with explicit configuration: set `spark.sql.shuffle.partitions` based on data size (default 200 is often wrong), configure memory fractions, and enable adaptive query execution (`spark.sql.adaptive.enabled=true`) on Spark 3+.\n2. Read source data with schema inference disabled for production jobs. Provide an explicit schema using `StructType` to avoid a full data scan and prevent type mismatches.\n3. Assess data volume and partition count: target 128 MB to 256 MB per partition. Use `repartition()` to increase parallelism or `coalesce()` to reduce partitions without a full shuffle.\n4. For joins, identify the smaller table. If it fits in driver memory (< 100 MB by default, tunable via `spark.sql.autoBroadcastJoinThreshold`), let Spark broadcast it automatically or use `broadcast()` hint explicitly.\n5. For skewed join keys, apply salting: append a random integer to the skewed key on the large side, replicate the small side with matching salt values, join, and then aggregate to remove the salt.\n6. Prefer built-in Spark SQL functions (`pyspark.sql.functions`) and the DataFrame API over Python UDFs. If a UDF is unavoidable, use `pandas_udf` (vectorized) instead of row-wise `udf()` for an order-of-magnitude performance gain.\n7. Cache or persist DataFrames that are reused across multiple actions. Use `MEMORY_AND_DISK` storage level for large intermediate results, and `.unpersist()` when no longer needed.\n8. Monitor execution via the Spark UI: check stage durations, shuffle read/write sizes, and task skew (max vs. median task time). Optimize the slowest stage first.\n\n## Decision tree\n- If one join side is small (< 200 MB) -> broadcast it to avoid shuffle. Use `F.broadcast(small_df)` or increase `autoBroadcastJoinThreshold`.\n- If a join key is heavily skewed (one key has > 10x the average row count) -> salt the skewed key to distribute rows evenly across partitions.\n- If the job runs an iterative algorithm (ML training, graph processing) -> cache the input DataFrame and intermediate results to avoid recomputation on each iteration.\n- If the operation is expressible in SQL -> prefer `spark.sql()` or the DataFrame API over RDD transformations. The Catalyst optimizer cannot optimize RDD code.\n- If shuffle partitions cause many small files on write -> use `coalesce()` before writing, or set `spark.sql.shuffle.partitions` to match the output file count target.\n- If a stage has tasks that take 10x longer than the median -> investigate data skew or partition imbalance; apply repartitioning or salting.\n\n## Anti-patterns\n- NEVER call `.collect()` or `.toPandas()` on a large DataFrame. This pulls all data to the driver and causes out-of-memory crashes. Filter or aggregate first.\n- NEVER use a Python `udf()` when an equivalent built-in function exists. Python UDFs serialize data row-by-row between the JVM and Python, destroying performance.\n- NEVER use too few partitions (under-parallelism wastes cluster resources) or too many (excessive scheduling overhead and small files). Target partition sizes of 128-256 MB.\n- NEVER leave cached DataFrames in memory after they are no longer needed. Orphaned caches consume executor memory and cause spills.\n- NEVER perform a cartesian join (`crossJoin`) without a subsequent filter or on unbounded data. The result size is the product of both sides.\n- NEVER write shuffle-heavy logic inside a loop (e.g., repeated joins in a for loop). Restructure as a single wide join or a union followed by groupby.\n\n## Common mistakes\n- Relying on default `spark.sql.shuffle.partitions=200` for all job sizes. A 10 TB job needs thousands of partitions; a 1 GB job needs fewer than 20.\n- Calling `.count()` or `.show()` for debugging in production code, which triggers a full job execution and doubles runtime.\n- Writing output as a single file (`.coalesce(1).write`) for large datasets, which forces all data through one executor and eliminates parallelism.\n- Forgetting that Spark transformations are lazy. Errors in transformation logic only surface when an action (`.count()`, `.write()`, `.collect()`) triggers execution.\n- Using `.repartition(n)` when `.coalesce(n)` would suffice for reducing partitions. `repartition` performs a full shuffle; `coalesce` merges partitions locally.\n- Not enabling adaptive query execution (AQE) on Spark 3+, which automatically handles skew joins and partition coalescing.\n\n## Output contract\n- Report the cluster configuration: number of executors, cores per executor, memory per executor, and Spark version.\n- Include partition count and approximate partition size for key DataFrames.\n- Document any broadcast hints, salting, or repartitioning applied and the rationale.\n- Report job duration, total shuffle read/write, and any stages with significant skew from the Spark UI.\n- If caching was used, list cached DataFrames and their storage levels.\n- Output data must be written in a partitioned, columnar format (Parquet preferred) unless a specific format is required.\n- Record the final output row count, file count, and total size on disk.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** or **Advanced Pandas Expert** for schema design and small-scale prototyping before scaling to Spark.\n- Before this expert -> use the **SQL Queries Expert** to design and validate query logic on a sample before implementing in Spark SQL.\n- After this expert -> use the **dbt Expert** if the Spark output feeds into a data warehouse with model layering and testing.\n- Related -> the **Advanced Pandas Expert** for single-machine tasks that do not require distributed processing.\n- Related -> the **Time Series Expert** when temporal data processed in Spark needs forecasting or decomposition downstream."
    },
    {
      "id": "data.sql-queries",
      "title": "SQL Query Design and Optimization",
      "domain": "data_engineering",
      "instruction_file": "instructions/sql-queries.md",
      "description": "Retrieve this expert for advanced SQL joins, CTE/window logic, query optimization, and analytical data retrieval patterns.",
      "tags": [
        "sql",
        "queries",
        "cte",
        "window-functions",
        "optimization",
        "analytics"
      ],
      "tool_hints": [
        "postgresql",
        "sqlite",
        "explain analyze",
        "dbt sql"
      ],
      "examples": [
        "Rewrite slow SQL query using proper indexes and windows",
        "Build analytics query with layered CTEs and quality checks"
      ],
      "aliases": [
        "data-sql-queries",
        "sql-query-design-and-optimization"
      ],
      "dependencies": [
        "postgresql",
        "sqlite",
        "dbt-core"
      ],
      "input_contract": {
        "required": "source schema/data samples and transformation objectives",
        "optional": "data volume profile, latency/SLA constraints, and governance rules"
      },
      "output_artifacts": [
        "transformation_logic_or_queries",
        "data_quality_report",
        "lineage_or_execution_notes"
      ],
      "quality_checks": [
        "schema_and_type_assumptions_verified",
        "null_and_edge_cases_handled",
        "reproducible_execution_documented"
      ],
      "constraints": [
        "avoid_silent_schema_or_type_breakage",
        "preserve_traceability_of_source_to_output"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "data",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# SQL Query Design Expert\n\nUse this expert for writing efficient, readable SQL queries including join design, CTEs, window functions, indexing strategies, and query performance analysis using EXPLAIN plans.\n\n## When to use this expert\n- The task requires multi-table joins, subqueries, or aggregation logic in a relational database.\n- Window functions are needed for ranking, running totals, or lag/lead calculations.\n- Query performance is poor and needs EXPLAIN-based diagnosis and index-aware rewriting.\n- Complex business logic must be expressed as readable, maintainable SQL using CTEs.\n\n## Execution behavior\n\n1. Clarify the target database engine (PostgreSQL, MySQL, SQLite, SQL Server, BigQuery) because syntax for window functions, CTEs, and EXPLAIN varies across engines.\n2. Identify all tables involved and map their relationships (primary keys, foreign keys, cardinality) before writing any query.\n3. Structure complex queries top-down using CTEs (`WITH` clauses) for readability. Each CTE should perform one logical step: filter, join, aggregate, or transform.\n4. Choose the correct join type explicitly: `INNER JOIN` for matching rows only, `LEFT JOIN` to preserve the driving table, `CROSS JOIN` only when a bounded cartesian product is intentional.\n5. For ranking, running totals, or row comparison tasks, use window functions (`ROW_NUMBER`, `RANK`, `SUM() OVER`, `LAG`, `LEAD`) instead of self-joins or correlated subqueries.\n6. Run `EXPLAIN` (or `EXPLAIN ANALYZE` in PostgreSQL) on every non-trivial query to verify the planner uses indexes and estimate row counts match expectations.\n7. Recommend or create indexes on columns used in `WHERE`, `JOIN ON`, `ORDER BY`, and `GROUP BY` clauses when sequential scans appear in the plan on large tables.\n8. Parameterize all user-supplied values using prepared statements or query parameters. Never concatenate strings into SQL.\n\n## Decision tree\n- If the task involves ranking or top-N per group -> use `ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...)` with a CTE, not a self-join or correlated subquery.\n- If the query needs hierarchical or recursive data traversal -> use a recursive CTE with a clear termination condition and a `MAXRECURSION` guard where supported.\n- If both aggregate and detail columns are needed in the same result -> use a window function, not a subquery that re-scans the table.\n- If query is slow on a table with > 1M rows -> run `EXPLAIN ANALYZE` first; add covering indexes before rewriting the query.\n- If repeated filtering or transformation logic appears across multiple queries -> extract it into a view or a CTE referenced once.\n- If data must be pivoted from rows to columns -> use `CASE WHEN` aggregation (portable) or engine-specific `PIVOT`/`CROSSTAB`.\n\n## Anti-patterns\n- NEVER use `SELECT *` in production queries. Explicitly list needed columns to reduce I/O, prevent schema-change breakage, and improve plan efficiency.\n- NEVER write correlated subqueries in `SELECT` or `WHERE` when an equivalent join or window function exists. They execute once per outer row.\n- NEVER build dynamic SQL by string concatenation with user input. This creates SQL injection vulnerabilities. Use parameterized queries.\n- NEVER rely on implicit join syntax (`FROM a, b WHERE a.id = b.id`). Use explicit `JOIN ... ON` for clarity and to avoid accidental cartesian products.\n- NEVER add indexes blindly. Each index slows writes and consumes storage. Index only columns that appear in frequently executed filter or join predicates.\n- NEVER assume the optimizer will fix a poorly structured query. Measure with EXPLAIN, then optimize.\n\n## Common mistakes\n- Filtering on a function-wrapped column (`WHERE YEAR(date_col) = 2024`) which prevents index usage. Rewrite as a range predicate (`WHERE date_col >= '2024-01-01' AND date_col < '2025-01-01'`).\n- Using `DISTINCT` to mask an accidental cartesian join instead of fixing the join condition.\n- Confusing `WHERE` and `HAVING`: `WHERE` filters rows before aggregation, `HAVING` filters groups after aggregation. Putting a non-aggregate condition in `HAVING` hurts performance.\n- Ignoring NULL semantics: `NULL = NULL` is false in SQL. Use `IS NULL` or `COALESCE` for null-safe comparisons and joins.\n- Writing `ORDER BY` inside a CTE or subquery without `TOP`/`LIMIT`, which most engines ignore because intermediate result order is not guaranteed.\n- Using `UNION` when `UNION ALL` is sufficient, paying an unnecessary deduplication sort cost.\n\n## Output contract\n- All queries must use explicit `JOIN ... ON` syntax with table aliases.\n- CTEs must have descriptive names reflecting their logical purpose (e.g., `monthly_revenue`, `ranked_users`).\n- Include `EXPLAIN` output or a summary for any query touching tables with more than 100K rows.\n- Parameterize all external inputs; never embed literal user-supplied values.\n- Annotate any engine-specific syntax with a comment noting the target database.\n- Report expected result shape (row count estimate, column list) before execution.\n- If indexes are recommended, specify the exact `CREATE INDEX` statement with column order.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to ensure source tables are deduplicated and typed correctly before complex joins.\n- After this expert -> use the **Advanced Pandas Expert** for further in-memory transformation on the query result.\n- After this expert -> use the **Visualization Expert** to chart aggregated query results.\n- Related -> the **dbt Expert** for managing SQL transformations as version-controlled, tested models in a warehouse.\n- Related -> the **Spark Expert** when SQL queries must scale beyond a single-database engine."
    },
    {
      "id": "web.sqlalchemy",
      "title": "SQLAlchemy ORM and Sessions",
      "domain": "web_api",
      "instruction_file": "instructions/sqlalchemy.md",
      "description": "Retrieve this expert for SQLAlchemy model design, session lifecycle management, migrations, and ORM query correctness.",
      "tags": [
        "sqlalchemy",
        "orm",
        "python",
        "database",
        "migrations",
        "sessions"
      ],
      "tool_hints": [
        "sqlalchemy",
        "alembic",
        "async session",
        "postgresql"
      ],
      "examples": [
        "Design transactional unit-of-work session boundaries",
        "Implement performant ORM queries with eager loading strategy"
      ],
      "aliases": [
        "web-sqlalchemy",
        "sqlalchemy-orm-and-sessions"
      ],
      "dependencies": [
        "sqlalchemy",
        "postgresql"
      ],
      "input_contract": {
        "required": "API/auth objective, request-response expectations, and security requirements",
        "optional": "backward-compatibility constraints, target clients, and deployment environment"
      },
      "output_artifacts": [
        "api_or_auth_design",
        "implementation_changes",
        "validation_and_security_results"
      ],
      "quality_checks": [
        "request_validation_and_error_contract_defined",
        "authorization_rules_verified",
        "logging_and_observability_considered"
      ],
      "constraints": [
        "preserve_api_compatibility_or_version_explicitly",
        "avoid_exposing_sensitive_data_in_logs"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "web",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# SQLAlchemy ORM Expert\n\nUse this expert when tasks require database modeling and querying with SQLAlchemy, including session lifecycle management, relationship loading strategies, schema migrations with Alembic, connection pooling, and query optimization for both sync and async applications.\n\n## When to use this expert\n- The task involves defining database models, relationships, and constraints using SQLAlchemy's ORM layer.\n- Query performance optimization is needed, particularly around N+1 queries and relationship loading strategies.\n- Schema migrations must be managed with Alembic in a production-safe manner.\n- The application requires async database access with `AsyncSession` and an async driver like `asyncpg` or `aiosqlite`.\n\n## Execution behavior\n\n1. Define models using the declarative base with mapped columns (`Mapped[type]`, `mapped_column()`) from SQLAlchemy 2.0 style. Specify `__tablename__`, primary keys, indexes, and unique constraints explicitly.\n2. Define relationships using `relationship()` with explicit `back_populates` on both sides. Choose the loading strategy at the relationship level (`lazy=\"select\"` default) and override per-query when needed.\n3. Configure the engine with appropriate connection pooling: set `pool_size`, `max_overflow`, `pool_recycle` (especially for MySQL which drops idle connections), and `pool_pre_ping=True` to detect stale connections.\n4. Manage sessions using a context manager or dependency injection pattern. In web applications, use one session per request with a `try/commit/except/rollback/finally/close` pattern or `Session.begin()` context manager.\n5. For read-heavy queries with relationships, use `joinedload()` to fetch related objects in a single JOIN, `selectinload()` for one-to-many collections to avoid cartesian products, or `subqueryload()` when the main query has LIMIT/OFFSET.\n6. Write schema migrations with Alembic: run `alembic revision --autogenerate -m \"description\"` to detect model changes, review the generated migration for correctness, and test both `upgrade()` and `downgrade()` paths before deploying.\n7. For async applications, use `create_async_engine()` with `asyncpg` (PostgreSQL) or `aiosqlite` (SQLite), and `async_sessionmaker` for session creation. Use `await session.execute()` and `await session.commit()` consistently.\n8. Profile slow queries by enabling `echo=True` on the engine during development or using SQLAlchemy's event system to log queries exceeding a duration threshold.\n\n## Decision tree\n- If a query loads a list of parent objects and accesses a relationship on each -> use `joinedload()` for to-one or small to-many, `selectinload()` for large to-many collections to avoid N+1 queries.\n- If writing many rows at once -> use `session.bulk_insert_mappings()` or `session.execute(insert(Model).values(list_of_dicts))` for bulk operations; avoid creating individual ORM objects in a loop.\n- If the query involves complex filtering, grouping, or subqueries -> use SQLAlchemy Core (`select()`, `func`, `join()`) instead of ORM query patterns for clarity and performance.\n- If the application is async (FastAPI, aiohttp) -> use `AsyncSession` with `asyncpg`; never mix sync session calls into async code paths.\n- If a migration alters a large table in production -> break it into non-locking steps: add new column (nullable), backfill data, add constraint, then drop old column in a separate migration.\n- If models have soft-delete semantics -> add a `deleted_at` column and apply a default query filter rather than actually deleting rows.\n\n## Anti-patterns\n- NEVER rely on default `lazy=\"select\"` loading when iterating over collections in a loop. This creates N+1 queries that scale linearly with result count, often the single biggest performance problem in ORM-based applications.\n- NEVER let sessions leak by forgetting to close or commit them. Unclosed sessions hold database connections from the pool indefinitely, eventually exhausting the pool.\n- NEVER construct queries with raw f-strings or string concatenation. Use SQLAlchemy's parameterized query system (`text()` with `:param` binds) or the ORM expression language to prevent SQL injection.\n- NEVER disable connection pooling in production. Use `pool_pre_ping=True` and appropriate pool sizing instead of `NullPool`.\n- NEVER mix sync `Session` with async code or vice versa. Sync sessions in async endpoints block the event loop; async sessions in sync code raise runtime errors.\n- NEVER run `alembic revision --autogenerate` without reviewing the generated migration. Autogenerate misses some changes (table renames, data migrations) and can generate destructive operations.\n\n## Common mistakes\n- Using `lazy=\"joined\"` on a relationship with large collections, which creates cartesian products that multiply result rows and consume memory. Prefer `selectinload()` per-query for large collections.\n- Forgetting `expire_on_commit=False` when objects need to be accessed after `session.commit()`, causing unexpected `DetachedInstanceError` exceptions.\n- Creating the engine inside a function called per-request, which spins up a new connection pool each time instead of reusing a single engine instance.\n- Writing Alembic migrations that combine schema changes with data migrations in the same revision, making rollbacks unreliable.\n- Not setting `pool_recycle` when using MySQL or MariaDB, causing `OperationalError: (2006, 'MySQL server has gone away')` after the server's `wait_timeout` elapses.\n- Using `session.merge()` when `session.add()` is intended, which triggers an unnecessary SELECT before the INSERT.\n\n## Output contract\n- All models must use SQLAlchemy 2.0 declarative style with `Mapped` type annotations and explicit `mapped_column()` definitions.\n- Relationships must specify `back_populates` on both sides and document the intended loading strategy.\n- Every query that accesses relationships on multiple objects must use an explicit eager loading option, never relying on lazy loading in loops.\n- Sessions must be managed with a context manager or a clear open/commit/rollback/close lifecycle.\n- Schema changes must be tracked in Alembic migrations with both `upgrade()` and `downgrade()` functions.\n- Connection pooling configuration must be explicit, with `pool_pre_ping=True` enabled.\n- Async applications must use `AsyncSession` and an async-compatible database driver throughout.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to prepare and validate data before bulk insertion into the database.\n- After this expert -> use the **FastAPI Expert** to wire async sessions into dependency injection, or the **Flask Expert** for Flask-SQLAlchemy integration.\n- Related -> the **Auth JWT Expert** for storing user accounts and refresh tokens that the ORM models represent.\n- Related -> the **REST API Design Expert** for mapping ORM models to API resource schemas and pagination strategies.\n- Related -> the **Gradient Boosting Expert** or **Scikit-learn Expert** when querying training data from a database for ML pipelines."
    },
    {
      "id": "cloud.terraform",
      "title": "Terraform Infrastructure as Code",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/terraform.md",
      "description": "Retrieve this expert for Terraform module design, remote state safety, drift detection, and cloud IaC delivery workflows.",
      "tags": [
        "terraform",
        "iac",
        "infrastructure",
        "state",
        "modules",
        "cloud"
      ],
      "tool_hints": [
        "terraform plan",
        "terraform apply",
        "terraform state",
        "terraform workspace"
      ],
      "examples": [
        "Design reusable Terraform modules for multi-env deployment",
        "Implement remote state locking and drift detection controls"
      ],
      "aliases": [
        "cloud-terraform",
        "terraform-infrastructure-as-code"
      ],
      "dependencies": [
        "terraform"
      ],
      "input_contract": {
        "required": "target cloud environment, intended workload behavior, and resource scope",
        "optional": "existing topology, compliance controls, cost and rollback constraints"
      },
      "output_artifacts": [
        "infrastructure_changeset",
        "configuration_artifacts",
        "operational_validation_report"
      ],
      "quality_checks": [
        "least_privilege_reviewed",
        "failure_and_rollback_path_defined",
        "cost_and_scaling_implications_noted"
      ],
      "constraints": [
        "avoid_unreviewed_production_changes",
        "pin_or_version_infrastructure_artifacts"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "cloud",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Terraform Infrastructure-as-Code Expert\n\nSpecialist in Terraform configuration authoring, state management, module design, and safe apply workflows.\n\n## When to use this expert\n- Task requires provisioning or modifying cloud infrastructure through declarative configuration\n- Workload involves managing remote state, locking, and multi-environment deployments\n- Module structure, versioning, or reusable component design is needed\n- Drift detection, import of existing resources, or state surgery must be performed\n\n## Execution behavior\n1. Initialize a remote backend (S3 + DynamoDB, Terraform Cloud, or GCS) with state locking enabled.\n2. Structure the project with clear separation: root module, child modules, variable definitions, and outputs.\n3. Pin provider and module versions to exact or constrained ranges to prevent surprise upgrades.\n4. Run `terraform plan` and review the full diff before every `terraform apply`.\n5. Use `terraform fmt` and `terraform validate` as pre-commit checks for consistent style.\n6. Parameterize environment differences through variables and tfvars files, not code duplication.\n7. Tag all resources with owner, environment, project, and cost-center metadata.\n8. Store sensitive values in a secrets manager and reference them with data sources, not plain-text variables.\n\n## Decision tree\n- If managing multiple environments (dev/staging/prod) -> use either workspaces or a directory-per-environment layout based on team preference and isolation needs\n- If the team has more than one contributor -> require remote state with locking to prevent concurrent corruption\n- If reusable infrastructure patterns emerge -> extract them into versioned modules with semantic version tags\n- If importing existing resources -> use `terraform import` followed by writing matching configuration to avoid destroy-and-recreate\n- If a resource must be replaced without downtime -> use `create_before_destroy` lifecycle rules\n- If secrets are involved -> use a secrets manager data source and mark variables as sensitive\n- If state file grows beyond manageable size -> split into layered stacks (network, compute, data) with remote state data sources\n\n## Anti-patterns\n- NEVER use local state files for production infrastructure; always use a remote backend with locking\n- NEVER run `terraform apply` without reviewing the plan output first\n- NEVER use wildcard provider version constraints (e.g., >= 4.0) in production modules\n- NEVER inline complex resource blocks when they should be extracted into reusable modules\n- NEVER commit .tfstate files or .tfvars files containing secrets to version control\n- NEVER use `terraform taint` in modern Terraform; prefer `terraform apply -replace=RESOURCE`\n\n## Common mistakes\n- Forgetting to enable DynamoDB state locking, allowing concurrent applies that corrupt state\n- Using `count` for resources that may be reordered; prefer `for_each` with stable map keys\n- Placing all resources in a single monolithic root module instead of composing smaller modules\n- Hardcoding region, account ID, or environment names instead of using variables and data sources\n- Ignoring plan output warnings about forces-replacement, which destroy resources before recreating them\n- Not using `terraform state mv` when refactoring module paths, causing unnecessary destroy-and-recreate cycles\n\n## Output contract\n- Provide all .tf files with consistent formatting (terraform fmt applied)\n- Include a variables.tf with descriptions, types, defaults, and validation rules\n- Document the backend configuration with bucket, key, region, and lock table\n- Output resource identifiers needed by downstream consumers\n- Include a written plan summary describing what will be created, changed, or destroyed\n- Specify provider version constraints and required_providers block\n- Provide a tfvars example file with placeholder values for each environment\n\n## Composability hints\n- Downstream: aws-s3, aws-lambda, aws-vpc, kubernetes experts consume the infrastructure Terraform provisions\n- Upstream: github-actions expert for CI/CD pipelines that run terraform plan on PR and terraform apply on merge\n- Related: docker expert when Terraform provisions container registries or ECS/EKS task definitions\n- Related: aws-vpc expert as a common first module to provision before compute or storage resources\n- Related: aws-s3 expert for provisioning S3 buckets used as Terraform remote state backends"
    },
    {
      "id": "data.time-series",
      "title": "Time Series Analysis and Forecasting",
      "domain": "data_science",
      "instruction_file": "instructions/time-series.md",
      "description": "Retrieve this expert for stationarity checks, decomposition, forecasting model selection, and time-aware validation.",
      "tags": [
        "time-series",
        "forecasting",
        "arima",
        "seasonality",
        "stationarity",
        "temporal-data"
      ],
      "tool_hints": [
        "statsmodels tsa",
        "prophet",
        "scikit-learn timeseriessplit",
        "pandas"
      ],
      "examples": [
        "Build leakage-safe temporal cross-validation forecast pipeline",
        "Compare ARIMA/SARIMA with ML lag-feature approach"
      ],
      "aliases": [
        "data-time-series",
        "time-series-analysis-and-forecasting"
      ],
      "dependencies": [
        "prophet",
        "pandas"
      ],
      "input_contract": {
        "required": "source schema/data samples and transformation objectives",
        "optional": "data volume profile, latency/SLA constraints, and governance rules"
      },
      "output_artifacts": [
        "transformation_logic_or_queries",
        "data_quality_report",
        "lineage_or_execution_notes"
      ],
      "quality_checks": [
        "schema_and_type_assumptions_verified",
        "null_and_edge_cases_handled",
        "reproducible_execution_documented"
      ],
      "constraints": [
        "avoid_silent_schema_or_type_breakage",
        "preserve_traceability_of_source_to_output"
      ],
      "risk_level": "medium",
      "maturity": "stable",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "data",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Time Series Analysis Expert\n\nUse this expert for temporal data modeling including stationarity testing, seasonal decomposition, ARIMA/SARIMA forecasting, Prophet models, time-aware cross-validation, and lag-based feature engineering.\n\n## When to use this expert\n- The dataset has a datetime index and the task involves forecasting, trend detection, or seasonality analysis.\n- Stationarity must be verified before fitting any parametric model.\n- Cross-validation must respect temporal ordering (no random shuffling).\n- Feature engineering from timestamps, lags, or rolling statistics is needed for ML-based forecasting.\n\n## Execution behavior\n\n1. Parse and sort the datetime index. Verify the frequency is consistent (daily, hourly, etc.) and fill or flag any gaps before proceeding.\n2. Visualize the raw series, its rolling mean, and rolling standard deviation to form an initial hypothesis about trend and seasonality.\n3. Test stationarity with the Augmented Dickey-Fuller (ADF) test and confirm with the KPSS test. If they disagree, apply differencing and retest.\n4. Decompose the series using `seasonal_decompose` (additive or multiplicative) or STL decomposition to isolate trend, seasonal, and residual components.\n5. Select the model family: use ARIMA/SARIMA for univariate with clear autocorrelation structure, Prophet for series with strong seasonality and holiday effects, or gradient boosting with lag features for multivariate settings.\n6. Fit the model and inspect residuals: check for remaining autocorrelation (Ljung-Box test), normality (Q-Q plot), and heteroscedasticity. If residuals show structure, the model is underfit.\n7. Validate using expanding-window or sliding-window cross-validation. Never use random k-fold splits on time series data.\n8. Report point forecasts with prediction intervals. Quantify forecast accuracy using MAE, RMSE, and MAPE on the held-out period.\n\n## Decision tree\n- If the series shows strong, regular seasonality -> use SARIMA with seasonal order `(P,D,Q,m)` or Prophet with automatic seasonality detection.\n- If multiple related series must be forecast together -> use hierarchical reconciliation (bottom-up, top-down, or optimal reconciliation) to ensure coherence.\n- If exogenous variables are available and correlated with the target -> use ARIMAX, SARIMAX, or a supervised ML model with lag features and calendar features.\n- If forecasts are consumed in real-time with incoming data -> use an expanding window for retraining, not a fixed-window model frozen at training time.\n- If the series has abrupt level shifts or structural breaks -> segment the series at break points and model each regime separately, or use a model robust to changepoints (Prophet).\n- If the data is high-frequency (sub-minute) with noise -> apply a smoothing filter (exponential, Savitzky-Golay) before decomposition, and note the lag introduced.\n\n## Anti-patterns\n- NEVER split time series data randomly into train and test sets. This leaks future information and produces unrealistically optimistic metrics.\n- NEVER fit ARIMA on a non-stationary series without differencing. The model assumptions are violated and forecasts will diverge.\n- NEVER overfit on small samples by using high-order AR or MA terms. Use AIC/BIC for order selection and validate out of sample.\n- NEVER ignore residual autocorrelation after fitting. Significant autocorrelation means the model has not captured all learnable signal.\n- NEVER report forecast accuracy without prediction intervals. Point forecasts alone hide the uncertainty range.\n\n## Common mistakes\n- Using `seasonal_decompose` with the wrong `period` parameter, producing meaningless seasonal components. Always match period to the true data frequency (e.g., 7 for daily data with weekly seasonality, 12 for monthly with annual seasonality).\n- Confusing ADF and KPSS null hypotheses: ADF tests H0 = unit root (non-stationary), KPSS tests H0 = stationary. They are complementary, not interchangeable.\n- Applying log transforms to series that contain zero or negative values, producing NaN or errors. Use a Box-Cox transform with a shift instead.\n- Building lag features and then accidentally including the current-period target value as a feature, creating direct leakage.\n- Evaluating multi-step forecasts with single-step accuracy metrics. Multi-step errors compound; measure at each horizon separately.\n- Ignoring calendar effects (holidays, weekends, month-end) that produce systematic residual patterns in business data.\n\n## Output contract\n- Report stationarity test results (ADF p-value, KPSS p-value) and any differencing or transformations applied.\n- Include a decomposition plot (trend, seasonal, residual) or describe its findings.\n- Provide residual diagnostics: ACF plot summary, Ljung-Box p-value, and a normality assessment.\n- Report forecast accuracy on held-out data with at least two metrics (MAE, RMSE, or MAPE).\n- Include prediction intervals (e.g., 80% and 95%) alongside point forecasts.\n- Document the cross-validation strategy used (expanding window size, step size, number of folds).\n- State the forecast horizon and frequency explicitly.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** to handle missing timestamps, duplicates, and timezone normalization.\n- Before this expert -> use the **Advanced Pandas Expert** for resampling, datetime indexing, and lag feature construction.\n- After this expert -> use the **Visualization Expert** to produce forecast plots with confidence bands and decomposition charts.\n- Related -> the **Statistics Expert** for formal hypothesis tests on residuals or structural break detection.\n- Related -> the **Quantitative Finance Expert** when the time series represents asset prices or returns requiring financial-specific treatment."
    },
    {
      "id": "dl.transformers-finetuning",
      "title": "Transformers Fine-tuning",
      "domain": "deep_learning",
      "instruction_file": "instructions/transformers-finetuning.md",
      "description": "Retrieve this expert for HuggingFace transformer fine-tuning, LoRA/QLoRA adaptation, and GPU-efficient training workflows.",
      "tags": [
        "transformers",
        "finetuning",
        "huggingface",
        "lora",
        "qlora",
        "nlp"
      ],
      "tool_hints": [
        "transformers trainer",
        "peft",
        "bitsandbytes",
        "datasets"
      ],
      "examples": [
        "Fine-tune text classifier with LoRA and evaluation metrics",
        "Run QLoRA on constrained GPU memory with stable checkpoints"
      ],
      "aliases": [
        "dl-transformers-finetuning",
        "transformers-fine-tuning"
      ],
      "dependencies": [
        "transformers",
        "peft",
        "bitsandbytes",
        "datasets"
      ],
      "input_contract": {
        "required": "task definition, model family choice, and dataset readiness details",
        "optional": "GPU memory budget, adapter strategy, and checkpoint cadence"
      },
      "output_artifacts": [
        "training_configuration",
        "model_or_adapter_artifacts",
        "evaluation_and_limitations_summary"
      ],
      "quality_checks": [
        "train_validation_split_strategy_documented",
        "checkpoint_and_recovery_plan_defined",
        "resource_constraints_accounted_for"
      ],
      "constraints": [
        "avoid_overstating_generalization",
        "track_dataset_model_and_prompt_versions"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "install_extra": "deep-learning",
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v2"
      },
      "instruction_text": "# Transformers Fine-tuning Expert\n\nUse this expert when tasks require fine-tuning pretrained transformer models via the HuggingFace ecosystem, including Trainer API workflows, parameter-efficient methods (LoRA/QLoRA), dataset preparation, and memory-efficient training strategies.\n\n## When to use this expert\n- The task involves fine-tuning a pretrained language model for classification, NER, summarization, translation, or generation.\n- Parameter-efficient fine-tuning (LoRA, QLoRA) is needed to adapt large models on limited GPU memory.\n- The user needs guidance on tokenization, dynamic padding, evaluation metrics, or checkpoint management with HuggingFace Trainer.\n- Multi-GPU training with DeepSpeed or FSDP is required for models that exceed single-GPU capacity.\n\n## Execution behavior\n\n1. Select the base model from HuggingFace Hub based on the task, language, and size constraints. Prefer models with active maintenance and published benchmarks. Check the model card for license, training data, and known limitations.\n2. Load the tokenizer and model together using `AutoTokenizer.from_pretrained` and the appropriate `AutoModelFor*` class. Verify the tokenizer's special tokens (BOS, EOS, PAD) match the model's expectations; set `tokenizer.pad_token = tokenizer.eos_token` if no pad token exists.\n3. Prepare the dataset using `datasets.map()` with the tokenizer. Use dynamic padding via `DataCollatorWithPadding` instead of padding to `max_length` at tokenization time. For sequence-to-sequence tasks, use `DataCollatorForSeq2Seq`.\n4. Define `TrainingArguments` with explicit settings: `learning_rate` (typically 2e-5 for full fine-tuning, 1e-4 to 3e-4 for LoRA), `warmup_ratio=0.06`, `weight_decay=0.01`, `evaluation_strategy=\"steps\"`, `save_strategy=\"steps\"`, and `load_best_model_at_end=True`.\n5. For large models (>= 7B parameters), apply QLoRA: load the base model in 4-bit with `BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)`, then wrap with `peft.get_peft_model()` using `LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05)`.\n6. For medium models (1B-7B) on constrained hardware, enable gradient checkpointing with `model.gradient_checkpointing_enable()` and set `gradient_accumulation_steps` to simulate larger effective batch sizes.\n7. Pass a `compute_metrics` function to the Trainer that calculates task-appropriate metrics (accuracy, F1, ROUGE, BLEU). Log metrics to Weights & Biases or TensorBoard via `TrainingArguments(report_to=\"wandb\")`.\n8. After training, merge LoRA adapters back into the base model with `model.merge_and_unload()` for simplified inference, or keep them separate for multi-adapter serving. Push the final model to HuggingFace Hub or save locally with `model.save_pretrained()`.\n\n## Decision tree\n- If classification or token classification (NER) -> use `Trainer` with `compute_metrics` and `AutoModelForSequenceClassification` or `AutoModelForTokenClassification`.\n- If text generation or summarization -> use `Seq2SeqTrainer` with `predict_with_generate=True` and `GenerationConfig`.\n- If model is large (> 7B parameters) -> apply QLoRA with 4-bit quantization via bitsandbytes and PEFT to fit in a single GPU.\n- If dataset is small (< 5K samples) -> use LoRA with low rank (r=8 to 16) to reduce overfitting; increase dropout to 0.1.\n- If multi-GPU is available -> configure DeepSpeed ZeRO Stage 2 or 3 via a `ds_config.json`, or use FSDP with `fsdp=\"full_shard\"` in TrainingArguments.\n- If inference speed is the priority -> quantize the final model with GPTQ or AWQ, or export to GGUF for llama.cpp serving.\n\n## Anti-patterns\n- NEVER fine-tune all parameters of a large model (> 3B) when LoRA achieves comparable quality at a fraction of the memory and cost.\n- NEVER train without periodic evaluation. Set `evaluation_strategy=\"steps\"` with a reasonable interval to detect overfitting early and enable `load_best_model_at_end`.\n- NEVER pad all sequences to `max_length` at tokenization time. This wastes GPU compute on padding tokens. Use dynamic padding with a `DataCollator` instead.\n- NEVER ignore special tokens when preparing labels. For causal LM fine-tuning, mask padding tokens in labels with `-100` so the loss function ignores them.\n- NEVER skip learning rate warmup for transformer fine-tuning. Cold-starting at the full learning rate destabilizes pretrained weights.\n- NEVER assume the default generation config is suitable. Set `max_new_tokens`, `temperature`, and `do_sample` explicitly for reproducible outputs.\n\n## Common mistakes\n- Forgetting to set `tokenizer.padding_side = \"left\"` for decoder-only models during batched generation, causing misaligned outputs.\n- Using a learning rate appropriate for full fine-tuning (2e-5) with LoRA, which is too low. LoRA typically needs 1e-4 to 3e-4.\n- Not setting `torch_dtype=torch.bfloat16` when loading the model, causing it to load in FP32 and doubling memory usage unnecessarily.\n- Passing `max_length` in tokenizer but not truncating, so sequences longer than the model's context window cause silent errors or crashes.\n- Training a causal LM without setting `is_decoder=True` or using the wrong `AutoModelFor*` class, producing nonsensical outputs.\n- Saving only the LoRA adapter without recording the exact base model version, making it impossible to reproduce the full model later.\n\n## Output contract\n- Save the fine-tuned model (or adapter weights) with `save_pretrained()`, including the tokenizer and generation config.\n- Report training loss curve, validation metrics per evaluation step, and final test set performance.\n- Record the base model name, revision hash, PEFT config (if used), and quantization settings for full reproducibility.\n- Include the effective batch size (per_device_batch_size * gradient_accumulation_steps * num_gpus) and total training steps.\n- Provide an inference example showing how to load the model and generate predictions.\n- Record GPU type, peak memory usage, and total training wall-clock time.\n- If LoRA was used, note whether adapters were merged or kept separate, and document the merge procedure.\n\n## Composability hints\n- Before this expert -> use the **Data Cleaning Expert** for text normalization and deduplication of training corpora.\n- Before this expert -> use the **NLP Expert** for exploratory tokenization analysis and baseline evaluation with pretrained models.\n- After this expert -> use the **Machine Learning Export Expert** to convert the model to ONNX or TensorRT for optimized serving.\n- After this expert -> use the **LangChain Agents Expert** to integrate the fine-tuned model into an agentic RAG or tool-use pipeline.\n- Related -> the **Hyperparameter Tuning Expert** for optimizing LoRA rank, learning rate, and warmup with Optuna.\n- Related -> the **PyTorch Training Expert** for custom training loops that go beyond the Trainer API."
    },
    {
      "id": "role.data-analyst",
      "title": "Data Analyst Role Orchestrator",
      "domain": "role_orchestrator",
      "instruction_file": "roles/data-analyst.md",
      "description": "Role expert that orchestrates profiling, cleaning, EDA, baseline modeling, and visualization for tabular analytics tasks.",
      "tags": [
        "role",
        "data-analyst",
        "eda",
        "pandas",
        "visualization",
        "baseline-modeling"
      ],
      "tool_hints": [
        "data.pandas-advanced",
        "data.sql-queries",
        "viz.matplotlib-seaborn",
        "ml.sklearn-modeling",
        "stats.scipy-statsmodels"
      ],
      "examples": [
        "Profile messy dataset, clean it, and deliver insight dashboard",
        "Run EDA plus baseline prediction with leakage-safe validation"
      ],
      "aliases": [
        "role-data-analyst",
        "analytics-orchestrator"
      ],
      "dependencies": [
        "data.pandas-advanced",
        "data.sql-queries",
        "viz.matplotlib-seaborn",
        "ml.sklearn-modeling",
        "stats.scipy-statsmodels"
      ],
      "input_contract": {
        "required": "tabular dataset and analysis objective",
        "optional": "target metric, business context, and prediction requirement"
      },
      "output_artifacts": [
        "data_profile_summary",
        "eda_findings",
        "visual_report",
        "baseline_model_report"
      ],
      "quality_checks": [
        "missingness_and_dtype_audit_completed",
        "insights_backed_by_numeric_evidence",
        "charts_have_labels_units_and_titles"
      ],
      "constraints": [
        "no_causal_claims_from_correlation_only",
        "use_only_allowed_dependencies_unless_explicitly_authorized"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "roles",
        "instruction_version": "v1"
      },
      "instruction_text": "# Data Analyst Role Expert\n\nUse this role when the request needs end-to-end tabular analysis: data profiling, cleaning, exploratory analysis, baseline modeling, and clear visual communication.\n\n## Allowed expert dependencies\n\n- `data.pandas-advanced`\n- `data.sql-queries`\n- `viz.matplotlib-seaborn`\n- `ml.sklearn-modeling`\n- `stats.scipy-statsmodels`\n\n## Execution behavior\n\n1. Start with a data quality audit:\n   nulls, dtypes, duplicates, outliers, key integrity, and temporal coverage.\n2. Normalize and clean data using reproducible transformations.\n3. Produce concise EDA:\n   distributions, trends, segmentation, and relationship charts.\n4. If prediction is requested, build a leakage-safe baseline model with validation metrics.\n5. Explain findings in business terms:\n   what changed, how much, and what action is implied.\n6. End with caveats and next steps.\n\n## Output contract\n\n- `profile_summary`: row/column counts, missingness, type issues, and anomalies.\n- `eda_insights`: ranked insights with numeric evidence.\n- `visuals`: labeled plots with clear units and titles.\n- `model_section` (optional): baseline model, metrics, and limitations.\n- `repro_steps`: commands/notebook steps to reproduce.\n\n## Guardrails\n\n- Do not skip data validation before insights.\n- Do not claim causality from correlation.\n- Do not use tools outside allowed dependencies unless explicitly approved."
    },
    {
      "id": "role.financial-analyst",
      "title": "Financial Analyst Role Orchestrator",
      "domain": "role_orchestrator",
      "instruction_file": "roles/financial-analyst.md",
      "description": "Role expert for financial KPI analysis, scenario modeling, forecasting, and risk-aware recommendations.",
      "tags": [
        "role",
        "financial-analyst",
        "finance",
        "forecasting",
        "scenario-analysis",
        "risk"
      ],
      "tool_hints": [
        "data.pandas-advanced",
        "data.time-series",
        "stats.scipy-statsmodels",
        "finance.financial-modeling",
        "finance.quantitative",
        "viz.matplotlib-seaborn"
      ],
      "examples": [
        "Build base/upside/downside forecast with sensitivity analysis",
        "Produce financial trend report with assumptions and risk notes"
      ],
      "aliases": [
        "role-financial-analyst",
        "finance-orchestrator"
      ],
      "dependencies": [
        "data.pandas-advanced",
        "data.time-series",
        "stats.scipy-statsmodels",
        "finance.financial-modeling",
        "finance.quantitative",
        "viz.matplotlib-seaborn"
      ],
      "input_contract": {
        "required": "financial dataset with timeframe and target question",
        "optional": "scenario assumptions, risk tolerance, and reporting horizon"
      },
      "output_artifacts": [
        "assumptions_table",
        "financial_kpi_summary",
        "scenario_analysis_output",
        "risk_and_limitations_report"
      ],
      "quality_checks": [
        "assumptions_explicitly_documented",
        "time_alignment_and_units_validated",
        "forecast_errors_and_uncertainty_reported"
      ],
      "constraints": [
        "do_not_present_estimates_as_certainty",
        "separate_observed_facts_from_modeled_assumptions",
        "use_only_allowed_dependencies_unless_explicitly_authorized"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "roles",
        "instruction_version": "v1"
      },
      "instruction_text": "# Financial Analyst Role Expert\n\nUse this role when the request needs financial analysis with explicit assumptions, risk framing, and decision-ready outputs.\n\n## Allowed expert dependencies\n\n- `data.pandas-advanced`\n- `data.time-series`\n- `stats.scipy-statsmodels`\n- `finance.financial-modeling`\n- `finance.quantitative`\n- `viz.matplotlib-seaborn`\n\n## Execution behavior\n\n1. Validate data quality first:\n   accounting consistency, missing values, period alignment, and unit conventions.\n2. Build core financial views:\n   growth, margin, cash flow proxies, and segmentation trends.\n3. Run scenario analysis:\n   base, upside, downside with transparent assumptions.\n4. If forecasting is requested, use time-aware validation and report error metrics.\n5. Present risk/uncertainty:\n   sensitivity bands, key drivers, and assumption fragility.\n6. Convert analysis into actionable recommendations with clear tradeoffs.\n\n## Output contract\n\n- `assumptions_table`: all model assumptions and data caveats.\n- `financial_summary`: KPI trends and performance decomposition.\n- `scenario_output`: base/upside/downside outcomes with driver impacts.\n- `charts`: time-series and sensitivity visuals with units and date axis.\n- `risk_notes`: non-trivial risks, limitations, and monitoring triggers.\n\n## Guardrails\n\n- Never present estimates as guaranteed outcomes.\n- Separate observed facts from model-driven assumptions.\n- Do not use tools outside allowed dependencies unless explicitly approved."
    },
    {
      "id": "ml.sklearn-pipeline-ops",
      "title": "Scikit-learn Pipeline Ops",
      "domain": "machine_learning",
      "instruction_file": "instructions/sklearn-pipeline-ops.md",
      "description": "Retrieve this expert for production-grade sklearn pipelines, calibration, thresholding, packaging, and monitoring-ready outputs.",
      "tags": [
        "sklearn",
        "pipeline",
        "columntransformer",
        "calibration",
        "thresholding",
        "model-ops"
      ],
      "tool_hints": [
        "sklearn.pipeline",
        "sklearn.compose",
        "sklearn.calibration",
        "joblib"
      ],
      "examples": [
        "Build deployment-ready pipeline with preprocessing parity checks",
        "Tune decision threshold with precision-recall tradeoff analysis"
      ],
      "aliases": [
        "ml-sklearn-pipeline-ops",
        "sklearn-production-pipelines"
      ],
      "dependencies": [
        "scikit-learn",
        "pandas",
        "numpy",
        "joblib"
      ],
      "input_contract": {
        "required": "training dataset with target variable and deployment objective",
        "optional": "class imbalance profile, threshold policy, and serving constraints"
      },
      "output_artifacts": [
        "fitted_sklearn_pipeline",
        "calibration_threshold_report",
        "deployment_readiness_notes"
      ],
      "quality_checks": [
        "preprocessing_parity_train_vs_inference",
        "baseline_vs_candidate_comparison_present",
        "metric_selection_aligned_to_objective"
      ],
      "constraints": [
        "no_untracked_manual_preprocessing",
        "no_train_only_metric_reporting"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Scikit-learn Pipeline Ops Expert\n\nUse this expert for production-style scikit-learn pipelines: robust preprocessing, calibration, thresholding, packaging, and monitoring preparation.\n\n## When to use this expert\n\n- The task needs reliable sklearn pipelines beyond ad-hoc notebooks.\n- You need consistent preprocessing across training and inference.\n- The user asks for deployment-ready artifacts, model cards, or monitoring hooks.\n- Classification threshold tuning or probability calibration is required.\n\n## Execution behavior\n\n1. Define split strategy and leakage boundaries first.\n2. Build `ColumnTransformer` + `Pipeline` with explicit type handling.\n3. Train baseline and compare with at least one alternative.\n4. Add calibration and threshold policy when classification decisions matter.\n5. Export model artifacts with dependency/version metadata.\n6. Provide post-deployment monitoring checklist.\n\n## Output expectations\n\n- Reproducible sklearn pipeline code.\n- Metric table with confidence intervals or variance across folds.\n- Calibration and threshold report (for classification).\n- Serialized artifact(s) and compatibility notes.\n\n## Quality checks\n\n- Preprocessing parity between train/inference confirmed.\n- Leakage and data-snooping risks documented.\n- Metric choice matches business objective."
    },
    {
      "id": "stats.experiment-design",
      "title": "Statistical Experiment Design and A/B Testing",
      "domain": "statistics",
      "instruction_file": "instructions/stats-experiment-design.md",
      "description": "Retrieve this expert for experiment planning, power analysis, sample-size estimation, and robust A/B test interpretation.",
      "tags": [
        "statistics",
        "ab-testing",
        "power-analysis",
        "sample-size",
        "hypothesis-testing",
        "effect-size"
      ],
      "tool_hints": [
        "scipy.stats",
        "statsmodels.stats.power",
        "numpy",
        "pandas"
      ],
      "examples": [
        "Design A/B experiment with MDE and power targets",
        "Validate sample ratio mismatch and run robust significance checks"
      ],
      "aliases": [
        "stats-experiment-design",
        "ab-test-design"
      ],
      "dependencies": [
        "scipy",
        "statsmodels",
        "numpy",
        "pandas"
      ],
      "input_contract": {
        "required": "primary metric, hypothesis, and experiment constraints",
        "optional": "historical variance, expected effect size, and traffic allocation"
      },
      "output_artifacts": [
        "experiment_plan",
        "power_sample_size_sheet",
        "decision_summary_with_uncertainty"
      ],
      "quality_checks": [
        "primary_metric_pre_registered",
        "multiple_testing_or_peeking_risks_addressed",
        "effect_size_and_confidence_intervals_reported"
      ],
      "constraints": [
        "avoid_p_value_only_conclusions",
        "separate_statistical_and_practical_significance"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Statistical Experiment Design Expert\n\nUse this expert for A/B testing and controlled experiments, including sample size planning, power analysis, randomization checks, and interpretation.\n\n## When to use this expert\n\n- The task involves treatment vs control comparisons.\n- The user asks for significance, power, or minimum detectable effect.\n- There is a need to design or audit experiment validity.\n- Multiple hypothesis tests or sequential looks are expected.\n\n## Execution behavior\n\n1. Define hypothesis, primary metric, guardrail metrics, and decision thresholds.\n2. Estimate sample size using expected effect size, variance, and alpha/beta.\n3. Validate randomization balance and check for sample-ratio mismatch.\n4. Run statistical tests with assumptions and robustness alternatives.\n5. Report effect size and uncertainty, not just p-values.\n6. Document risks from peeking, multiple testing, and population shifts.\n\n## Output expectations\n\n- Experiment plan with assumptions.\n- Power/sample-size worksheet.\n- Statistical test outcomes with effect sizes and confidence intervals.\n- Clear decision recommendation with caveats.\n\n## Quality checks\n\n- Primary metric fixed before analysis.\n- Multiple-testing adjustment considered when needed.\n- Practical significance evaluated alongside statistical significance."
    },
    {
      "id": "stats.model-diagnostics",
      "title": "Statistical Model Diagnostics",
      "domain": "statistics",
      "instruction_file": "instructions/stats-model-diagnostics.md",
      "description": "Retrieve this expert for residual diagnostics, calibration assessment, cohort error analysis, and model reliability checks.",
      "tags": [
        "statistics",
        "diagnostics",
        "residuals",
        "calibration",
        "drift",
        "reliability"
      ],
      "tool_hints": [
        "statsmodels",
        "scipy.stats",
        "sklearn.calibration",
        "matplotlib"
      ],
      "examples": [
        "Run residual and heteroskedasticity checks for regression model",
        "Evaluate classification calibration and subgroup error stability"
      ],
      "aliases": [
        "stats-model-diagnostics",
        "model-reliability-diagnostics"
      ],
      "dependencies": [
        "statsmodels",
        "scipy",
        "scikit-learn",
        "matplotlib"
      ],
      "input_contract": {
        "required": "trained model outputs, targets, and feature context",
        "optional": "cohort definitions, drift windows, and operating thresholds"
      },
      "output_artifacts": [
        "diagnostics_report",
        "cohort_error_breakdown",
        "risk_mitigation_actions"
      ],
      "quality_checks": [
        "diagnostic_tests_match_model_type",
        "influential_points_or_outliers_reviewed",
        "subgroup_instability_assessed"
      ],
      "constraints": [
        "avoid_global_metric_only_evaluation",
        "avoid_action_recommendations_without_evidence"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Statistical Model Diagnostics Expert\n\nUse this expert for rigorous model diagnostics: residual behavior, calibration quality, stability checks, and error decomposition.\n\n## When to use this expert\n\n- The user requests confidence in model reliability, not only headline metrics.\n- You need residual checks for regression models.\n- You need calibration, confusion tradeoffs, or error slicing for classification.\n- There is concern about drift, subgroup performance, or unstable behavior.\n\n## Execution behavior\n\n1. Evaluate baseline metrics and distribution of errors.\n2. Run residual diagnostics:\n   heteroskedasticity, autocorrelation, and influential-point checks as applicable.\n3. For classification, inspect calibration and threshold sensitivity.\n4. Segment errors by key cohorts to identify instability.\n5. Summarize failure modes and propose mitigation steps.\n\n## Output expectations\n\n- Diagnostics report with plots and test statistics.\n- Cohort-level error table.\n- Calibration or residual quality summary.\n- Action list for model hardening.\n\n## Quality checks\n\n- Diagnostics aligned to model type and objective.\n- Outliers/influential points reviewed before conclusions.\n- Recommendations include measurable follow-up checks."
    },
    {
      "id": "role.machine-learning-engineer",
      "title": "Machine Learning Engineer Role Orchestrator",
      "domain": "role_orchestrator",
      "instruction_file": "roles/machine-learning-engineer.md",
      "description": "Role expert for end-to-end ML delivery across feature pipelines, training, diagnostics, packaging, and deployment handoff.",
      "tags": [
        "role",
        "machine-learning-engineer",
        "mlops",
        "sklearn",
        "evaluation",
        "deployment"
      ],
      "tool_hints": [
        "data.pandas-advanced",
        "ml.feature-engineering",
        "ml.sklearn-modeling",
        "ml.sklearn-pipeline-ops",
        "ml.hyperparameter-tuning",
        "ml.gradient-boosting",
        "stats.scipy-statsmodels",
        "stats.experiment-design",
        "stats.model-diagnostics",
        "ml.model-export",
        "viz.matplotlib-seaborn"
      ],
      "examples": [
        "Build production-ready ML pipeline with diagnostics and export artifacts",
        "Deliver model handoff with monitoring plan and rollback triggers"
      ],
      "aliases": [
        "role-ml-engineer",
        "ml-engineering-orchestrator"
      ],
      "dependencies": [
        "data.pandas-advanced",
        "ml.feature-engineering",
        "ml.sklearn-modeling",
        "ml.sklearn-pipeline-ops",
        "ml.hyperparameter-tuning",
        "ml.gradient-boosting",
        "stats.scipy-statsmodels",
        "stats.experiment-design",
        "stats.model-diagnostics",
        "ml.model-export",
        "viz.matplotlib-seaborn"
      ],
      "input_contract": {
        "required": "dataset, target objective, and model success metric",
        "optional": "latency constraints, deployment target, and compliance requirements"
      },
      "output_artifacts": [
        "problem_contract",
        "pipeline_spec",
        "evaluation_report",
        "model_artifacts",
        "deployment_notes"
      ],
      "quality_checks": [
        "baseline_before_tuning",
        "leakage_controls_verified",
        "calibration_or_residual_checks_present",
        "export_compatibility_verified"
      ],
      "constraints": [
        "do_not_report_train_only_metrics",
        "include_segment_level_performance",
        "require_reproducibility_metadata",
        "use_only_allowed_dependencies_unless_explicitly_authorized"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "roles",
        "instruction_version": "v1"
      },
      "instruction_text": "# Machine Learning Engineer Role Expert\n\nUse this role for end-to-end ML delivery: data prep, feature pipelines, modeling, validation, packaging, and deployment-ready outputs.\n\n## Allowed expert dependencies\n\n- `data.pandas-advanced`\n- `ml.feature-engineering`\n- `ml.sklearn-modeling`\n- `ml.sklearn-pipeline-ops`\n- `ml.hyperparameter-tuning`\n- `ml.gradient-boosting`\n- `stats.scipy-statsmodels`\n- `stats.experiment-design`\n- `stats.model-diagnostics`\n- `ml.model-export`\n- `viz.matplotlib-seaborn`\n\n## Execution behavior\n\n1. Validate problem framing:\n   objective, target definition, metric, constraints, and success criteria.\n2. Build reproducible feature pipeline with leakage-safe train/validation/test strategy.\n3. Train baseline before tuning; then optimize with tracked experiments.\n4. Run diagnostics:\n   calibration, error slices, residual checks, stability, and drift risk.\n5. Package model for serving with metadata and compatibility checks.\n6. Produce an implementation handoff:\n   assumptions, known risks, and monitoring plan.\n\n## Output contract\n\n- `problem_contract`: objective, target, metric, and constraints.\n- `pipeline_spec`: preprocessing, feature logic, and split strategy.\n- `evaluation_report`: primary metrics, calibration, and error analysis.\n- `model_artifacts`: export format, version metadata, and reproducibility notes.\n- `deployment_notes`: serving assumptions, drift indicators, and rollback triggers.\n\n## Guardrails\n\n- Do not tune before establishing a baseline.\n- Do not report only aggregate metrics; include segment-level behavior.\n- Do not ship model artifacts without reload/inference parity checks.\n- Do not use tools outside allowed dependencies unless explicitly approved."
    },
    {
      "id": "role.data-engineer",
      "title": "Data Engineer Role Orchestrator",
      "domain": "role_orchestrator",
      "instruction_file": "roles/data-engineer.md",
      "description": "Role expert for scalable data pipelines, quality enforcement, and production-ready data platform delivery.",
      "tags": [
        "role",
        "data-engineer",
        "spark",
        "sql",
        "dbt",
        "pipelines"
      ],
      "tool_hints": [
        "data.spark",
        "data.sql-queries",
        "data.dbt",
        "data.pandas-advanced",
        "web.sqlalchemy",
        "cloud.docker",
        "cloud.kubernetes",
        "cloud.terraform"
      ],
      "examples": [
        "Build warehouse-ready pipeline with quality checks and observability",
        "Design Spark plus dbt workflow with replay and backfill procedures"
      ],
      "aliases": [
        "role-data-engineer",
        "data-platform-orchestrator"
      ],
      "dependencies": [
        "data.spark",
        "data.sql-queries",
        "data.dbt",
        "data.pandas-advanced",
        "web.sqlalchemy",
        "cloud.docker",
        "cloud.kubernetes",
        "cloud.terraform"
      ],
      "input_contract": {
        "required": "source datasets, target schema, and freshness expectations",
        "optional": "lineage requirements, cost constraints, and backfill policy"
      },
      "output_artifacts": [
        "pipeline_contract",
        "transformation_assets",
        "quality_report",
        "deployment_bundle",
        "operations_plan"
      ],
      "quality_checks": [
        "schema_evolution_handled",
        "idempotent_pipeline_design",
        "partition_strategy_documented",
        "data_quality_checks_embedded"
      ],
      "constraints": [
        "avoid_unbounded_scans_on_large_tables",
        "require_lineage_and_freshness_documentation",
        "use_only_allowed_dependencies_unless_explicitly_authorized"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "roles",
        "instruction_version": "v1"
      },
      "instruction_text": "# Data Engineer Role Expert\n\nUse this role for data platform work: ingestion, transformation, quality controls, orchestration, and warehouse-ready outputs.\n\n## Allowed expert dependencies\n\n- `data.spark`\n- `data.sql-queries`\n- `data.dbt`\n- `data.pandas-advanced`\n- `web.sqlalchemy`\n- `cloud.docker`\n- `cloud.kubernetes`\n- `cloud.terraform`\n\n## Execution behavior\n\n1. Define contracts first:\n   source schema, freshness SLA, lineage expectations, and failure policy.\n2. Build ingestion and transformation logic with idempotent behavior.\n3. Apply data quality checks:\n   null thresholds, uniqueness, referential integrity, and schema drift.\n4. Optimize compute strategy:\n   partitioning, predicate pushdown, and join/skew controls.\n5. Produce deployment-ready artifacts:\n   SQL/dbt/Spark jobs, environment config, and runbook notes.\n6. End with observability:\n   metrics, alerts, and replay/backfill procedures.\n\n## Output contract\n\n- `pipeline_contract`: schemas, SLAs, lineage scope, and failure handling.\n- `transformation_assets`: SQL/dbt/Spark logic with execution notes.\n- `quality_report`: checks, thresholds, and known data risks.\n- `deployment_bundle`: container/infrastructure notes and runtime assumptions.\n- `operations_plan`: monitoring, alerting, and incident response guide.\n\n## Guardrails\n\n- Do not ship pipelines without data quality checks.\n- Do not run unbounded scans on large datasets by default.\n- Do not ignore schema evolution and backward compatibility.\n- Do not use tools outside allowed dependencies unless explicitly approved."
    },
    {
      "id": "role.ml-researcher",
      "title": "ML Researcher Role Orchestrator",
      "domain": "role_orchestrator",
      "instruction_file": "roles/ml-researcher.md",
      "description": "Role expert for hypothesis-driven ML research, ablations, diagnostics, and reproducible experimental reporting.",
      "tags": [
        "role",
        "ml-researcher",
        "experimentation",
        "ablation",
        "pytorch",
        "transformers"
      ],
      "tool_hints": [
        "dl.pytorch-training",
        "dl.transformers-finetuning",
        "ml.hyperparameter-tuning",
        "ml.gradient-boosting",
        "stats.experiment-design",
        "stats.model-diagnostics",
        "stats.scipy-statsmodels",
        "data.pandas-advanced",
        "viz.matplotlib-seaborn"
      ],
      "examples": [
        "Run structured model ablations with reproducible experiment tracking",
        "Produce research report with confidence intervals and failure analysis"
      ],
      "aliases": [
        "role-ml-researcher",
        "research-orchestrator"
      ],
      "dependencies": [
        "dl.pytorch-training",
        "dl.transformers-finetuning",
        "ml.hyperparameter-tuning",
        "ml.gradient-boosting",
        "stats.experiment-design",
        "stats.model-diagnostics",
        "stats.scipy-statsmodels",
        "data.pandas-advanced",
        "viz.matplotlib-seaborn"
      ],
      "input_contract": {
        "required": "research objective, target metric, and baseline definition",
        "optional": "compute budget, experiment matrix, and publication criteria"
      },
      "output_artifacts": [
        "research_plan",
        "training_log_summary",
        "results_table",
        "diagnostics",
        "next_experiments"
      ],
      "quality_checks": [
        "baseline_plus_ablation_required",
        "uncertainty_and_variance_reported",
        "subgroup_or_slice_behavior_reviewed"
      ],
      "constraints": [
        "avoid_single_run_claims",
        "avoid_sota_claims_without_comparable_baselines",
        "use_only_allowed_dependencies_unless_explicitly_authorized"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "roles",
        "instruction_version": "v1"
      },
      "instruction_text": "# ML Researcher Role Expert\n\nUse this role for hypothesis-driven modeling research: experiment design, model iteration, ablation analysis, and reproducible reporting.\n\n## Allowed expert dependencies\n\n- `dl.pytorch-training`\n- `dl.transformers-finetuning`\n- `ml.hyperparameter-tuning`\n- `ml.gradient-boosting`\n- `stats.experiment-design`\n- `stats.model-diagnostics`\n- `stats.scipy-statsmodels`\n- `data.pandas-advanced`\n- `viz.matplotlib-seaborn`\n\n## Execution behavior\n\n1. State research hypothesis and success metric before training.\n2. Build reproducible training/evaluation protocol:\n   seed policy, split logic, and tracking fields.\n3. Run baseline plus at least one ablation or alternative model.\n4. Analyze reliability:\n   calibration, subgroup behavior, and error decomposition.\n5. Report effect sizes, confidence bounds, and practical significance.\n6. Summarize what is proven vs uncertain, and propose next experiments.\n\n## Output contract\n\n- `research_plan`: hypotheses, metrics, and experiment matrix.\n- `training_log_summary`: configs, checkpoints, and compute notes.\n- `results_table`: baseline/variant outcomes with uncertainty.\n- `diagnostics`: calibration/residual/subgroup analysis outputs.\n- `next_experiments`: prioritized follow-up tests.\n\n## Guardrails\n\n- Do not claim SOTA without rigorous baseline comparison.\n- Do not present single-run metrics as stable conclusions.\n- Do not omit uncertainty or variance in reported outcomes.\n- Do not use tools outside allowed dependencies unless explicitly approved."
    },
    {
      "id": "bi.dashboard-design",
      "title": "BI Dashboard Design and Storytelling",
      "domain": "business_intelligence",
      "instruction_file": "instructions/bi-dashboard-design.md",
      "description": "Retrieve this expert for KPI-driven dashboard architecture, chart selection, stakeholder communication, and adoption-focused BI reporting.",
      "tags": [
        "business-intelligence",
        "dashboard",
        "kpi",
        "storytelling",
        "reporting",
        "executive-view"
      ],
      "tool_hints": [
        "matplotlib",
        "seaborn",
        "tableau",
        "power bi"
      ],
      "examples": [
        "Design executive and analyst dashboard pages with drill-down structure",
        "Define KPI cards and trend visuals for weekly business review"
      ],
      "aliases": [
        "bi-dashboard-design",
        "dashboard-storytelling"
      ],
      "dependencies": [
        "data.sql-queries",
        "data.dbt",
        "viz.matplotlib-seaborn"
      ],
      "input_contract": {
        "required": "business objective, audience, and target KPIs",
        "optional": "dashboard tool choice, refresh cadence, and governance constraints"
      },
      "output_artifacts": [
        "dashboard_blueprint",
        "kpi_layout_spec",
        "interaction_and_filter_plan"
      ],
      "quality_checks": [
        "chart_types_match_business_questions",
        "kpi_cards_have_definition_and_owner",
        "time_window_and_filters_are_explicit"
      ],
      "constraints": [
        "avoid_visual_clutter_without_decision_value",
        "avoid_unlabeled_or_ambiguous_kpis"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# BI Dashboard Design Expert\n\nUse this expert for business intelligence dashboard planning, KPI hierarchy design, and stakeholder-friendly visual storytelling.\n\n## When to use this expert\n\n- The task requires a dashboard layout for business stakeholders.\n- You need KPI selection with drill-down paths and alert thresholds.\n- The user asks for executive summary plus analyst detail views.\n- Metric communication clarity matters more than raw model complexity.\n\n## Execution behavior\n\n1. Define audience and decision context first.\n2. Select leading and lagging KPIs with clear formulas.\n3. Design page structure:\n   overview, diagnostics, segment drill-down, and trend detail.\n4. Recommend chart types that match question type (comparison, trend, mix, distribution).\n5. Apply interaction design:\n   filters, date controls, and cross-highlighting behavior.\n6. Add governance notes:\n   refresh cadence, ownership, and alert criteria.\n\n## Output expectations\n\n- Dashboard blueprint with section-level purpose.\n- KPI catalog with definitions and owners.\n- Visualization recommendations with rationale.\n- Rollout checklist for validation and adoption.\n\n## Quality checks\n\n- KPI definitions are unambiguous.\n- Every chart has business question alignment.\n- Filters and time windows are explicitly documented."
    },
    {
      "id": "bi.metric-semantic-modeling",
      "title": "BI Metric and Semantic Modeling",
      "domain": "business_intelligence",
      "instruction_file": "instructions/bi-metric-semantic-modeling.md",
      "description": "Retrieve this expert for semantic layer design, metric governance, dimensional modeling, and trusted KPI standardization.",
      "tags": [
        "business-intelligence",
        "semantic-layer",
        "metric-governance",
        "dimensional-modeling",
        "star-schema",
        "kpi"
      ],
      "tool_hints": [
        "sql",
        "dbt",
        "metrics layer",
        "lineage docs"
      ],
      "examples": [
        "Define canonical metric dictionary with grain and owner",
        "Design star-schema marts for BI dashboards and ad hoc analysis"
      ],
      "aliases": [
        "bi-metric-semantic-modeling",
        "semantic-metric-layer"
      ],
      "dependencies": [
        "data.sql-queries",
        "data.dbt",
        "data.pandas-advanced"
      ],
      "input_contract": {
        "required": "entity definitions, required KPIs, and analysis grain",
        "optional": "lineage tool, SCD policy, and ownership model"
      },
      "output_artifacts": [
        "semantic_model_blueprint",
        "metric_dictionary",
        "quality_test_matrix"
      ],
      "quality_checks": [
        "metric_grain_is_explicit",
        "join_paths_prevent_double_counting",
        "kpi_time_window_semantics_defined"
      ],
      "constraints": [
        "avoid_implicit_metric_definitions",
        "avoid_untracked_schema_changes"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# BI Metric and Semantic Modeling Expert\n\nUse this expert for trusted metric definitions, semantic layer design, dimensional modeling, and business-ready data marts.\n\n## When to use this expert\n\n- The request needs standardized KPI definitions across teams.\n- You need star-schema style marts for BI tools.\n- Metric disagreement exists across departments.\n- Governance, lineage, and consistency are core concerns.\n\n## Execution behavior\n\n1. Inventory business entities, events, and required grain.\n2. Define dimensions, facts, and slowly changing dimension policy.\n3. Specify canonical KPI formulas and time-window semantics.\n4. Build semantic naming conventions and metric ownership map.\n5. Add validation tests for nulls, uniqueness, and referential integrity.\n6. Document lineage from source to KPI output.\n\n## Output expectations\n\n- Semantic model blueprint (entities, grain, joins).\n- Metric dictionary with formulas and caveats.\n- Data quality test matrix.\n- Governance and ownership handoff notes.\n\n## Quality checks\n\n- Metric grain is explicit and consistent.\n- Join paths avoid double counting.\n- KPI formulas include time-window definitions."
    },
    {
      "id": "role.bi-analyst",
      "title": "BI Analyst Role Orchestrator",
      "domain": "role_orchestrator",
      "instruction_file": "roles/bi-analyst.md",
      "description": "Role expert for KPI-centric business analysis, dashboard planning, and governed reporting outputs for stakeholders.",
      "tags": [
        "role",
        "bi-analyst",
        "business-intelligence",
        "dashboard",
        "kpi",
        "reporting"
      ],
      "tool_hints": [
        "bi.dashboard-design",
        "bi.metric-semantic-modeling",
        "data.sql-queries",
        "data.dbt",
        "data.pandas-advanced",
        "stats.scipy-statsmodels",
        "viz.matplotlib-seaborn"
      ],
      "examples": [
        "Build BI reporting spec with governed KPI definitions",
        "Produce stakeholder dashboard blueprint with drill-down analysis paths"
      ],
      "aliases": [
        "role-bi-analyst",
        "business-intelligence-orchestrator"
      ],
      "dependencies": [
        "bi.dashboard-design",
        "bi.metric-semantic-modeling",
        "data.sql-queries",
        "data.dbt",
        "data.pandas-advanced",
        "stats.scipy-statsmodels",
        "viz.matplotlib-seaborn"
      ],
      "input_contract": {
        "required": "business questions, KPI scope, and audience",
        "optional": "tooling preferences, refresh cadence, and governance policy"
      },
      "output_artifacts": [
        "business_questions",
        "kpi_dictionary",
        "analysis_summary",
        "dashboard_blueprint",
        "governance_notes"
      ],
      "quality_checks": [
        "kpi_dictionary_defined",
        "grain_consistency_validated",
        "dashboard_refresh_and_ownership_defined"
      ],
      "constraints": [
        "avoid_mixed_grain_metrics",
        "require_lineage_and_metric_definition_for_published_kpis",
        "use_only_allowed_dependencies_unless_explicitly_authorized"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "roles",
        "instruction_version": "v1"
      },
      "instruction_text": "# BI Analyst Role Expert\n\nUse this role for business reporting workflows: metric definition, SQL-based analysis, dashboard specification, and stakeholder-ready decision support.\n\n## Allowed expert dependencies\n\n- `bi.dashboard-design`\n- `bi.metric-semantic-modeling`\n- `data.sql-queries`\n- `data.dbt`\n- `data.pandas-advanced`\n- `stats.scipy-statsmodels`\n- `viz.matplotlib-seaborn`\n\n## Execution behavior\n\n1. Clarify business objective, audience, and decision timeline.\n2. Define KPI dictionary with formula, grain, and owner.\n3. Build or validate data extraction and transformation logic.\n4. Generate key insights with segment and trend breakdowns.\n5. Produce dashboard blueprint and reporting cadence recommendations.\n6. Document caveats, assumptions, and follow-up analysis opportunities.\n\n## Output contract\n\n- `business_questions`: prioritized decision questions.\n- `kpi_dictionary`: definitions, grain, and owners.\n- `analysis_summary`: concise insight statements with numeric evidence.\n- `dashboard_blueprint`: layout, interactions, and refresh policy.\n- `governance_notes`: lineage, quality checks, and ownership model.\n\n## Guardrails\n\n- Do not mix KPI definitions across grains.\n- Do not publish dashboards without metric lineage and refresh ownership.\n- Do not use tools outside allowed dependencies unless explicitly approved."
    },
    {
      "id": "bi.tableau-authoring",
      "title": "Tableau Authoring and Governance",
      "domain": "business_intelligence",
      "instruction_file": "instructions/bi-tableau-authoring.md",
      "description": "Retrieve this expert for Tableau dashboard design, interaction patterns, performance tuning, and governed publishing workflows.",
      "tags": [
        "business-intelligence",
        "tableau",
        "dashboard",
        "interactions",
        "governance",
        "publish"
      ],
      "tool_hints": [
        "tableau desktop",
        "tableau server",
        "tableau cloud",
        "extracts"
      ],
      "examples": [
        "Design Tableau dashboard with action filters and KPI drill paths",
        "Optimize workbook extract and publish workflow for stakeholders"
      ],
      "aliases": [
        "bi-tableau-authoring",
        "tableau-governance"
      ],
      "dependencies": [
        "data.sql-queries",
        "data.dbt",
        "bi.metric-semantic-modeling"
      ],
      "input_contract": {
        "required": "target business KPIs and dashboard audience",
        "optional": "publish environment, refresh cadence, and permission model"
      },
      "output_artifacts": [
        "tableau_blueprint",
        "interaction_map",
        "publish_governance_checklist"
      ],
      "quality_checks": [
        "kpi_lineage_to_source_verified",
        "dashboard_interaction_defaults_defined",
        "publish_ownership_documented"
      ],
      "constraints": [
        "avoid_unmanaged_published_workbooks",
        "avoid_ambiguous_kpi_calculations"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Tableau Authoring and Governance Expert\n\nUse this expert for Tableau dashboard authoring, extract/performance tuning, and governed publishing workflows.\n\n## When to use this expert\n\n- The task requires Tableau workbook design or optimization.\n- You need dashboard interactions and filter behavior for business users.\n- The user needs Tableau Server/Cloud publish governance.\n- Performance issues exist in workbook load or query response.\n\n## Execution behavior\n\n1. Define audience and KPI objective before layout work.\n2. Design dashboards with action filters, parameter controls, and drill paths.\n3. Optimize extracts and query patterns for responsive interaction.\n4. Apply naming conventions, folder structure, and certifiable data source policy.\n5. Document publish permissions, refresh cadence, and ownership.\n\n## Output expectations\n\n- Tableau dashboard blueprint and interaction map.\n- Performance and extract strategy recommendations.\n- Publish governance checklist.\n\n## Quality checks\n\n- KPI cards and calculations are traceable to source metrics.\n- Dashboards include usability-focused filtering defaults.\n- Publish model includes clear ownership and refresh policy."
    },
    {
      "id": "bi.powerbi-modeling",
      "title": "Power BI Modeling and DAX",
      "domain": "business_intelligence",
      "instruction_file": "instructions/bi-powerbi-modeling.md",
      "description": "Retrieve this expert for Power BI data modeling, DAX measure design, row-level security, and refresh strategy.",
      "tags": [
        "business-intelligence",
        "powerbi",
        "dax",
        "semantic-model",
        "rls",
        "refresh"
      ],
      "tool_hints": [
        "power bi desktop",
        "power bi service",
        "dax",
        "incremental refresh"
      ],
      "examples": [
        "Build Power BI semantic model with reusable DAX measures",
        "Configure RLS and incremental refresh for production reporting"
      ],
      "aliases": [
        "bi-powerbi-modeling",
        "powerbi-dax"
      ],
      "dependencies": [
        "data.sql-queries",
        "data.dbt",
        "bi.metric-semantic-modeling"
      ],
      "input_contract": {
        "required": "business KPIs, source schema, and report objectives",
        "optional": "security requirements, workspace policy, and refresh windows"
      },
      "output_artifacts": [
        "powerbi_model_spec",
        "dax_measure_catalog",
        "security_and_refresh_plan"
      ],
      "quality_checks": [
        "dax_filter_context_validated",
        "relationship_cardinality_reviewed",
        "rls_and_refresh_policy_documented"
      ],
      "constraints": [
        "avoid_ambiguous_model_relationships",
        "avoid_unvalidated_dax_measures"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Power BI Modeling and DAX Expert\n\nUse this expert for Power BI semantic modeling, DAX measure design, and service deployment practices.\n\n## When to use this expert\n\n- The task requires Power BI data model and relationship design.\n- You need robust DAX measures and time-intelligence calculations.\n- The user asks for row-level security or workspace governance.\n- Dataset refresh and incremental loading must be production-ready.\n\n## Execution behavior\n\n1. Model star-schema entities and verify relationship cardinality.\n2. Define reusable DAX measures with explicit filter context behavior.\n3. Configure row-level security and workspace-level access controls.\n4. Recommend incremental refresh strategy and dataset partition policy.\n5. Document report pages, measure lineage, and publish lifecycle.\n\n## Output expectations\n\n- Data model and relationship design notes.\n- DAX measure catalog and validation checks.\n- Security and refresh deployment plan.\n\n## Quality checks\n\n- DAX measures are tested across key filter contexts.\n- Model avoids ambiguous relationships and double counting.\n- Access and refresh policies are explicit and auditable."
    },
    {
      "id": "bi.looker-modeling",
      "title": "Looker Modeling and LookML",
      "domain": "business_intelligence",
      "instruction_file": "instructions/bi-looker-modeling.md",
      "description": "Retrieve this expert for LookML model design, explore governance, and performant semantic delivery in Looker.",
      "tags": [
        "business-intelligence",
        "looker",
        "lookml",
        "semantic-layer",
        "explores",
        "governance"
      ],
      "tool_hints": [
        "lookml",
        "looker explores",
        "pdt",
        "aggregate tables"
      ],
      "examples": [
        "Design LookML explores with governed measures and dimensions",
        "Tune Looker semantic model using PDT and aggregate strategies"
      ],
      "aliases": [
        "bi-looker-modeling",
        "looker-lookml"
      ],
      "dependencies": [
        "data.sql-queries",
        "data.dbt",
        "bi.metric-semantic-modeling"
      ],
      "input_contract": {
        "required": "entity model, metric requirements, and explore needs",
        "optional": "access policy, PDT strategy, and deployment workflow"
      },
      "output_artifacts": [
        "lookml_model_blueprint",
        "explore_design_spec",
        "governance_and_deploy_checklist"
      ],
      "quality_checks": [
        "measure_logic_matches_kpi_dictionary",
        "explore_join_fanout_risks_reviewed",
        "access_and_ownership_policy_documented"
      ],
      "constraints": [
        "avoid_unreviewed_semantic_model_changes",
        "avoid_duplicate_measure_definitions"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Looker Modeling and LookML Expert\n\nUse this expert for LookML semantic modeling, explore design, and governed metric delivery in Looker.\n\n## When to use this expert\n\n- The task needs LookML model/view/explore design.\n- You need centralized metric logic and reusable explores.\n- The user asks for access filters and model governance.\n- Query performance and cache behavior must be tuned.\n\n## Execution behavior\n\n1. Define model grain, join relationships, and explore entry points.\n2. Implement dimensions/measures with consistent naming and descriptions.\n3. Add access filters, user attributes, and permission-aware explores.\n4. Optimize query patterns with aggregate tables and PDT strategy.\n5. Document deployment and code review workflow for LookML changes.\n\n## Output expectations\n\n- LookML modeling blueprint.\n- Explore and metric design recommendations.\n- Governance and deployment checklist.\n\n## Quality checks\n\n- Measure definitions match canonical KPI dictionary.\n- Explore joins prevent fan-out and double counting.\n- Access controls and model ownership are documented."
    },
    {
      "id": "role.analytics-engineer",
      "title": "Analytics Engineer Role Orchestrator",
      "domain": "role_orchestrator",
      "instruction_file": "roles/analytics-engineer.md",
      "description": "Role expert for governed analytics delivery across semantic modeling, tested transformations, and BI platform implementation.",
      "tags": [
        "role",
        "analytics-engineer",
        "semantic-layer",
        "dbt",
        "bi-platforms",
        "metric-governance"
      ],
      "tool_hints": [
        "bi.metric-semantic-modeling",
        "bi.dashboard-design",
        "bi.tableau-authoring",
        "bi.powerbi-modeling",
        "bi.looker-modeling",
        "data.dbt",
        "data.sql-queries",
        "data.pandas-advanced",
        "stats.scipy-statsmodels",
        "viz.matplotlib-seaborn"
      ],
      "examples": [
        "Build analytics engineering plan for dbt plus BI semantic delivery",
        "Deliver governed KPI model across Tableau, Power BI, and Looker"
      ],
      "aliases": [
        "role-analytics-engineer",
        "semantic-delivery-orchestrator"
      ],
      "dependencies": [
        "bi.metric-semantic-modeling",
        "bi.dashboard-design",
        "bi.tableau-authoring",
        "bi.powerbi-modeling",
        "bi.looker-modeling",
        "data.dbt",
        "data.sql-queries",
        "data.pandas-advanced",
        "stats.scipy-statsmodels",
        "viz.matplotlib-seaborn"
      ],
      "input_contract": {
        "required": "business metric scope, source systems, and target BI platform",
        "optional": "governance constraints, ownership model, and release cadence"
      },
      "output_artifacts": [
        "semantic_contract",
        "transformation_spec",
        "bi_delivery_plan",
        "governance_pack",
        "acceptance_checks"
      ],
      "quality_checks": [
        "semantic_grain_defined",
        "metric_parity_verified_across_tools",
        "lineage_refresh_and_access_governed"
      ],
      "constraints": [
        "avoid_dashboard_specific_metric_forks_without_approval",
        "require_tested_transformations_before_publish",
        "use_only_allowed_dependencies_unless_explicitly_authorized"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "roles",
        "instruction_version": "v1"
      },
      "instruction_text": "# Analytics Engineer Role Expert\n\nUse this role for governed analytics delivery between data engineering and BI: semantic modeling, tested transformations, and publish-ready metrics.\n\n## Allowed expert dependencies\n\n- `bi.metric-semantic-modeling`\n- `bi.dashboard-design`\n- `bi.tableau-authoring`\n- `bi.powerbi-modeling`\n- `bi.looker-modeling`\n- `data.dbt`\n- `data.sql-queries`\n- `data.pandas-advanced`\n- `stats.scipy-statsmodels`\n- `viz.matplotlib-seaborn`\n\n## Execution behavior\n\n1. Define business entities, grain, and KPI contracts.\n2. Build tested transformation and semantic layers.\n3. Validate KPI logic across tools and filter contexts.\n4. Design BI delivery patterns for the chosen platform (Tableau/Power BI/Looker).\n5. Add quality, lineage, ownership, and refresh governance.\n6. Deliver implementation plan with monitoring and change-management notes.\n\n## Output contract\n\n- `semantic_contract`: entities, grain, joins, and KPI rules.\n- `transformation_spec`: SQL/dbt logic and validation tests.\n- `bi_delivery_plan`: platform-specific dashboard/model implementation notes.\n- `governance_pack`: lineage, ownership, refresh, and access policy.\n- `acceptance_checks`: test cases for KPI parity and dashboard correctness.\n\n## Guardrails\n\n- Do not publish metrics without grain and formula definitions.\n- Do not permit dashboard-specific KPI forks without governance approval.\n- Do not use tools outside allowed dependencies unless explicitly approved."
    },
    {
      "id": "cloud.azure-storage",
      "title": "Azure Storage and Data Lake",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/azure-storage.md",
      "description": "Retrieve this expert for Azure Blob and ADLS Gen2 architecture, data layout, access policy, and lifecycle governance.",
      "tags": [
        "azure",
        "storage",
        "adls",
        "blob",
        "data-lake",
        "cloud"
      ],
      "tool_hints": [
        "azure blob storage",
        "adls gen2",
        "managed identity",
        "azure rbac"
      ],
      "examples": [
        "Design raw to curated ADLS layout with partitioning policy",
        "Configure secure data lake access model for analytics teams"
      ],
      "aliases": [
        "cloud-azure-storage",
        "azure-data-lake"
      ],
      "dependencies": [
        "azure storage",
        "adls gen2",
        "managed identity"
      ],
      "input_contract": {
        "required": "data domains, retention policy, and access requirements",
        "optional": "cost constraints, lifecycle policy, and downstream consumers"
      },
      "output_artifacts": [
        "storage_architecture_blueprint",
        "security_and_access_plan",
        "partitioning_and_layout_spec"
      ],
      "quality_checks": [
        "partitioning_conventions_defined",
        "least_privilege_access_modeled",
        "retention_and_deletion_policies_documented"
      ],
      "constraints": [
        "avoid_flat_unpartitioned_data_lakes",
        "avoid_unmanaged_public_access"
      ],
      "risk_level": "medium",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Azure Storage and Data Lake Expert\n\nUse this expert for Azure Blob Storage and ADLS Gen2 design, access policy, data layout, and ingestion readiness.\n\n## When to use this expert\n\n- The task requires cloud object storage for analytics pipelines.\n- You need partitioning and folder conventions for BI/ML workloads.\n- Access control, encryption, and lifecycle policy decisions are needed.\n- The user asks for Azure-native data lake setup guidance.\n\n## Execution behavior\n\n1. Define storage zones:\n   raw, curated, and serving layers with retention policy.\n2. Design partitioning and file format strategy for query efficiency.\n3. Apply access model:\n   RBAC, managed identities, and least-privilege data access.\n4. Recommend lifecycle, backup, and cost-control settings.\n5. Document integration paths for ADF, Synapse, and BI consumers.\n\n## Output expectations\n\n- Storage architecture blueprint.\n- Security and access policy plan.\n- Partitioning and data layout recommendations.\n- Operational checklist for monitoring and cost.\n\n## Quality checks\n\n- Folder and partition conventions are explicit.\n- Access model aligns with least privilege.\n- Data retention and deletion policies are defined."
    },
    {
      "id": "cloud.azure-data-factory",
      "title": "Azure Data Factory Pipelines",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/azure-data-factory.md",
      "description": "Retrieve this expert for Azure Data Factory pipeline orchestration, ETL scheduling, retries, and operational reliability patterns.",
      "tags": [
        "azure",
        "data-factory",
        "orchestration",
        "etl",
        "scheduling",
        "pipelines"
      ],
      "tool_hints": [
        "azure data factory",
        "copy activity",
        "mapping data flows",
        "triggers"
      ],
      "examples": [
        "Build parameterized ADF ingestion and transformation workflows",
        "Define retry and backfill strategy with data quality gates"
      ],
      "aliases": [
        "cloud-azure-data-factory",
        "adf-pipelines"
      ],
      "dependencies": [
        "azure data factory",
        "azure storage",
        "data.sql-queries"
      ],
      "input_contract": {
        "required": "source and target systems with freshness SLA",
        "optional": "retry policy, backfill windows, and quality threshold rules"
      },
      "output_artifacts": [
        "pipeline_topology_design",
        "retry_and_failure_policy",
        "monitoring_and_alerting_plan"
      ],
      "quality_checks": [
        "idempotency_strategy_defined",
        "quality_gates_before_publish",
        "alert_ownership_and_thresholds_set"
      ],
      "constraints": [
        "avoid_non_recoverable_pipeline_steps",
        "avoid_hidden_environment_specific_logic"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Azure Data Factory Pipelines Expert\n\nUse this expert for Azure Data Factory orchestration, ETL pipeline design, scheduling, and operational reliability.\n\n## When to use this expert\n\n- The task requires orchestrated ingestion or transformation in Azure.\n- You need dependency-aware pipeline scheduling and retries.\n- The user asks for robust copy activities and parameterized workflows.\n- Monitoring, alerting, and failure recovery are in scope.\n\n## Execution behavior\n\n1. Define source/target contracts and freshness SLAs.\n2. Design pipeline topology:\n   ingestion, transformation, validation, and publish stages.\n3. Implement parameterized activities and environment configuration.\n4. Add retry, dead-letter, and backfill handling strategy.\n5. Specify monitoring, alerts, and operational runbooks.\n\n## Output expectations\n\n- Pipeline orchestration design.\n- Activity-level failure and retry policy.\n- Monitoring and operational support plan.\n- Deployment notes for environment promotion.\n\n## Quality checks\n\n- Pipeline idempotency strategy documented.\n- Data quality gates inserted before publish.\n- Alert thresholds and on-call ownership defined."
    },
    {
      "id": "cloud.azure-synapse",
      "title": "Azure Synapse Analytics",
      "domain": "cloud_infrastructure",
      "instruction_file": "instructions/azure-synapse.md",
      "description": "Retrieve this expert for Synapse SQL and Spark architecture, serving model design, and BI-facing optimization.",
      "tags": [
        "azure",
        "synapse",
        "sql-pool",
        "spark",
        "analytics",
        "warehouse"
      ],
      "tool_hints": [
        "azure synapse",
        "dedicated sql pool",
        "serverless sql",
        "synapse spark"
      ],
      "examples": [
        "Choose Synapse workload architecture for BI and ML consumers",
        "Optimize Synapse serving model for dashboard latency and cost"
      ],
      "aliases": [
        "cloud-azure-synapse",
        "synapse-analytics"
      ],
      "dependencies": [
        "cloud.azure-storage",
        "data.sql-queries",
        "data.spark"
      ],
      "input_contract": {
        "required": "workload latency goals, concurrency profile, and source systems",
        "optional": "cost budget, security boundaries, and BI serving targets"
      },
      "output_artifacts": [
        "synapse_architecture_recommendation",
        "query_optimization_plan",
        "bi_integration_strategy"
      ],
      "quality_checks": [
        "architecture_matches_workload_profile",
        "security_boundaries_documented",
        "serving_grain_and_metric_consistency_verified"
      ],
      "constraints": [
        "avoid_unbounded_compute_without_cost_guardrails",
        "avoid_ambiguous_serving_models"
      ],
      "risk_level": "high",
      "maturity": "beta",
      "metadata": {
        "provider_support": [
          "codex",
          "claude"
        ],
        "owner": "community",
        "catalog_tier": "expanded",
        "instruction_version": "v1"
      },
      "instruction_text": "# Azure Synapse Analytics Expert\n\nUse this expert for Synapse warehouse/lakehouse architecture, SQL and Spark workload planning, and BI-serving optimization.\n\n## When to use this expert\n\n- The task requires Synapse-based analytics platform design.\n- You need dedicated SQL pool or serverless query strategy.\n- Spark and SQL workloads must coexist with predictable performance.\n- The user asks for BI-ready modeling and cost/performance tradeoffs.\n\n## Execution behavior\n\n1. Choose workload architecture:\n   serverless SQL, dedicated SQL pools, Spark, or hybrid.\n2. Define ingestion-to-serving flow with model grain and refresh strategy.\n3. Optimize storage/query patterns:\n   partitioning, distribution, indexing, and caching.\n4. Add workload governance:\n   access control, resource classes, and cost guardrails.\n5. Document BI connectivity and semantic model implications.\n\n## Output expectations\n\n- Synapse architecture recommendation with tradeoffs.\n- Query and data layout optimization plan.\n- Governance, security, and cost-control checklist.\n- BI integration strategy (Power BI/Tableau/Looker path).\n\n## Quality checks\n\n- Architecture aligns with workload latency and concurrency requirements.\n- Security and access boundaries are explicit.\n- Serving model avoids double counting and ambiguous grain."
    }
  ]
}
